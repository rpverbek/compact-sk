{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHM North America challenge '23\n",
    "\n",
    "# 03 - Offline vibration fingerprint extraction\n",
    "\n",
    "This notebook constructs the context-sensitive vibration fingerprints from the vibration data that was previously preprocessed in [02_data_processing.ipynb](02_data_processing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from conscious_engie_icare.data.phm_data_handler import CACHING_FOLDER_NAME, FPATH_DF_V_TRAIN_FOLDS, \\\n",
    "                                                        FPATH_META_DATA_TRAIN_FOLDS, FPATH_META_DATA_TRAIN_FOLDS, \\\n",
    "                                                        fetch_and_unzip_data\n",
    "from conscious_engie_icare.nmf_profiling import extract_nmf_per_number_of_component\n",
    "from conscious_engie_icare.viz.viz import illustrate_nmf_components_for_paper\n",
    "from conscious_engie_icare import distance_metrics\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import string\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# ignore convergence warnings (1000 iterations reached by NMF)\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `CACHE_RESULTS` to `True` to cache the results of the feature extraction process. This will speed up the notebook execution time in subsequent executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_RESULTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [previously preprocessed](02_data_processing.ipynb) vibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_and_unzip_data()\n",
    "\n",
    "with open(FPATH_DF_V_TRAIN_FOLDS, 'rb') as file:\n",
    "    df_V_train_folds = pickle.load(file)\n",
    "\n",
    "with open(FPATH_META_DATA_TRAIN_FOLDS, 'rb') as file:\n",
    "    meta_data_train_folds = pickle.load(file)\n",
    "\n",
    "# extract list of frequency band columns for later usage\n",
    "cols_ = df_V_train_folds[0].columns\n",
    "BAND_COLS = cols_[cols_.str.contains('band')].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)\n",
    "\n",
    "Below, we decompose the vibration decomposition matrix $\\mathbf{V}$ into a weight matrix $\\mathbf{W}$ and a component matrix $\\mathbf{H}$ for each of the 100 folds.\n",
    "We decompose the matrix into up to 40 components, i.e. there are around 4000 different non-negative matrix factorizations performed.\n",
    "As the cell takes around 20 minutes to execute, we cache the results to avoid re-executing it in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMPUTE = False        # only set to True, if NMF should be recomputed\n",
    "MAX_N_COMPONENTS = 40    # maximum number of components used to recompute\n",
    "\n",
    "for fold, df_V_train_ in enumerate(tqdm(df_V_train_folds, desc='Extracting NMF models per fold')):\n",
    "    fpath = os.path.join(CACHING_FOLDER_NAME, 'df_nmf_models_folds', f'df_nmf_models_folds_{fold}.pkl')\n",
    "    if not os.path.exists(fpath) or RECOMPUTE:\n",
    "        df_nmf_models_ = extract_nmf_per_number_of_component(\n",
    "            df_V_train_, n_components=MAX_N_COMPONENTS, timestamps=df_V_train_.index, verbose=False\n",
    "        )\n",
    "        if CACHE_RESULTS:\n",
    "            os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "            pickle.dump(df_nmf_models_, open(fpath, 'wb'))\n",
    "\n",
    "# load data from disk\n",
    "fpaths = glob.glob(os.path.join(CACHING_FOLDER_NAME, 'df_nmf_models_folds', f'df_nmf_models_folds_*.pkl'))\n",
    "# sort in ascending order based on the integer value <i> in df_nmf_models_folds_<i>.pkl\n",
    "fpaths = sorted(fpaths, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "df_nmf_models_folds = [pickle.load(open(fpath, 'rb')) for fpath in tqdm(fpaths, desc='Loading folds from disk')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we illustrate the decomposition of the vibration data for the first fold.\n",
    "\n",
    "- The left plot shows the decomposition matrix $\\mathbf{V}$ as it was preprocessed in the previous notebook. Each vibration measurement is represented as a row in the matrix, each column represents an order transformed and normalized bin. We stack the 3 different measurement directions on top of each other, resulting in 645 vibration measurements, corresponding to the 215 measurements in the training set.\n",
    "- The middle two plots visualize how well the decomposition matrix $\\mathbf{V}$ can be approximated. \n",
    "    - The top plot illustrates the cumulative explained variance of a principal component analysis (PCA) of the decomposition matrix $\\mathbf{V}$. It serves as an indication of an upper bound for how well the signal could be expressed using PCA.\n",
    "    - The bottom plot illustrates the reconstruction error of the decomposition matrix $\\mathbf{V}$ using the NMF components. The reconstruction error is calculated as the Frobenius norm of the difference between the original matrix and the reconstructed matrix.\n",
    "- The right plot illustrates the component matrix $\\mathbf{H}$. Each individual component expresses a different peak. The number of components is determined by how many components in a in parallel running PCA are needed to explain 95% of the variance in the data, corresponding to 5 components in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hyperparameters for NMF\n",
    "FOLD = 2\n",
    "QMIN = 0.001\n",
    "QMAX = 0.999\n",
    "MIN_EXPLAINED_VARIANCE = 95\n",
    "ORDER_COMPONENTS = None # don't order the components\n",
    "\n",
    "df_V_train_ = df_V_train_folds[FOLD]\n",
    "df_nmf_models_ = df_nmf_models_folds[FOLD]\n",
    "\n",
    "# get minimum and maximum for feature space\n",
    "vmin = df_V_train_.stack().quantile(q=QMIN)\n",
    "print(f'    - took {vmin} (=0.01 quantile) as vmin for plotting feature space')\n",
    "vmax = df_V_train_.stack().quantile(q=QMAX)\n",
    "print(f'    - took {vmax} (=0.99 quantile) as vmax for plotting feature space')\n",
    "\n",
    "# calculate explained variance\n",
    "print(f'- calculating explained variance...')\n",
    "pca = PCA(n_components=len(BAND_COLS), random_state=42)\n",
    "pca.fit(df_V_train_)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "explained_variance_ratio = explained_variance_ratio[:39]\n",
    "\n",
    "fig, _ = illustrate_nmf_components_for_paper(\n",
    "    df_V_train_, explained_variance_ratio, df_nmf_models_, pd.Series(BAND_COLS),\n",
    "    min_explained_variance=MIN_EXPLAINED_VARIANCE, order_components=ORDER_COMPONENTS,\n",
    "    vmin=vmin, vmax=vmax, xlims=(-1,101), plot_x_ticks=[0, 10, 20, 30, 40, 49]\n",
    ")\n",
    "\n",
    "#fig.savefig(os.path.join('figs', 'nmf_exemplary_fold.pdf'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize across all folds the same number of components. We choose 5 based on the analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 5\n",
    "COMPONENT_COLUMNS = list(range(N_COMPONENTS))  # used later\n",
    "model_folds = [df_nmf_models_[(df_nmf_models_.n_components == N_COMPONENTS)].iloc[0] for df_nmf_models_ in df_nmf_models_folds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline vibration fingerprint extraction\n",
    "\n",
    "The process view is simplified in this particular use case.\n",
    "In contrast to the industrial dataset, in this dataset there are no timestamps. \n",
    "However, we know speed and torque for each measurement. \n",
    "Therefore, we merge the fingerprint with the operating mode.\n",
    "Below we plot the weights of a single measurement from the training set.\n",
    "All measurement directions (x, y and z) are stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_train_with_OM_folds = []\n",
    "\n",
    "for i, (model_, df_V_train_, meta_data_train_) in enumerate(zip(model_folds, df_V_train_folds, meta_data_train_folds)):\n",
    "    W_train_ = model_.W.reshape(-1, N_COMPONENTS)\n",
    "    df_W_train_ = pd.DataFrame(W_train_)\n",
    "    #display(f'Fold {i}. Shape: {W_train_.shape}')\n",
    "    df_W_train_.index = df_V_train_.index\n",
    "    df_W_train_['direction'] = meta_data_train_['direction']\n",
    "\n",
    "    # add operating mode (OM)\n",
    "    df_W_train_with_OM_ = pd.merge(df_W_train_, meta_data_train_.drop(columns=['direction']), left_index=True, right_index=True)\n",
    "    df_W_train_with_OM_folds.append(df_W_train_with_OM_)\n",
    "\n",
    "# illustrate weights of single measurement \n",
    "rpm=100\n",
    "torque=500\n",
    "run=1\n",
    "fold=0\n",
    "\n",
    "df_W_train_with_OM_ = df_W_train_with_OM_folds[fold]\n",
    "df_ = df_W_train_with_OM_[(df_W_train_with_OM_['rotational speed [RPM]']==rpm) &\n",
    "                          (df_W_train_with_OM_['torque [Nm]']==torque) &\n",
    "                          (df_W_train_with_OM_['sample_id']==run)]\n",
    "df_ = df_.set_index('direction')\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(df_[list(range(N_COMPONENTS))], annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "ax.set_title(f'Measurement {run} @ {rpm} rpm, {torque} Nm');\n",
    "ax.set_xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **context-sensitive vibration fingerprint** is the aggregation over all measurements of a given operating mode.\n",
    "Below, we plot the context-sensitive vibration fingerprint for the same operating mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_train_with_OM_ = df_W_train_with_OM_folds[fold]\n",
    "df_ = df_W_train_with_OM_[(df_W_train_with_OM_['rotational speed [RPM]']==rpm) & (df_W_train_with_OM_['torque [Nm]']==torque)]\n",
    "df_ = df_[list(range(N_COMPONENTS)) + ['direction']].groupby('direction').mean()\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(df_, annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "ax.set_title(f'Vibration fingerprint @ {rpm} rpm, {torque} Nm');\n",
    "ax.set_xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the vibration fingerprint and the measurement are very similar, due to at least following reasons: \n",
    "- As the measurements were taken in a controlled environment, the operating mode is constant for each operating condition.\n",
    "- There are very few measurements per operating mode. Therefore, the fingerprint is influenced to a high degree by the single measurement.\n",
    "\n",
    "The vibration fingerprint serves as key component of the context-sensitive anomaly detection.\n",
    "\n",
    "In what follows we construct the vibration fingerprint for each operating mode in the training set, where we treat each unique combination of speed and torque as a separate operating mode. We then aggregate the fingerprints for each operating mode. The 73 fingerprints are visualized below. Note that the number of fingerprints may vary depending on the fold, as not all operating modes are present in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_FINGERPRINTS = True   # set SHOW_FINGERPRINTS to False, if visualization should not be shown\n",
    "\n",
    "# for each unique combination of RPM and torque, assign a unique cluster label\n",
    "cluster_label_unique_name_mapping_folds = []\n",
    "for i, df_W_train_with_OM_ in enumerate(df_W_train_with_OM_folds):\n",
    "    df_W_train_with_OM_['cluster_label_unique'] = df_W_train_with_OM_.groupby(['rotational speed [RPM]', 'torque [Nm]']).ngroup()\n",
    "    df_W_train_with_OM_folds[i] = df_W_train_with_OM_\n",
    "    cluster_label_unique_name_mapping_folds.append(df_W_train_with_OM_.groupby('cluster_label_unique').first()[['rotational speed [RPM]', 'torque [Nm]']].reset_index())\n",
    "\n",
    "# extract operating mode wise fingerprints\n",
    "grouping_vars = ['direction', 'cluster_label_unique']\n",
    "fingerprints_folds = []\n",
    "for i, df_W_train_with_OM_ in enumerate(df_W_train_with_OM_folds):\n",
    "    df_ = df_W_train_with_OM_[COMPONENT_COLUMNS + grouping_vars].copy()\n",
    "    fingerprints_ = {\n",
    "        om: om_group.groupby(['direction']).mean().drop(columns=['cluster_label_unique']) for om, om_group in df_.groupby('cluster_label_unique')\n",
    "    }\n",
    "    fingerprints_folds.append(fingerprints_)\n",
    "\n",
    "# illustrate fingerprints\n",
    "fingerprints_ = fingerprints_folds[fold]\n",
    "cluster_label_unique_name_mapping_ = cluster_label_unique_name_mapping_folds[fold]\n",
    "if SHOW_FINGERPRINTS:\n",
    "    nrows = math.ceil(len(fingerprints_) / 3)\n",
    "    fig, axes = plt.subplots(figsize=(15, 2*nrows), nrows=nrows, ncols=3, sharex=True, sharey=True)\n",
    "    for om, ax in tqdm(zip(fingerprints_, axes.flat), total=len(fingerprints_), desc='Plotting fingerprints'):\n",
    "        om_group = fingerprints_[om]\n",
    "        om_group.columns = om_group.columns.astype(str)\n",
    "        sns.heatmap(om_group, annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "        rpm = cluster_label_unique_name_mapping_[cluster_label_unique_name_mapping_.cluster_label_unique == om]['rotational speed [RPM]'].values[0]\n",
    "        Nm = cluster_label_unique_name_mapping_[cluster_label_unique_name_mapping_.cluster_label_unique == om]['torque [Nm]'].values[0]\n",
    "        ax.set_title(f'OM {om}, ({rpm} rpm, {Nm} Nm))')\n",
    "        ax.set_xlabel('component')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "What follows in here can be seen as additional exploration. \n",
    "It is not necessary to understand the anomaly detection work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between fingerprints\n",
    "\n",
    "Below, we visualize the pairwise distance between the fingerprints. We observe that the fingerprints are very similar in some cases, when the operating conditions are similar. For instance, fingerprints 0-4 are similar, as they express the vibrations at 100 rpm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances = []\n",
    "fingerprints_ = fingerprints_folds[fold]\n",
    "for om1 in fingerprints_:\n",
    "    fp1 = fingerprints_[om1]\n",
    "    for om2 in fingerprints_:\n",
    "        fp2 = fingerprints_[om2]\n",
    "        dist_ = distance_metrics.cosine_distance(fp1, fp2)\n",
    "        pairwise_distances.append({'om1': om1, 'om2': om2, 'dist': dist_})\n",
    "df_pairwise_dist = pd.DataFrame(pairwise_distances)\n",
    "\n",
    "df_plot = df_pairwise_dist.pivot(columns=\"om1\", index=\"om2\", values=\"dist\")\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "sns.heatmap(df_plot, ax=ax, cmap='Blues', annot=False, fmt=\".2f\")\n",
    "ax.set_title(f\"Cosine distance between fingerpints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing consensus fingerprints\n",
    "\n",
    "A possible extension of our approach is to utilize consensus fingerprints.\n",
    "Consensus fingerprints are constructed by summarizing similar fingerprints by clustering.\n",
    "This idea was not used in the anomaly detection work flow, but is presented here for completeness.\n",
    "\n",
    "Below we show the dendrograms for clustering the fingerprints using hierarchical clustering with 4 different linkage methods. The dendrograms illustrate how the fingerprints are clustered based on their similarity. The fingerprints are clustered based on the cosine distance between the fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints_ = fingerprints_folds[fold]\n",
    "\n",
    "def plot_dendrogram(linkage_matrix, ax=None, **kwargs):\n",
    "    dendrogram(linkage_matrix, ax=ax, **kwargs)\n",
    "    ax.set_title('Hierarchical Clustering Dendrogram')\n",
    "    ax.set_xlabel('Samples')\n",
    "    ax.set_ylabel('Distance')\n",
    "    return ax\n",
    "\n",
    "fingerprints_feature_space = np.vstack(pd.Series(fingerprints_).apply(lambda df: df.stack().values).to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15, 10), ncols=2, nrows=2, sharey=True)\n",
    "axes = axes.flatten()\n",
    "fig.suptitle('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "# Single linkage clustering\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='single')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[0])\n",
    "ax.set_title('Single-link (nearest point)')\n",
    "\n",
    "# Complete linkage clustering\n",
    "linkage_matrix_ward = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='complete')\n",
    "ax = plot_dendrogram(linkage_matrix_ward, ax=axes[1])\n",
    "ax.set_title('Complete-link (farthest point)')\n",
    "\n",
    "# Average linkage clustering: WPGMA\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='weighted')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[2])\n",
    "ax.set_title('Average-link (WPGMA)')\n",
    "\n",
    "# Average linkage clustering: UPGMA\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='average')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[3])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Average-link (UPGMA): preferred method')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we choose the average linkage method, we can see that the fingerprints are clustered into 4 clusters using a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to determine the number of clusters (you can adjust this threshold as needed)\n",
    "threshold = 0.5\n",
    "\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='average')\n",
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=ax, color_threshold=threshold, above_threshold_color='k')\n",
    "ax.set_title('Average-link (UPGMA)')\n",
    "ax.axhline(y=threshold, c='k', ls='--', lw=1)\n",
    "\n",
    "# Get the cluster labels based on the threshold\n",
    "cluster_labels = fcluster(linkage_matrix_avg, threshold, criterion='distance')\n",
    "# generate dictionary where each operating mode is mapped to the respective group and save locally\n",
    "cluster_labels_dict = dict(zip(list(string.ascii_uppercase[0:len(cluster_labels)]), cluster_labels))\n",
    "#if CACHE_RESULTS:\n",
    "if False:\n",
    "    with open('cluster_labels_dict.pickle', 'wb') as fp:\n",
    "        pickle.dump(cluster_labels_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible clustering of the fingerprints is shown below.\n",
    "Each row is a fingerprint and each column is a component of the fingerprint (over all three directions).\n",
    "For instance, the fingerprints 0-4 which were previously shown to be very similar, as they all express vibrations at 100 rpm, would be clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_color = {1: 'blue', 2: 'red', 3: 'green', 4: 'orange', 5: 'purple', 6: 'brown', 7: 'pink', 8: 'gray', 9: 'olive', 10: 'cyan'}\n",
    "\n",
    "dfs_ = {om: fingerprints_[om].values.flatten() for om in fingerprints_}\n",
    "fig, ax = plt.subplots(figsize=(4, 18))\n",
    "ax.set_title('Fingerprinting featurespace with corresponding cluster', fontsize=32)\n",
    "\n",
    "df = pd.DataFrame(dfs_).T\n",
    "sns.heatmap(df, ax=ax, cmap='Blues', annot=False, fmt=\".2f\", norm=LogNorm())\n",
    "texts = []\n",
    "for tick, cluster_label_ in zip(ax.get_yticklabels(), cluster_labels):\n",
    "    tick.set_color(tick_color[cluster_label_])\n",
    "    texts.append(f'{tick.get_text()} ({cluster_label_})')\n",
    "ax.set_xlabel('Component', fontsize=24)\n",
    "ax.set_ylabel('Fingerprint (cluster-id)', fontsize=24)\n",
    "ax.set_yticklabels(texts, rotation=0, fontsize=16);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â©, 2023, Sirris"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHM North America challenge '23\n",
    "\n",
    "# Offline vibration fingerprint extraction\n",
    "\n",
    "This notebook constructs the context-sensitive vibration fingerprints from the vibration data that was previously preprocessed in [02_data_processing.ipynb](02_data_processing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "To run the offline fingerprint extraction, following requirements must be met:\n",
    "\n",
    "| Name | Abbreviated | Format | Used for |\n",
    "|------|-------------|--------|----------|\n",
    "| Decomposition matrix | $\\mathbf{V}$ | $\\mathbb{R}^{n\\times m}$ | Matrix with preprocessed vibrations that are subsequently disaggregated. $n$ vibration measurements, $m$ frequency bins. In this notebook series, we create 100 separate folds, hence there are 100 decomposition matrices stored in `df_V_train_folds`. |\n",
    "| Meta data |  | pandas dataframe | Contains for each of the $n$ vibration measurements additional information, such as the vibration measurement direction or the pitting level used in the experiment. Stored in `meta_data_train_folds`. |\n",
    "| Operational data (included in meta data) | | | Features which indicate in which operating mode the asset is running. Normally, this would be a timeseries independently sampled from the vibration measurements. As in PHM North America, all samples were sampled at static operating conditions, there are $n$ process measurements. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from conscious_engie_icare.data.phm_data_handler import CACHING_FOLDER_NAME, fetch_and_unzip_data, \\\n",
    "                                                        load_data, BASE_PATH_HEALTHY, BASE_PATHS_TEST, PITTING_LEVELS, \\\n",
    "                                                        FPATH_DISTANCES\n",
    "from conscious_engie_icare.nmf_profiling import extract_nmf_per_number_of_component, extract_nmf_incremental, \\\n",
    "                                                derive_df_vib, derive_df_orders\n",
    "from conscious_engie_icare.viz.viz import illustrate_nmf_components_for_paper, illustrate_nmf_components_interactive\n",
    "from conscious_engie_icare import distance_metrics\n",
    "from conscious_engie_icare.normalization import normalize_1\n",
    "from conscious_engie_icare.util import calculate_roc_characteristics, calc_tpr_at_fpr_threshold, calc_fpr_at_tpr_threshold\n",
    "from conscious_engie_icare.preprocessing import get_and_preprocess_healthy_data, get_and_preprocess_unhealthy_data\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import string\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import kneed\n",
    "from ipywidgets import interact, Layout\n",
    "import ipywidgets as widgets\n",
    "from copy import deepcopy\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import re\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ignore convergence warnings (1000 iterations reached by NMF)\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `CACHE_RESULTS` to `True` to cache the results of the feature extraction process. This will speed up the notebook execution time in subsequent executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute = True\n",
    "df_V_train, meta_data_train, df_data_healthy_test, f = get_and_preprocess_healthy_data(recompute=recompute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [previously preprocessed](02_data_processing.ipynb) vibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of frequency band columns for later usage\n",
    "cols_ = df_V_train.columns\n",
    "BAND_COLS = cols_[cols_.str.contains('band')].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)\n",
    "\n",
    "Below, we decompose the vibration decomposition matrix $\\mathbf{V}$ into a weight matrix $\\mathbf{W}$ and a component matrix $\\mathbf{H}$ for each of the 100 folds.\n",
    "We decompose the matrix into up to 40 components, i.e. there are around 4000 different non-negative matrix factorizations performed.\n",
    "As the cell takes around 20 minutes to execute, we cache the results to avoid re-executing it in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Cell with function that extracts NMF profiles. No interactivity or plots, so just a button to click that starts the extraction.\n",
    " \n",
    " > Using 20 components now takes 10 minutes. Too long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMPUTE = True        # only set to True, if NMF should be recomputed\n",
    "MAX_N_COMPONENTS = 40    # maximum number of components used to recompute\n",
    "\n",
    "fpath = os.path.join(CACHING_FOLDER_NAME, 'df_nmf_models', f'df_nmf_models.pkl')\n",
    "if not os.path.exists(fpath) or RECOMPUTE:\n",
    "    df_nmf_models = extract_nmf_incremental(\n",
    "        df_V_train, max_n_components=MAX_N_COMPONENTS, timestamps=df_V_train.index, verbose=True)\n",
    "    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "    pickle.dump(df_nmf_models, open(fpath, 'wb'))\n",
    "\n",
    "else:\n",
    "    # load data from disk\n",
    "    fpath = os.path.join(CACHING_FOLDER_NAME, 'df_nmf_models', f'df_nmf_models.pkl')\n",
    "\n",
    "    df_nmf_models = pickle.load(open(fpath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we illustrate the decomposition of the vibration data for the first fold.\n",
    "\n",
    "- The left plot shows the decomposition matrix $\\mathbf{V}$ as it was preprocessed in the previous notebook. Each vibration measurement is represented as a row in the matrix, each column represents an order transformed and normalized bin. We stack the 3 different measurement directions on top of each other, resulting in 645 vibration measurements, corresponding to the 215 measurements in the training set.\n",
    "- The middle two plots visualize how well the decomposition matrix $\\mathbf{V}$ can be approximated. \n",
    "    - The top plot illustrates the cumulative explained variance of a principal component analysis (PCA) of the decomposition matrix $\\mathbf{V}$. It serves as an indication of an upper bound for how well the signal could be expressed using PCA.\n",
    "    - The bottom plot illustrates the reconstruction error of the decomposition matrix $\\mathbf{V}$ using the NMF components. The reconstruction error is calculated as the Frobenius norm of the difference between the original matrix and the reconstructed matrix.\n",
    "- The right plot illustrates the component matrix $\\mathbf{H}$. Each individual component expresses a different peak. The number of components is determined by how many components in a in parallel running PCA are needed to explain 95% of the variance in the data, corresponding to 5 components in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plotting the results for the NMF. Output are consists of two parts: the top two plots are the explained variance ratio of the PCA, the second one is the reconstructione error of the NMF. Below that, the NMF components are plot, the exact number of which is variable. \n",
    "\n",
    "> The free options are the fold to plot, as well as the way to determine the number of components (knee point detection, using a minimum variance threshold, or both). The user can also specify the minimum and maximum number of components. If the minimum explained variance is used (or both methods), this threshold can be set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_values = illustrate_nmf_components_interactive(df_V_train, df_nmf_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize across all folds the same number of components. We choose 5 based on the analysis above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = saved_values['n_components']\n",
    "COMPONENT_COLUMNS = list(range(N_COMPONENTS))  # used later\n",
    "model = df_nmf_models[(df_nmf_models.n_components == N_COMPONENTS)].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline vibration fingerprint extraction\n",
    "\n",
    "The process view is simplified in this particular use case.\n",
    "In contrast to the industrial dataset, in this dataset there are no timestamps. \n",
    "However, we know speed and torque for each measurement. \n",
    "Therefore, we merge the fingerprint with the operating mode.\n",
    "Below we plot the weights of a single measurement from the training set.\n",
    "All measurement directions (x, y and z) are stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interactivity idea: Plot that shows the different fingerprints (user can chose the speed and torque, as well as fold).\n",
    "> Also include the individual measurements?\n",
    "> Maybe different kind of visualization so we can show both individual measurements and aggregated fingerprint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train = model.W.reshape(-1, N_COMPONENTS)\n",
    "df_W_train = pd.DataFrame(W_train)\n",
    "#display(f'Fold {i}. Shape: {W_train_.shape}')\n",
    "df_W_train.index = df_V_train.index\n",
    "df_W_train['direction'] = meta_data_train['direction']\n",
    "\n",
    "# add operating mode (OM)\n",
    "df_W_train_with_OM = pd.merge(df_W_train, meta_data_train.drop(columns=['direction']), left_index=True, right_index=True)\n",
    "\n",
    "# illustrate weights of single measurement \n",
    "rpm=100\n",
    "torque=500\n",
    "run=1\n",
    "\n",
    "df_ = df_W_train_with_OM[(df_W_train_with_OM['rotational speed [RPM]']==rpm) &\n",
    "                          (df_W_train_with_OM['torque [Nm]']==torque) &\n",
    "                          (df_W_train_with_OM['sample_id']==run)]\n",
    "df_ = df_.set_index('direction')\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(df_[list(range(N_COMPONENTS))], annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "ax.set_title(f'Measurement {run} @ {rpm} rpm, {torque} Nm');\n",
    "ax.set_xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **context-sensitive vibration fingerprint** is the aggregation over all measurements of a given operating mode.\n",
    "Below, we plot the context-sensitive vibration fingerprint for the same operating mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_W_train_with_OM[(df_W_train_with_OM['rotational speed [RPM]']==rpm) & (df_W_train_with_OM['torque [Nm]']==torque)]\n",
    "df_ = df_[list(range(N_COMPONENTS)) + ['direction']].groupby('direction').mean()\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(df_, annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "ax.set_title(f'Vibration fingerprint @ {rpm} rpm, {torque} Nm');\n",
    "ax.set_xlabel('component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the vibration fingerprint and the measurement are very similar, due to at least following reasons: \n",
    "- As the measurements were taken in a controlled environment, the operating mode is constant for each operating condition.\n",
    "- There are very few measurements per operating mode. Therefore, the fingerprint is influenced to a high degree by the single measurement.\n",
    "\n",
    "The vibration fingerprint serves as key component of the context-sensitive anomaly detection.\n",
    "\n",
    "In what follows we construct the vibration fingerprint for each operating mode in the training set, where we treat each unique combination of speed and torque as a separate operating mode. We then aggregate the fingerprints for each operating mode. The 73 fingerprints are visualized below. Note that the number of fingerprints may vary depending on the fold, as not all operating modes are present in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This should be merged with the above, since that allows for the user to plot a specific fingerprint. We can add the option to plot all of them.\n",
    "Computing them should be done prior to any plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_FINGERPRINTS = True   # set SHOW_FINGERPRINTS to False, if visualization should not be shown\n",
    "\n",
    "# for each unique combination of RPM and torque, assign a unique cluster label\n",
    "\n",
    "df_W_train_with_OM['cluster_label_unique'] = df_W_train_with_OM.groupby(['rotational speed [RPM]', 'torque [Nm]']).ngroup()\n",
    "cluster_label_unique_name_mapping = df_W_train_with_OM.groupby('cluster_label_unique').first()[['rotational speed [RPM]', 'torque [Nm]']].reset_index()\n",
    "\n",
    "# extract operating mode wise fingerprints\n",
    "grouping_vars = ['direction', 'cluster_label_unique']\n",
    "df_ = df_W_train_with_OM[COMPONENT_COLUMNS + grouping_vars].copy()\n",
    "fingerprints = {\n",
    "    om: om_group.groupby(['direction']).mean().drop(columns=['cluster_label_unique']) for om, om_group in df_.groupby('cluster_label_unique')\n",
    "}\n",
    "\n",
    "\n",
    "# illustrate fingerprints\n",
    "if SHOW_FINGERPRINTS:\n",
    "    nrows = math.ceil(len(fingerprints) / 3)\n",
    "    fig, axes = plt.subplots(figsize=(15, 2*nrows), nrows=nrows, ncols=3, sharex=True, sharey=True)\n",
    "    for om, ax in tqdm(zip(fingerprints, axes.flat), total=len(fingerprints), desc='Plotting fingerprints'):\n",
    "        om_group = fingerprints[om]\n",
    "        om_group.columns = om_group.columns.astype(str)\n",
    "        sns.heatmap(om_group, annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=0.1, cbar=False)\n",
    "        rpm = cluster_label_unique_name_mapping[cluster_label_unique_name_mapping.cluster_label_unique == om]['rotational speed [RPM]'].values[0]\n",
    "        Nm = cluster_label_unique_name_mapping[cluster_label_unique_name_mapping.cluster_label_unique == om]['torque [Nm]'].values[0]\n",
    "        ax.set_title(f'OM {om}, ({rpm} rpm, {Nm} Nm))')\n",
    "        ax.set_xlabel('component')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional exploration\n",
    "\n",
    "What follows in here can be seen as additional exploration. \n",
    "It is not necessary to understand the anomaly detection work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between fingerprints\n",
    "\n",
    "Below, we visualize the pairwise distance between the fingerprints. We observe that the fingerprints are very similar in some cases, when the operating conditions are similar. For instance, fingerprints 0-4 are similar, as they express the vibrations at 100 rpm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances = []\n",
    "for om1 in fingerprints:\n",
    "    fp1 = fingerprints[om1]\n",
    "    for om2 in fingerprints:\n",
    "        fp2 = fingerprints[om2]\n",
    "        dist_ = distance_metrics.cosine_distance(fp1, fp2)\n",
    "        pairwise_distances.append({'om1': om1, 'om2': om2, 'dist': dist_})\n",
    "df_pairwise_dist = pd.DataFrame(pairwise_distances)\n",
    "\n",
    "df_plot = df_pairwise_dist.pivot(columns=\"om1\", index=\"om2\", values=\"dist\")\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "sns.heatmap(df_plot, ax=ax, cmap='Blues', annot=False, fmt=\".2f\")\n",
    "ax.set_title(f\"Cosine distance between fingerpints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing consensus fingerprints\n",
    "\n",
    "A possible extension of our approach is to utilize consensus fingerprints.\n",
    "Consensus fingerprints are constructed by summarizing similar fingerprints by clustering.\n",
    "This idea was not used in the anomaly detection work flow, but is presented here for completeness.\n",
    "\n",
    "Below we show the dendrograms for clustering the fingerprints using hierarchical clustering with 4 different linkage methods. The dendrograms illustrate how the fingerprints are clustered based on their similarity. The fingerprints are clustered based on the cosine distance between the fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(linkage_matrix, ax=None, **kwargs):\n",
    "    dendrogram(linkage_matrix, ax=ax, **kwargs)\n",
    "    ax.set_title('Hierarchical Clustering Dendrogram')\n",
    "    ax.set_xlabel('Samples')\n",
    "    ax.set_ylabel('Distance')\n",
    "    return ax\n",
    "\n",
    "fingerprints_feature_space = np.vstack(pd.Series(fingerprints).apply(lambda df: df.stack().values).to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15, 10), ncols=2, nrows=2, sharey=True)\n",
    "axes = axes.flatten()\n",
    "fig.suptitle('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "# Single linkage clustering\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='single')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[0])\n",
    "ax.set_title('Single-link (nearest point)')\n",
    "\n",
    "# Complete linkage clustering\n",
    "linkage_matrix_ward = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='complete')\n",
    "ax = plot_dendrogram(linkage_matrix_ward, ax=axes[1])\n",
    "ax.set_title('Complete-link (farthest point)')\n",
    "\n",
    "# Average linkage clustering: WPGMA\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='weighted')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[2])\n",
    "ax.set_title('Average-link (WPGMA)')\n",
    "\n",
    "# Average linkage clustering: UPGMA\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='average')\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=axes[3])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Average-link (UPGMA): preferred method')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we choose the average linkage method, we can see that the fingerprints are clustered into 4 clusters using a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to determine the number of clusters (you can adjust this threshold as needed)\n",
    "threshold = 0.5\n",
    "\n",
    "linkage_matrix_avg = linkage(pdist(fingerprints_feature_space, metric='cosine'), optimal_ordering=True, method='average')\n",
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax = plot_dendrogram(linkage_matrix_avg, ax=ax, color_threshold=threshold, above_threshold_color='k')\n",
    "ax.set_title('Average-link (UPGMA)')\n",
    "ax.axhline(y=threshold, c='k', ls='--', lw=1)\n",
    "\n",
    "# Get the cluster labels based on the threshold\n",
    "cluster_labels = fcluster(linkage_matrix_avg, threshold, criterion='distance')\n",
    "# generate dictionary where each operating mode is mapped to the respective group and save locally\n",
    "cluster_labels_dict = dict(zip(list(string.ascii_uppercase[0:len(cluster_labels)]), cluster_labels))\n",
    "#if CACHE_RESULTS:\n",
    "if False:\n",
    "    with open('cluster_labels_dict.pickle', 'wb') as fp:\n",
    "        pickle.dump(cluster_labels_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible clustering of the fingerprints is shown below.\n",
    "Each row is a fingerprint and each column is a component of the fingerprint (over all three directions).\n",
    "For instance, the fingerprints 0-4 which were previously shown to be very similar, as they all express vibrations at 100 rpm, would be clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_color = {1: 'blue', 2: 'red', 3: 'green', 4: 'orange', 5: 'purple', 6: 'brown', 7: 'pink', 8: 'gray', 9: 'olive', 10: 'cyan'}\n",
    "\n",
    "dfs_ = {om: fingerprints[om].values.flatten() for om in fingerprints}\n",
    "fig, ax = plt.subplots(figsize=(4, 18))\n",
    "ax.set_title('Fingerprinting featurespace with corresponding cluster', fontsize=32)\n",
    "\n",
    "df = pd.DataFrame(dfs_).T\n",
    "sns.heatmap(df, ax=ax, cmap='Blues', annot=False, fmt=\".2f\", norm=LogNorm())\n",
    "texts = []\n",
    "for tick, cluster_label_ in zip(ax.get_yticklabels(), cluster_labels):\n",
    "    tick.set_color(tick_color[cluster_label_])\n",
    "    texts.append(f'{tick.get_text()} ({cluster_label_})')\n",
    "ax.set_xlabel('Component', fontsize=24)\n",
    "ax.set_ylabel('Fingerprint (cluster-id)', fontsize=24)\n",
    "ax.set_yticklabels(texts, rotation=0, fontsize=16);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing test data\n",
    "\n",
    "In order to create a balanced test set, we first load the process and vibration data of the test set that exhibit a high level of pitting and merge it with the healthy data which was not used in the training set.\n",
    "\n",
    "The test data consists of two different conditions:\n",
    "1. **Anomaly condition**: Pitting level 1-8. For each level of pitting, there are between 267 and 304 samples in the test set that were recorded at different speeds and torques.\n",
    "2. **Normal condition**: Healthy data\n",
    "\n",
    "The test data is preprocessed in the same way as the training data, including\n",
    "1. Conversion from time to frequency domain\n",
    "2. Order transformation and binning\n",
    "3. Frequency-band normalization\n",
    "\n",
    "As these steps were all already explained in the [previous notebook on data preprocessing](02_data_processing.ipynb), we will not further go into detail again.\n",
    "The cell below preprocesses the test data in the same way.\n",
    "As a result, two lists of dataframes are constructed: \n",
    "1. `df_orders_test_folds`, where each dataframe contains the frequency transformed measurements, \n",
    "2. `meta_data_test_folds`, where each dataframe contains the process data and some additional information on the measurements.\n",
    "An excerpt is shown below or the first fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompute = False\n",
    "df_orders_test, meta_data_test = get_and_preprocess_unhealthy_data(df_data_healthy_test, f, recompute=recompute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive vibration weights\n",
    "\n",
    "After the vibration data of the test set was preprocessed in the same way as the training data, the weight matrix $\\mathbf{W}$ is extracted in this section.\n",
    "In the subsequent anomaly detection, $\\mathbf{W}$ will be compared to the context-sensitive fingerprint of the same operating mode.\n",
    "The cell below calculates the weight matrices for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract df_W_offline and df_W_online\n",
    "def extract_vibration_weights_per_measurement_period(measurement_periods, col_names, band_cols, normalization, model, verbose=False):\n",
    "    Ws = []\n",
    "    for period in tqdm(measurement_periods, disable=not verbose, desc='Extracting vibration weights per measurement period'):\n",
    "        assert len(period) == 3, 'should have exactly 3 directions per measurement period'\n",
    "        band_column_names = period.columns[period.columns.str.contains('band_')]\n",
    "        V = period.set_index(['direction'])[band_column_names]  # already normalized\n",
    "        W = model.nmf.transform(V.to_numpy())\n",
    "        W = pd.DataFrame(W, columns=col_names)\n",
    "        Ws.append({\n",
    "            'unique_sample_id': period.unique_sample_id.unique()[0],\n",
    "            'V_normalized': V,\n",
    "            'W': W\n",
    "        })\n",
    "    return pd.DataFrame(Ws)\n",
    "\n",
    "# extract train vibration measurement periods\n",
    "\n",
    "df_V_train[['unique_sample_id', 'direction']] = meta_data_train[['unique_sample_id', 'direction']]\n",
    "train_vibration_measurement_periods = []\n",
    "for sample_id, group in df_V_train.groupby('unique_sample_id'):\n",
    "    measurement_period = {\n",
    "        'start': 'unknown', \n",
    "        'stop': 'unknown',\n",
    "        'group': group,\n",
    "        'sample_id': sample_id,\n",
    "    }\n",
    "    train_vibration_measurement_periods.append(group)\n",
    "\n",
    "# extract test vibration measurement periods\n",
    "\n",
    "\n",
    "df_V_test_normalized = normalize_1(df_orders_test, BAND_COLS)\n",
    "df_ = df_V_test_normalized\n",
    "df_[['sample_id', 'unique_sample_id', 'direction']] = meta_data_test[['sample_id', 'unique_sample_id', 'direction']]\n",
    "test_vibration_measurement_periods = []\n",
    "test_vibration_measurement_periods_meta_data = []\n",
    "n_index_errors = 0\n",
    "for unique_sample_id, group in df_.groupby('unique_sample_id'):\n",
    "    rpm = meta_data_test[meta_data_test['unique_sample_id'] == unique_sample_id]['rotational speed [RPM]'].unique()[0]\n",
    "    torque = meta_data_test[meta_data_test['unique_sample_id'] == unique_sample_id]['torque [Nm]'].unique()[0]\n",
    "    try:\n",
    "        om = cluster_label_unique_name_mapping[\n",
    "            (cluster_label_unique_name_mapping['rotational speed [RPM]'] == rpm) & \n",
    "            (cluster_label_unique_name_mapping['torque [Nm]'] == torque)\n",
    "        ]['cluster_label_unique'].iloc[0]\n",
    "    except IndexError:\n",
    "        n_index_errors += 1\n",
    "        om = -1\n",
    "    measurement_period = {\n",
    "        'start': 'unknown', \n",
    "        'stop': 'unknown',\n",
    "        'group': group,\n",
    "        'unique_sample_id': unique_sample_id,\n",
    "        'rpm': rpm,\n",
    "        'torque': torque,\n",
    "        'unique_cluster_label': om\n",
    "    }\n",
    "    test_vibration_measurement_periods.append(group)\n",
    "    test_vibration_measurement_periods_meta_data.append(measurement_period)\n",
    "    \n",
    "n_total = len(test_vibration_measurement_periods)\n",
    "\n",
    "    \n",
    "df_W_offline = extract_vibration_weights_per_measurement_period(train_vibration_measurement_periods, fingerprints[0].columns, BAND_COLS, normalize_1, model)\n",
    "df_W_online = extract_vibration_weights_per_measurement_period(test_vibration_measurement_periods, fingerprints[0].columns, BAND_COLS, normalize_1, model)\n",
    "\n",
    "df_W_online.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the derived weights for a single measurements below. The format of the weights is the same as of the fingerprint shown in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = 5\n",
    "period = 10\n",
    "\n",
    "usid = df_W_online['unique_sample_id'][period]\n",
    "df_ = meta_data_test[meta_data_test['unique_sample_id']==usid]\n",
    "rpm = df_['rotational speed [RPM]'].iloc[0]\n",
    "torque = df_['torque [Nm]'].iloc[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.heatmap(df_W_online['W'][period], annot=True, fmt=\".6f\", ax=ax, cmap='Blues', vmin=0, vmax=0.05, cbar=False)\n",
    "ax.set_title(f'Derived weights for measurement {period} @ {rpm} rpm, {torque} Nm');\n",
    "ax.set_yticklabels(['x', 'y', 'z'], rotation=0)\n",
    "ax.set_ylabel('Measurement direction')\n",
    "ax.set_xlabel('Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection \n",
    "\n",
    "Because the measurements are not timestamped, it is not possible to order the measurements in time and calculate a cumulative anomaly score.\n",
    "Therefore, we perform only the measurement-wise anomaly detection in this section.\n",
    "\n",
    "## Distance to fingerprints\n",
    "\n",
    "Per operating mode, we calculate the distances of the derived weights to the corresponding fingerprint below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell takes around 60 minutes to run (!) --> going to cache the results\n",
    "SHOW_DISTANCES = False\n",
    "\n",
    "def calculate_distances_per_measurement_period(measurement_period, fingerprints, verbose=False):\n",
    "    # pointwise Mahalanobis distance\n",
    "    fingerprint_matrix = np.array([fingerprints[om].to_numpy().flatten() for om in fingerprints])\n",
    "    # calculate covariance matrix\n",
    "    fingerprint_S = np.cov(fingerprint_matrix.T)\n",
    "    # calculate inverse\n",
    "    fingerprint_SI = np.linalg.inv(fingerprint_S)\n",
    "    # calculate mu\n",
    "    fingerprint_mu = fingerprint_matrix.mean(axis=0)\n",
    "    df_dist_ = []\n",
    "    for idx, row in tqdm(measurement_period.iterrows(), total=len(measurement_period), disable=not verbose):\n",
    "        for om in fingerprints:\n",
    "            weights = row['W']\n",
    "            fingerprint = fingerprints[om]\n",
    "            tmp = {\n",
    "                'idx': idx,\n",
    "                'data': row, \n",
    "                'om': om, \n",
    "                #'frobenius_norm': distance_metrics.frobenius_norm(weights, fingerprint),\n",
    "                #'frobenius_norm_pow2': distance_metrics.frobenius_norm_v2(weights, fingerprint),\n",
    "                #'frobenius_norm_sqrt': distance_metrics.frobenius_norm_v3(weights, fingerprint),\n",
    "                'cosine_distance': distance_metrics.cosine_distance(weights, fingerprint),\n",
    "                'manhattan_distance': distance_metrics.manhattan_distance(weights, fingerprint),\n",
    "            }\n",
    "            df_dist_.append(tmp)\n",
    "    df_dist_ = pd.DataFrame(df_dist_)\n",
    "    return df_dist_\n",
    "\n",
    "# calculate/load distances\n",
    "fpath_offline = os.path.join(FPATH_DISTANCES, f'df_dist_offline.pkl')\n",
    "os.makedirs(os.path.dirname(fpath_offline), exist_ok=True)\n",
    "fpath_online = os.path.join(FPATH_DISTANCES, f'df_dist_online.pkl')\n",
    "os.makedirs(os.path.dirname(fpath_online), exist_ok=True)\n",
    "if os.path.exists(fpath_offline) and os.path.exists(fpath_online):\n",
    "    # load cached distances\n",
    "    df_dist_offline = pickle.load(open(fpath_offline, 'rb'))\n",
    "    df_dist_online = pickle.load(open(fpath_online, 'rb'))\n",
    "else:\n",
    "    # calculate distances and cache results\n",
    "    df_dist_offline = calculate_distances_per_measurement_period(df_W_offline, fingerprints=fingerprints)\n",
    "    pickle.dump(df_dist_offline, open(fpath_offline, 'wb'))\n",
    "    df_dist_online = calculate_distances_per_measurement_period(df_W_online, fingerprints=fingerprints)\n",
    "    pickle.dump(df_dist_online, open(fpath_online, 'wb'))\n",
    "\n",
    "\n",
    "# pivot cosine distance:\n",
    "# for each measurement period (row), get the distance to each operating mode (column)\n",
    "df_cosine = df_dist_online[['idx', 'om', 'cosine_distance']].pivot(index='idx', columns='om', values='cosine_distance')\n",
    "# assign the corresponding operating mode to the given row (if known), else, assign -1\n",
    "df_cosine[['rpm', 'torque', 'unique_cluster_label']] = pd.DataFrame(test_vibration_measurement_periods_meta_data)[['rpm', 'torque', 'unique_cluster_label']]\n",
    "\n",
    "g = sns.displot(data=df_dist_offline, \n",
    "                x=\"cosine_distance\", col=\"om\", col_wrap=10, height=1, aspect=2, bins=20, kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the cosine distance as appropriate metric.\n",
    "Below, we create a pivot table of the cosine distance and compare distances between measurements and their corresponding fingerprints (left) and other fingerprints (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_own_cluster_center = []\n",
    "for idx, row in df_cosine.iterrows():\n",
    "    om = row['unique_cluster_label']\n",
    "    if om != -1:\n",
    "        distance_to_own_cluster_center.append(row[om])\n",
    "    else:\n",
    "        distance_to_own_cluster_center.append(np.nan)\n",
    "df_cosine['distance_to_own_cluster_center'] = distance_to_own_cluster_center\n",
    "df_cosine['pitting'] = df_W_online['unique_sample_id'].str.contains(f'pitting_level_')\n",
    "df_cosine['pitting_level'] = df_W_online['unique_sample_id'].str.extract(r'pitting_level_(\\d)')\n",
    "df_cosine['pitting_level'] = df_cosine['pitting_level'].fillna(0).astype(int)\n",
    "\n",
    "print('Pivot table with distances to all fingerprints (0 - 72), corresponding rpm and torque values, and additional information on the anomaly condition:')\n",
    "display(df_cosine.head())\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16, 8), ncols=2)\n",
    "\n",
    "ax = df_cosine['distance_to_own_cluster_center'].plot(kind='hist', bins=20, ax=axes[0], alpha=0.5, legend=False)\n",
    "ax.set_title('Distance to own cluster centers')\n",
    "ax.set_xlabel('Cosine distance')\n",
    "\n",
    "# plot distance to other cluster centers\n",
    "ax = df_cosine.drop(columns=['rpm', 'torque', 'unique_cluster_label', 'distance_to_own_cluster_center', 'pitting', 'pitting_level']).melt()['value'].plot(kind='hist', bins=20, ax=axes[1], alpha=0.5, legend=False)\n",
    "ax.set_title('Distance to other cluster centers')\n",
    "ax.set_xlabel('Cosine distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine distance to the context-sensitive fingerprint is typically higher, when pitting is present.\n",
    "This is expected, as the context-sensitive fingerprint is derived from healthy data without pitting and data with pitting is likely to have vibration patterns not present in the healthy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_cosine, y='distance_to_own_cluster_center', x='pitting_level')\n",
    "ax.set_ylabel('Distance to context sensitive fingerprint')\n",
    "ax.set_title(f'Distance to own cluster center per pitting level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-curves\n",
    "\n",
    "Whether a datapoint is labelled as anomalous depends on a predefined distance threshold.\n",
    "In this section, we caclulate ROC-curves by varying this distance threshold.\n",
    "\n",
    "An ROC-curve is calculated per fold. Below, we visualize the ROC curve for the first fold.\n",
    "It can be observed that the anomaly detection generally performs well with a area under the curve (AUC) of $0.962$.\n",
    "Operators aim for a high true positive rate (TPR) while minimizing false alarms (keeping the false positive rate (FPR) low). We track the TPR at a stable FPR of 0.1 (TPR@FPR=0.1), which represents the TPR when there are 10% false positives. In this scenario, the TPR is 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "df_cosine = df_cosine[df_cosine.unique_cluster_label != -1]  # removed unknown cluster labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the general ROC curve\n",
    "fpr, tpr, roc_auc = calculate_roc_characteristics(df_cosine)\n",
    "tpr_at_fpr = calc_tpr_at_fpr_threshold(tpr, fpr, threshold=threshold)\n",
    "ax.plot(fpr, tpr, color='blue', lw=4, label=f'overall (area = {roc_auc:.3f})', alpha=0.66)\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='baseline')\n",
    "ax.plot([0, threshold], [tpr_at_fpr, tpr_at_fpr], color='red', lw=2, linestyle='--', label=f'TPR@FPR={threshold:.2f} = {tpr_at_fpr:.2f}')\n",
    "ax.plot([threshold, threshold], [0, tpr_at_fpr], color='red', lw=2, linestyle='--')\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(f'ROC Curve')\n",
    "n_total = len(df_cosine)\n",
    "n_healthy = len(df_cosine[df_cosine['pitting'] == False])\n",
    "n_unhealthy = len(df_cosine[df_cosine['pitting'] == True])\n",
    "text = f\"n={n_total} ({n_healthy} healthy, {n_unhealthy} unhealthy)\"\n",
    "ax.annotate(xy=(0.1, 0.025), text=text)\n",
    "\n",
    "ax.legend(loc='lower right', title='Pitting severity level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternatve objective criterium is to keep the FPR as low as possible at a high TPR.\n",
    "The plot below illustrates the FPR@TPR=0.1. In the shown fold, FPR@TPR=0.1 is 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cosine = df_cosine[df_cosine.unique_cluster_label != -1]  # removed unknown cluster labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "threshold = 0.90\n",
    "\n",
    "# Plot the general ROC curve\n",
    "fpr, tpr, roc_auc = calculate_roc_characteristics(df_cosine)\n",
    "fpr_at_tpr = calc_fpr_at_tpr_threshold(tpr, fpr, threshold=threshold)\n",
    "ax.plot(fpr, tpr, color='blue', lw=4, label=f'overall (area = {roc_auc:.3f})', alpha=0.66)\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='baseline')\n",
    "ax.plot([fpr_at_tpr, fpr_at_tpr], [0, threshold], color='green', lw=2, linestyle='--', label=f'FPR@TPR={threshold:.2f} = {fpr_at_tpr:.2f}')\n",
    "ax.plot([0, fpr_at_tpr], [threshold, threshold], color='green', lw=2, linestyle='--')\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title(f'ROC Curve')\n",
    "n_total = len(df_cosine)\n",
    "n_healthy = len(df_cosine[df_cosine['pitting'] == False])\n",
    "n_unhealthy = len(df_cosine[df_cosine['pitting'] == True])\n",
    "text = f\"n={n_total} ({n_healthy} healthy, {n_unhealthy} unhealthy)\"\n",
    "ax.annotate(xy=(0.1, 0.025), text=text)\n",
    "\n",
    "ax.legend(loc='lower right', title='Pitting severity level');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we illustrated our contextual anomaly detection method on the example of a gearbox that is subject to pitting.\n",
    "We illustrated how to construct context-sensitive fingerprints and how to utilize the fingerprints for anomaly detection.\n",
    "We showed that our method is capable to detect anomalies in the test set while keeping the amount of false positives low.\n",
    "In a real-world setting, additional steps have to be taken to estimate the underlying operating mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â©, 2023, Sirris"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

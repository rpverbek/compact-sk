{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "TRAIN_OPERATIONAL_PATH = 'train_operational_readouts.csv'\n",
    "TRAIN_REPAIR_PATH = 'train_tte.csv'\n",
    "TRAIN_SPECIFICATIONS = 'train_specifications.csv' \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import kahypar\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from conscious_engie_icare import hypergraph_clustering\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_operational_readouts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_ts \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_OPERATIONAL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_ts\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/envs/compact/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/compact/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/compact/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/compact/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/compact/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_operational_readouts.csv'"
     ]
    }
   ],
   "source": [
    "df_ts = pd.read_csv(TRAIN_OPERATIONAL_PATH)\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries are monotonically increasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_ts[df_ts.vehicle_id == 0]\n",
    "ax = df_['171_0'].plot();\n",
    "ax.set_title('171_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_ts[df_ts.vehicle_id == 2]\n",
    "df_['171_0'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vehicle_ids = len(df_ts.vehicle_id.unique())\n",
    "print(f'Number of unique vehicles: {unique_vehicle_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_ts.vehicle_id.value_counts().plot.hist(bins=100)\n",
    "ax.set_title('Number of samples per vehicle')\n",
    "ax.set_xlabel('Samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifications data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specifications = pd.read_csv(TRAIN_SPECIFICATIONS, index_col='vehicle_id')\n",
    "df_specifications.iloc[5000:5005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename categories to avoid confusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_prefix(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: f'{col}_{x}')\n",
    "    return df\n",
    "\n",
    "df_specifications = add_column_prefix(df_specifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique categories differs between specifications.\n",
    "Below, we illustrate the distribution of categories.\n",
    "Note that for most specifications are highly imbalanced, with one category labelling the majority of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(20, 5), nrows=2, ncols=4, sharey=True)\n",
    "for spec, ax in zip(range(8), axes.flatten()):\n",
    "    df_specifications[f'Spec_{spec}'].value_counts().plot.bar(ax=ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig('distribution_specifications.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specifications['Spec_1'].value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_specifications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_specifications['Spec_0'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_specifications['Spec_1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_specifications['Spec_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_specifications['Spec_3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_specifications['Spec_4'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repair = pd.read_csv(TRAIN_REPAIR_PATH)\n",
    "df_repair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_repair.copy()\n",
    "df_['in_study_repair'] = df_repair['in_study_repair'].replace({0: 'False', 1: 'True'})\n",
    "ax = df_.in_study_repair.value_counts().plot.bar()\n",
    "ax.set_title('vehicle repairs')\n",
    "ax.set_ylabel('vehicles')\n",
    "ax.set_xlabel('Was repaired?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods: Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal concept analysis (FCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following line, where a concept lattice is constructed takes too long to run (because there are 3607 unique configurations and hence at least 3607 concepts over 23550 objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df_transformed = pd.get_dummies(df_specifications, columns=df_specifications.columns, drop_first=False)\n",
    "    print(f'original shape = {df_specifications.shape}')\n",
    "    print(f'transformed shape = {df_transformed.shape}')\n",
    "    cluster_table = df_transformed.to_numpy()\n",
    "    formal_context = FormalContext(cluster_table)\n",
    "    display(df_transformed.head())\n",
    "\n",
    "    n_unique_configurations = len(set([tuple(row) for row in df_transformed.to_numpy()]))\n",
    "    print(\"Number of unique combinations:\", n_unique_configurations)\n",
    "\n",
    "    L = ConceptLattice.from_context(formal_context)\n",
    "    print(\"# concepts:\", len(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "nodes_hyper = df_specifications.index.tolist()\n",
    "layers = df_specifications.to_dict(orient='series')\n",
    "for k, v in layers.items():\n",
    "    v.name = 'spec'\n",
    "\n",
    "# Create an *ordered* dictionary by iterating over each DataFrame, with\n",
    "# {cat_idx: [vehicle_ids]}\n",
    "category_to_vehicle_ids = OrderedDict()\n",
    "for spec_idx, df in layers.items():\n",
    "    for cat_idx, group in df.reset_index().groupby('spec', sort=False):\n",
    "        category_to_vehicle_ids[cat_idx] = group['vehicle_id'].tolist()\n",
    "\"\"\"\n",
    "category_to_vehicle_ids = {cat_idx: group['vehicle_id'].tolist()\n",
    "                           for spec_idx, df in layers.items()\n",
    "                           for cat_idx, group in df.reset_index().groupby('spec')}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Number of categories in total\", len(category_to_vehicle_ids))\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Create a list of hyperedge indices, where\n",
    "# each index indicates the start/stop if a segment of trucks belonging to the given category\n",
    "hyperedge_indices = []\n",
    "cnt = 0\n",
    "hyperedge_indices.append(cnt)\n",
    "for each in category_to_vehicle_ids.values():\n",
    "    cnt = cnt+len(each)\n",
    "    hyperedge_indices.append(cnt)\n",
    "\n",
    "# convert category_to_vehicle_ids to \n",
    "# (1) flat list = flat list of vehicle id's (as originally given in df_specifications), and \n",
    "# (2) hyperedges = indices of list of vehicle id's\n",
    "flat_list_ = [vehicle_id for vehicle_id_list in category_to_vehicle_ids.values() for vehicle_id in vehicle_id_list]   # 188400\n",
    "hyperedges = [nodes_hyper.index(i) for i in flat_list_]\n",
    "print(\"Number of category-truck combinations: \", len(hyperedges))\n",
    "print(\"-----------------------------------------------------\")\n",
    "num_nodes = len(nodes_hyper)\n",
    "num_nets = len(hyperedge_indices)-1\n",
    "\n",
    "k = 2\n",
    "# edge and node weight are not considered as they are not relavent in our case.\n",
    "# k values does not have a significance as we are not partitioning the hypergraph\n",
    "# using the algorithm.\n",
    "hypergraph = kahypar.Hypergraph(\n",
    "    num_nodes,                      # 23550\n",
    "    num_nets,                       # 90\n",
    "    index_vector=hyperedge_indices, # 91 ...\n",
    "    edge_vector=hyperedges,         # 188400\n",
    "    k=k                             # 2, no significance\n",
    ")\n",
    "context = kahypar.Context()\n",
    "print(\"number of edges of hyper-graph\", hypergraph.numEdges())\n",
    "# Clusters list:\n",
    "# Category with single truck is not considered as hyperedge.\n",
    "cluster_list = [cat_idx for cat_idx, sublist in category_to_vehicle_ids.items() if len(sublist) > 1]\n",
    "print(\"Unique categories with respect to edges: \", len(cluster_list))\n",
    "print(\"number of nodes of hyper-graph\", hypergraph.numNodes())\n",
    "print(\"number of pins of hyper-graph\", hypergraph.numPins())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mapping between naming convention of KaHyPar and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(hypergraph.edges())) == len(cluster_list)\n",
    "edge_id_to_category_id = {edge_id: category_id for edge_id, category_id in zip(hypergraph.edges(), cluster_list)}\n",
    "\n",
    "# Check if the renaming is correct:\n",
    "# If the code below runs without exceptions, then it is guaranteed that \n",
    "# (1) the order of nodes in the hypergraph (0, 1, 2, ...) corresponds to the order of trucks in df_specifications,\n",
    "#     i.e., first row of df_specifications corresponds to first row of nodes\n",
    "# (2) The renaming introduced in edge_id_to_category_id is correct.\n",
    "for i, (truck_id, specifications) in enumerate(df_specifications.iterrows()):\n",
    "    incindent_edges_ = list(hypergraph.incidentEdges(i))\n",
    "    specifications_ = specifications.tolist()\n",
    "    renamed_indicent_edges_ = [edge_id_to_category_id[e] for e in incindent_edges_]\n",
    "    msg = f\"Specification names don't match in line {i}\\n\" + \\\n",
    "          f\"renamed_indicent_edges: {renamed_indicent_edges_}\\n\" + \\\n",
    "          f\"specifications: {specifications_}\"\n",
    "    if len(renamed_indicent_edges_) == 8:\n",
    "        assert renamed_indicent_edges_ == specifications_, msg\n",
    "    else:\n",
    "        print(f\"Not considering truck {i}, since it has unique specification category.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i, (truck_id, specifications) in enumerate(df_specifications.iterrows()):\n",
    "    incindent_edges_ = list(hypergraph.incidentEdges(i))\n",
    "    renamed_indicent_edges = [edge_id_to_category_id[e] for e in incindent_edges_]\n",
    "    assert renamed_indicent_edges == specifications.tolist(), f\"Specification names don't match in line {i}\"\n",
    "\n",
    "renamed_indicent_edges\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the neighbourhood of each edge:\n",
    "\n",
    "For each hyperedge n_i, identify its set of neigbours. A hyperedge is considered a neighbor of another hyperedge, when they have at least one common sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_intersection_non_empty = lambda x, y: len(set(hypergraph.pins(x)).intersection(set(hypergraph.pins(y)))) > 0\n",
    "neighbourhood_ = {ni: set([nj for nj in hypergraph.edges() if check_if_intersection_non_empty(ni, nj)]) for ni in hypergraph.edges()}\n",
    "# rename neighbourhood and number_neighbours according to category names\n",
    "neighbourhood = {edge_id_to_category_id[k]: {edge_id_to_category_id[vi] for vi in v} for k, v in neighbourhood_.items()}\n",
    "number_neighbours = {ni: len(neighbours) for ni, neighbours in neighbourhood.items()}\n",
    "#print(\"number of neighbours\", number_neighbours)\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "pd.Series(number_neighbours).plot.bar(ax=ax);\n",
    "ax.set_ylabel('Number of neighbours');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Nearest Neighbourhood Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "NNS = {}\n",
    "for i_cat_idx, i_neighbourhood_edges in neighbourhood.items():   # !!! set of all i_neighbourhood_edges != set of all i_cat_idx\n",
    "    for j_cat_idx, j_neighbourhood_edges in neighbourhood.items():\n",
    "        if i_cat_idx != j_cat_idx:\n",
    "            if verbose:\n",
    "                print(\"checking intersection of \", i_cat_idx, \"and\", j_cat_idx, \n",
    "                      \"i.e.,\", neighbourhood[i_cat_idx], \"and\", neighbourhood[j_cat_idx])\n",
    "            intersection = neighbourhood[i_cat_idx].intersection(neighbourhood[j_cat_idx])\n",
    "            if (i_cat_idx not in intersection) or (j_cat_idx not in intersection): # or condition is not required. if i is in intersection then automatically j will be in the intersection. \n",
    "                if verbose:\n",
    "                    print(i_cat_idx, j_cat_idx, intersection, neighbourhood[i_cat_idx], neighbourhood[j_cat_idx])\n",
    "                NNS[(i_cat_idx, j_cat_idx)] = 0\n",
    "            else:\n",
    "                union = neighbourhood[i_cat_idx].union(neighbourhood[j_cat_idx])\n",
    "                NNS[(i_cat_idx, j_cat_idx)] = len(intersection)/len(union)\n",
    "                if verbose:\n",
    "                    print(union)\n",
    "\n",
    "print(len(NNS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarity score\n",
    "    - 0 means those edges do not overlap directly\n",
    "    - 1 means those edges overlap completely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "columns = []\n",
    "values = []\n",
    "for (idx, col), val in NNS.items():\n",
    "    index.append(idx)\n",
    "    columns.append(col)\n",
    "    values.append(val)\n",
    "df = pd.DataFrame({'Index': index, 'Column': columns, 'Value': values})\n",
    "df_snns = df.pivot(index='Index', columns='Column', values='Value')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "sns.heatmap(df_snns.fillna(1), cmap='viridis')\n",
    "ax.set_title('Nearest neighbor similarity (NNS)')\n",
    "fig.savefig('v2_nns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "fig = px.imshow(df_snns.fillna(1), \n",
    "                color_continuous_scale='viridis', \n",
    "                title='Nearest neighbor similarity (NNS)',\n",
    "                labels=dict(x=\"Columns\", y=\"Rows\", color=\"Similarity\"))\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    width=1000, \n",
    "    height=900,\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "# Show the heatmap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(df_snns.fillna(0) > 0.5, \n",
    "                color_continuous_scale='viridis', \n",
    "                title='Adjacency matrix',\n",
    "                labels=dict(x=\"Columns\", y=\"Rows\", color=\"Similarity\"))\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    width=1000, \n",
    "    height=900,\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "# Show the heatmap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_snns.fillna(0) > 0.5\n",
    "G = nx.Graph(df_)\n",
    "options = {\n",
    "    \"font_size\": 6,\n",
    "    \"font_color\": 'grey',\n",
    "    \"node_size\": 12,\n",
    "    \"with_labels\": True,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"blue\",\n",
    "    #\"linewidths\": 5,\n",
    "    #\"width\": 5,\n",
    "}\n",
    "pos = nx.spring_layout(G, k=0.2, seed=648)\n",
    "#pos = nx.multipartite_layout(G, subset_key=layers) # TODO\n",
    "fig, ax = plt.subplots(figsize=(21,21))\n",
    "nx.draw(G, pos, ax=ax, **options)\n",
    "fig.savefig('graph_0.5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Spec_7_Cat4' in G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_snns.fillna(0) > 0.6\n",
    "G = nx.Graph(df_)\n",
    "options = {\n",
    "    \"font_size\": 6,\n",
    "    \"font_color\": 'grey',\n",
    "    \"node_size\": 12,\n",
    "    \"with_labels\": True,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"blue\",\n",
    "    #\"linewidths\": 5,\n",
    "    #\"width\": 5,\n",
    "}\n",
    "pos = nx.spring_layout(G, k=0.2, seed=648)\n",
    "#pos = nx.multipartite_layout(G, subset_key=layers) # TODO\n",
    "fig, ax = plt.subplots(figsize=(21,21))\n",
    "nx.draw(G, pos, ax=ax, **options)\n",
    "fig.savefig('graph_0.6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_snns.fillna(0) > 0.7\n",
    "G = nx.Graph(df_)\n",
    "options = {\n",
    "    \"font_size\": 6,\n",
    "    \"font_color\": 'grey',\n",
    "    \"node_size\": 12,\n",
    "    \"with_labels\": True,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"blue\",\n",
    "    #\"linewidths\": 5,\n",
    "    #\"width\": 5,\n",
    "}\n",
    "pos = nx.spring_layout(G, k=0.2, seed=648)\n",
    "#pos = nx.multipartite_layout(G, subset_key=layers) # TODO\n",
    "fig, ax = plt.subplots(figsize=(21,21))\n",
    "nx.draw(G, pos, ax=ax, **options)\n",
    "fig.savefig('graph_0.7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_snns.fillna(0) > 0.8\n",
    "G = nx.Graph(df_)\n",
    "options = {\n",
    "    \"font_size\": 6,\n",
    "    \"font_color\": 'grey',\n",
    "    \"node_size\": 12,\n",
    "    \"with_labels\": True,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"blue\",\n",
    "    #\"linewidths\": 5,\n",
    "    #\"width\": 5,\n",
    "}\n",
    "pos = nx.spring_layout(G, k=0.2, seed=648)\n",
    "#pos = nx.multipartite_layout(G, subset_key=layers) # TODO\n",
    "fig, ax = plt.subplots(figsize=(21,21))\n",
    "nx.draw(G, pos, ax=ax, **options)\n",
    "fig.savefig('graph_0.8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.davis_southern_women_graph()  # Example graph\n",
    "communities = nx.community.greedy_modularity_communities(G)\n",
    "\n",
    "# Compute positions for the node clusters as if they were themselves nodes in a\n",
    "# supergraph using a larger scale factor\n",
    "supergraph = nx.cycle_graph(len(communities))\n",
    "superpos = nx.spring_layout(G, scale=50, seed=429)\n",
    "\n",
    "# Use the \"supernode\" positions as the center of each node cluster\n",
    "centers = list(superpos.values())\n",
    "pos = {}\n",
    "for center, comm in zip(centers, communities):\n",
    "    pos.update(nx.spring_layout(nx.subgraph(G, comm), center=center, seed=1430))\n",
    "\n",
    "# Nodes colored by cluster\n",
    "for nodes, clr in zip(communities, (\"tab:blue\", \"tab:orange\", \"tab:green\")):\n",
    "    nx.draw_networkx_nodes(G, pos=pos, nodelist=nodes, node_color=clr, node_size=100)\n",
    "nx.draw_networkx_edges(G, pos=pos)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "fig = px.imshow(df_snns.fillna(1), \n",
    "                color_continuous_scale='viridis', \n",
    "                title='Nearest neighbor similarity (NNS)',\n",
    "                labels=dict(x=\"Columns\", y=\"Rows\", color=\"Similarity\"))\n",
    "\n",
    "# Update layout for better appearance\n",
    "fig.update_layout(\n",
    "    width=1000, \n",
    "    height=900,\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "# Show the heatmap\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snns.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "\"\"\"\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cnt=0\n",
    "#df_snns=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "df_snns=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "for i in np.arange(0,total_edges,1):\n",
    "    for j in np.arange(0,total_edges,1):\n",
    "        if (i,j) in NNS:\n",
    "            cnt=cnt+1\n",
    "            df_snns.loc[i,j]=NNS[i,j]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_snns.fillna(1), cmap='viridis')\n",
    "ax.set_title('Nearest neighbor similarity (NNS)');\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the similarity matrix into distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNS_dist = {k: 1-v for k, v in NNS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining distance matrix.\n",
    "\n",
    "- More similar (more overlapping) means less distant\n",
    "- Distance metric: distance 1 means no overlap; distance 0 means full overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist = df_snns.map(lambda x: 1-x).fillna(1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_dist, cmap='viridis')\n",
    "ax.set_title('Distance matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dist = df_dist.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code replaced with code above\n",
    "\"\"\"\n",
    "array_dist = []\n",
    "for i in range(total_edges):\n",
    "    temp = []\n",
    "    for j in range(total_edges):\n",
    "        if i != j:\n",
    "            temp.append(NNS_dist[(min(i, j), max(i,j))])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    array_dist.append(temp)\n",
    "\n",
    "df_dist=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "for i in np.arange(0,total_edges,1):\n",
    "    for j in np.arange(0,total_edges,1):\n",
    "        # if (i<=j):\n",
    "        df_dist.loc[i,j]=array_dist[i][j]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(df_dist.fillna(1), cmap='viridis')\n",
    "ax.set_title('Distance matrix');\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_method = 'complete'\n",
    "distance_metric = 'precomputed'\n",
    "\n",
    "X = np.array(array_dist)\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None,  metric=distance_metric, linkage=linkage_method)\n",
    "model = model.fit(X)\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "hypergraph_clustering.plot_dendrogram(model, truncate_mode=\"level\", p=50, ax=ax)\n",
    "ax.set_title(f\"Hierarchical Clustering Dendrogram ({linkage_method} linkage)\")\n",
    "ax.set_xlabel(\"Unique specification Category\")\n",
    "ax.set_ylabel(f\"{distance_metric} linkage distance\");\n",
    "\n",
    "# Replace the tick labels\n",
    "new_labels = [edge_id_to_category_id[int(label.get_text())] for label in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(new_labels)\n",
    "fig.tight_layout()\n",
    "fig.savefig('v2_clustering_dendrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(distance_threshold=0.7, n_clusters=None,  metric=distance_metric, linkage=linkage_method)\n",
    "y_agglomerative = model.fit_predict(X)\n",
    "dict_clusters = {y: np.where(y_agglomerative == y)[0].tolist() for y in np.unique(y_agglomerative)}\n",
    "dict_clusters_ = {y: np.where(y_agglomerative == y)[0].tolist() for y in np.unique(y_agglomerative)}\n",
    "dict_clusters = {f'HG_{y}': [edge_id_to_category_id[y] for y in vs] for y, vs in dict_clusters_.items()}\n",
    "print(dict_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_clusters(final_clusters1, hypergraph1, method, Debug=False):\n",
    "    \"\"\" Mapping hyperedges to data objects to obtain the clustering solution of data objects. \"\"\"\n",
    "    temp_del = 0\n",
    "    clustering_nodes = {}\n",
    "    for key, val in final_clusters1.items():\n",
    "        if method == 'donot_inc_key_in_cluster':\n",
    "            pins_center = []\n",
    "        elif method == \"inc_key_in_cluster\":\n",
    "            pins_center = list(hypergraph1.pins(key))\n",
    "        for _ in val:\n",
    "            pins_center.extend(list(hypergraph1.pins(_)))\n",
    "        clustering_nodes[key] = set(pins_center)\n",
    "        temp_del = temp_del + len(set(pins_center))\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering of data objects\", clustering_nodes) # dict, key = center(hyperedge), values = data objects\n",
    "\n",
    "    # replacing the index of the data object with its short id\n",
    "    clus_nodes_short_id = {}\n",
    "    for key, val in clustering_nodes.items():\n",
    "        # print(val)\n",
    "        clus_nodes_short_id[key] = {nodes_hyper[x] for x in val} # note that sets are not ordered\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering solution, key = center (hyperedge), val = set of short_ids\")\n",
    "        print(clus_nodes_short_id)\n",
    "    return clus_nodes_short_id\n",
    "\n",
    "clus_sol = generate_final_clusters(dict_clusters_, hypergraph, 'donot_inc_key_in_cluster', Debug=False)\n",
    "len(clus_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = {y: len(clus_sol[y]) for y in clus_sol} # WHY MORE THAN BEFORE? --> check generate_final_clusters\n",
    "fig, ax = plt.subplots()\n",
    "pd.Series(n_).plot.barh(ax=ax)\n",
    "ax.set_title('Clustering of data objects (data objects can be labelled as multiple objects)')\n",
    "ax.set_xlabel('n')\n",
    "ax.set_ylabel('cluster label')\n",
    "fig.savefig('n_cluster_labels.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per hypergraph cluster, plot distibution of labels present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_dict_of_sets(input_dict):\n",
    "    reversed_dict = {}\n",
    "    for key, values_set in input_dict.items():\n",
    "        for value in values_set:\n",
    "            if value not in reversed_dict:\n",
    "                reversed_dict[value] = []\n",
    "            reversed_dict[value].append(key)\n",
    "    return reversed_dict\n",
    "\n",
    "reversed_dict = reverse_dict_of_sets(clus_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_list_occurrences(reversed_dict):\n",
    "    list_count = {}\n",
    "    for key_list in reversed_dict.values():\n",
    "        key_tuple = tuple(key_list)\n",
    "        if key_tuple not in list_count:\n",
    "            list_count[key_tuple] = 0\n",
    "        list_count[key_tuple] += 1\n",
    "    return list_count\n",
    "\n",
    "list_counts = count_list_occurrences(reversed_dict)\n",
    "list_counts_with_str_key = {str(k): v for k, v in list_counts.items()}\n",
    "fig, ax = plt.subplots(figsize=(8, 120))\n",
    "pd.Series(list_counts_with_str_key).sort_values().plot.barh(ax=ax)\n",
    "ax.set_title('Clustering of data objects (data objects can be labelled in multiple clusters)')\n",
    "ax.set_xlabel('n')\n",
    "ax.set_ylabel('cluster label combinations');\n",
    "fig.savefig('n_unqiue_cluster_label_combinations.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(list_counts_with_str_key)\n",
    "s.sort_values().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucks = df_specifications.copy()\n",
    "df_ = df_repair.copy()\n",
    "df_.index = df_['vehicle_id']\n",
    "df_trucks = pd.merge(df_trucks, df_, left_index=True, right_index=True)\n",
    "df_trucks = df_trucks.drop(columns=['vehicle_id'])\n",
    "df_trucks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_clustering_table(dict_):\n",
    "    # Step 1: Get all unique elements in the lists\n",
    "    unique_elements = set()\n",
    "    for value_list in dict_.values():\n",
    "        unique_elements.update(value_list)\n",
    "    \n",
    "    # Convert the set to a sorted list to ensure consistent column ordering\n",
    "    unique_elements = sorted(unique_elements)\n",
    "    \n",
    "    # Step 2: Create the DataFrame\n",
    "    result_df = pd.DataFrame(index=dict_.keys(), columns=unique_elements)\n",
    "    \n",
    "    # Step 3: Populate the DataFrame\n",
    "    for key, value_list in dict_.items():\n",
    "        result_df.loc[key] = [element in value_list for element in unique_elements]\n",
    "    return result_df\n",
    "\n",
    "df_clustering_solution = construct_clustering_table(reversed_dict)\n",
    "df_clustering_solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucks_clustering1 = df_trucks.copy()\n",
    "df_trucks_clustering1 = pd.merge(df_trucks_clustering1, df_clustering_solution, left_index=True, right_index=True)\n",
    "df_trucks_clustering1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGC = 0\n",
    "df_ = df_trucks_clustering1[df_trucks_clustering1[HGC]]\n",
    "fig, axes = plt.subplots(figsize=(15, 10), ncols=4, nrows=2)\n",
    "specs = ['Spec_0', 'Spec_1', 'Spec_2', 'Spec_3', 'Spec_4', 'Spec_5', 'Spec_6', 'Spec_7']\n",
    "for spec, ax in zip(specs, axes.flat):\n",
    "    df_[spec].value_counts().plot.bar(ax=ax)\n",
    "fig.suptitle(f'Hypergraph cluster {HGC}')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGC = 1\n",
    "df_ = df_trucks_clustering1[df_trucks_clustering1[HGC]]\n",
    "fig, axes = plt.subplots(figsize=(15, 10), ncols=4, nrows=2)\n",
    "specs = ['Spec_0', 'Spec_1', 'Spec_2', 'Spec_3', 'Spec_4', 'Spec_5', 'Spec_6', 'Spec_7']\n",
    "for spec, ax in zip(specs, axes.flat):\n",
    "    value_counts = df_[spec].value_counts()\n",
    "    colors = ['red' if value in dict_clusters[f'HG_{HGC}'] else 'blue' for value in value_counts.index]\n",
    "    value_counts.plot.bar(ax=ax, color=colors)\n",
    "    ax.set_title(spec)\n",
    "fig.suptitle(f'Hypergraph cluster {HGC}')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_kpi_df(df_trucks_clustering_):\n",
    "    # Initialize lists to store the results\n",
    "    numbers = []\n",
    "    percentages = []\n",
    "    counts = []\n",
    "    \n",
    "    # Iterate through each column that represents a number\n",
    "    for column in df_trucks_clustering_.columns:\n",
    "        if isinstance(column, int) or column.isdigit():\n",
    "            number_col = df_trucks_clustering_[column]\n",
    "            total_count = number_col.sum()  # Count of rows where the number column is True\n",
    "            if total_count > 0:\n",
    "                true_percentage = df_trucks_clustering_.loc[number_col, 'in_study_repair'].mean() * 100  # Percentage of True in in_study_repair\n",
    "            else:\n",
    "                true_percentage = 0\n",
    "            # Append results to lists\n",
    "            numbers.append(int(column))\n",
    "            percentages.append(true_percentage)\n",
    "            counts.append(total_count)\n",
    "    \n",
    "    # Create a result DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'cluster id': numbers,\n",
    "        'percentage': percentages,\n",
    "        'count': counts\n",
    "    })\n",
    "    return result_df\n",
    "\n",
    "results_df_clustering1 = construct_kpi_df(df_trucks_clustering1)\n",
    "results_df_clustering1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average percentage of True in in_study_repair\n",
    "overall_average_percentage = df_trucks_clustering1['in_study_repair'].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the percentage column\n",
    "df_plot = results_df_clustering1.sort_values(by='percentage')\n",
    "df_plot['cluster id'] = df_plot['cluster id'].astype(str)\n",
    "\n",
    "# Create the bubble plot\n",
    "fig = px.scatter(df_plot, x='cluster id', y='percentage', size='count', \n",
    "                 title='Bubble plot of cluster ID vs. percentage of repairs',\n",
    "                 labels={'number': 'Number', 'percentage': '% True in Study Repair'},\n",
    "                 size_max=60)\n",
    "fig.update_traces(marker_sizemin=2, selector=dict(type='scatter'))\n",
    "\n",
    "# Add a horizontal line for the average percentage\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=min(df_plot['cluster id']),\n",
    "    x1=len(df_plot['cluster id']),\n",
    "    y0=overall_average_percentage,\n",
    "    y1=overall_average_percentage,\n",
    "    line=dict(color=\"Red\", width=2),\n",
    ")\n",
    "\n",
    "# Add annotation for the average line\n",
    "fig.add_annotation(\n",
    "    x=len(df_plot['cluster id'].unique()),\n",
    "    y=overall_average_percentage,\n",
    "    text=f\"Average: {overall_average_percentage:.2f}%\",\n",
    "    showarrow=False,\n",
    "    yshift=10,\n",
    "    font=dict(color=\"Red\")\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucks_clustering1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "\n",
    "def snn(X, neighbor_num, min_shared_neighbor_num):\n",
    "    \"\"\"Perform Shared Nearest Neighbor (SNN) clustering algorithm clustering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape (n_samples, n_samples)\n",
    "    A feature array\n",
    "    neighbor_num : int\n",
    "    K number of neighbors to consider for shared nearest neighbor similarity\n",
    "    min_shared_neighbor_num : int\n",
    "    Number of nearest neighbors that need to share two data points to be considered part of the same cluster\n",
    "    \"\"\"\n",
    "\n",
    "    # for each data point, find their set of K nearest neighbors\n",
    "    knn_graph = kneighbors_graph(X, n_neighbors=neighbor_num, include_self=False)\n",
    "    neighbors = np.array([set(knn_graph[i].nonzero()[1]) for i in range(len(X))])\n",
    "    print('identified neighbours for each data point')\n",
    "\n",
    "    # the distance matrix is computed as the complementary of the proportion of shared neighbors between each pair of data points\n",
    "    snn_distance_matrix = np.asarray([[get_snn_distance(neighbors[i], neighbors[j]) for j in range(len(neighbors))] for i in range(len(neighbors))])\n",
    "    print(f'retrieved distance matrix {snn_distance_matrix.shape}')\n",
    "    \n",
    "    # perform DBSCAN with the shared-neighbor distance criteria for density estimation\n",
    "    dbscan = DBSCAN(min_samples=min_shared_neighbor_num, metric=\"precomputed\")\n",
    "    dbscan = dbscan.fit(snn_distance_matrix)\n",
    "    return dbscan.core_sample_indices_, dbscan.labels_\n",
    "\n",
    "\n",
    "def get_snn_similarity(x0, x1):\n",
    "    \"\"\"Calculate the shared-neighbor similarity of two sets of nearest neighbors, normalized by the maximum number of shared neighbors\"\"\"\n",
    "\n",
    "    return len(x0.intersection(x1)) / len(x0)\n",
    "\n",
    "\n",
    "def get_snn_distance(x0, x1):\n",
    "    \"\"\"Calculate the shared-neighbor distance of two sets of nearest neighbors, normalized by the maximum number of shared neighbors\"\"\"\n",
    "\n",
    "    return 1 - get_snn_similarity(x0, x1)\n",
    "\n",
    "\n",
    "class SNN(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"Class for performing the Shared Nearest Neighbor (SNN) clustering algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neighbor_num : int\n",
    "        K number of neighbors to consider for shared nearest neighbor similarity\n",
    "\n",
    "    min_shared_neighbor_proportion : float [0, 1]\n",
    "        Proportion of the K nearest neighbors that need to share two data points to be considered part of the same cluster\n",
    "\n",
    "    Note: Naming conventions for attributes are based on the analogous ones of DBSCAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neighbor_num, min_shared_neighbor_proportion):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        self.neighbor_num = neighbor_num\n",
    "        self.min_shared_neighbor_num = round(neighbor_num * min_shared_neighbor_proportion)\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        \"\"\"Perform SNN clustering from features or distance matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or array of shape (n_samples, n_samples)\n",
    "            A feature array\n",
    "        \"\"\"\n",
    "\n",
    "        clusters = snn(X, neighbor_num=self.neighbor_num, min_shared_neighbor_num=self.min_shared_neighbor_num)\n",
    "        self.core_sample_indices_, self.labels_ = clusters\n",
    "        if len(self.core_sample_indices_):\n",
    "            # fix for scipy sparse indexing issue\n",
    "            self.components_ = X[self.core_sample_indices_].copy()\n",
    "        else:\n",
    "            # no core samples\n",
    "            self.components_ = np.empty((0, X.shape[1]))\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Performs clustering on X and returns cluster labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\\n",
    "                array of shape (n_samples, n_samples)\n",
    "            A feature array, or array of distances between samples if\n",
    "            ``metric='precomputed'``.\n",
    "        sample_weight : array, shape (n_samples,), optional\n",
    "            Weight of each sample, such that a sample with a weight of at least\n",
    "            ``min_samples`` is by itself a core sample; a sample with negative\n",
    "            weight may inhibit its eps-neighbor from being core.\n",
    "            Note that weights are absolute, and default to 1.\n",
    "\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            cluster labels\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_sample_ = X_[np.random.choice(X_.shape[0], 1000, replace=False), :]\n",
    "\n",
    "if False:\n",
    "    X_ = df_clustering_solution.astype(int).to_numpy()\n",
    "    print(X_.shape)\n",
    "    clustering_algo = SNN(10, 0.5)\n",
    "    y_ = clustering_algo.fit_predict(X_)\n",
    "    print(pd.Series(y_).value_counts())\n",
    "    fig, ax = plt.subplots(figsize=(10, 40))\n",
    "    pd.Series(y_).value_counts().plot.barh(ax=ax);\n",
    "    fig.savefig('snn_barplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "unique_vehicle_ids = df_ts.vehicle_id.unique()\n",
    "sol = {}\n",
    "for id_ in tqdm(unique_vehicle_ids):\n",
    "    df_ = df_ts[df_ts.vehicle_id == id_]\n",
    "    sol[id_] = df_.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = sol[2]\n",
    "exp.iloc[:, 2:].plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = sol[3]\n",
    "exp.iloc[:, 2:].plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCA on hypergraph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cluster_table = df_kpi_clustering.filter(regex='^\\d+$').to_numpy()\n",
    "formal_context = FormalContext(cluster_table)\n",
    "formal_context.to_pandas()\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTE_COLUMNS = df_ts.columns.str.extract(r'(\\d+_\\d+)')[0]\n",
    "ATTRIBUTE_COLUMNS = ATTRIBUTE_COLUMNS[~ATTRIBUTE_COLUMNS.isna()].tolist()\n",
    "print(ATTRIBUTE_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Train-test split\n",
    "\n",
    "- TEST SET = all repaired trucks & equal amount of healthy trucks\n",
    "- TRAIN SET = rest of healthy trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_truck_ids = df_trucks_clustering1[df_trucks_clustering1.in_study_repair == 1].index.to_numpy()\n",
    "healthy_truck_ids = df_trucks_clustering1[df_trucks_clustering1.in_study_repair == 0].index.to_numpy()\n",
    "ALL_TRUCK_IDS_TEST = np.concatenate((repaired_truck_ids, np.random.choice(healthy_truck_ids, len(repaired_truck_ids), replace=False)))\n",
    "ALL_TRUCK_IDS_TRAIN = np.array(list(set(healthy_truck_ids) - set(ALL_TRUCK_IDS_TEST)))\n",
    "print(f\"Length of ALL_TRUCK_IDS_TEST={len(ALL_TRUCK_IDS_TEST)}\")\n",
    "print(f\"Length of ALL_TRUCK_IDS_TRAIN={len(ALL_TRUCK_IDS_TRAIN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_timeseries(df, vehicle_ids):\n",
    "    unique_features = set(int(col.split('_')[0]) for col in ATTRIBUTE_COLUMNS)\n",
    "    fig, axes = plt.subplots(figsize=(6*len(unique_features), 4*len(vehicle_ids)),\n",
    "                             nrows=len(vehicle_ids), ncols=len(unique_features), sharex=True)\n",
    "    if len(vehicle_ids) > 1:\n",
    "        for vehicle_id, axes_row in zip(vehicle_ids, axes):\n",
    "            df_ = df[df.vehicle_id == vehicle_id].set_index('time_step')\n",
    "            for feature, ax in zip(unique_features, axes_row):\n",
    "                df_plot_ = df_[df_.columns[df_.columns.str.contains(str(feature))]]\n",
    "                #df_plot_.plot(ax=ax, marker='o')\n",
    "                ax.plot(df_plot_, marker='o')\n",
    "                ax.set_xlabel('timestamp')\n",
    "                ax.set_ylabel('Cumulative count')\n",
    "                ax.set_title(feature)\n",
    "    else:\n",
    "        df_ = df[df.vehicle_id == vehicle_ids[0]].set_index('time_step')\n",
    "        for feature, ax in zip(unique_features, axes):\n",
    "            df_plot_ = df_[df_.columns[df_.columns.str.contains(str(feature))]]\n",
    "            ax.plot(df_plot_, marker='o')\n",
    "            ax.set_xlabel('abstract timestamp unit')\n",
    "            ax.set_ylabel('cumulative count')\n",
    "            ax.set_title(feature)\n",
    "    fig.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "vehicle_ids = [2]\n",
    "fig, axes = plot_cumulative_timeseries(df_ts, vehicle_ids)\n",
    "fig.savefig('cumulative_timeseries.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ts_filled`: Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentage = df_ts[ATTRIBUTE_COLUMNS].isnull().mean() * 100\n",
    "missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "missing_percentage.plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Attribute')\n",
    "ax.set_ylabel('Percentage of Missing Values [%]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill missing values for each vehicle_id based on time_step\n",
    "df_ts_filled = df_ts.copy()\n",
    "df_ts_filled = df_ts_filled.sort_values(by=['vehicle_id', 'time_step'])\n",
    "tqdm.pandas()\n",
    "tmp = df_ts_filled.groupby('vehicle_id')[ATTRIBUTE_COLUMNS].progress_apply(lambda group: group.ffill())\n",
    "tmp = tmp.fillna(0)\n",
    "df_ts_filled[ATTRIBUTE_COLUMNS] = tmp.reset_index(level=0, drop=True)\n",
    "del tmp\n",
    "print(f'Shape of df_ts_filled: {df_ts_filled.shape}')\n",
    "df_ts_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_cumulative_timeseries(df_ts_filled, vehicle_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ts_limited`: Limiting amount of sampled data (only required for some decomposition matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill missing values for each vehicle_id based on time_step\n",
    "df_ts_limited = df_ts_filled.copy()\n",
    "tmp = df_ts_limited.groupby('vehicle_id').progress_apply(lambda group: group.iloc[:25])\n",
    "df_ts_limited = tmp.reset_index(level=0, drop=True)\n",
    "del tmp\n",
    "print(df_ts_limited.shape)\n",
    "df_ts_limited.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_ids = [0, 2]\n",
    "fig, axes = plot_cumulative_timeseries(df_ts_limited, vehicle_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_limited.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ts_differentiated`: Differentiating dataset (only required for some decomposition matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiate(group):\n",
    "    \"\"\" Differentiate time series and copy first value. \"\"\"\n",
    "    group_differentiated = group.diff()\n",
    "    group_differentiated.iloc[0] = group.iloc[0]\n",
    "    return group_differentiated\n",
    "\n",
    "df_ts_differentiated = df_ts_filled.copy()\n",
    "tmp = df_ts_differentiated.groupby('vehicle_id')[ATTRIBUTE_COLUMNS].progress_apply(differentiate)\n",
    "df_ts_differentiated[ATTRIBUTE_COLUMNS] = tmp.reset_index(level=0, drop=True)\n",
    "del tmp\n",
    "print(f'Shape of df_ts_differentiated: {df_ts_differentiated.shape}')\n",
    "df_ts_differentiated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_ids = [0, 2, 100, 33641, 33643]\n",
    "fig, axes = plot_cumulative_timeseries(df_ts_differentiated, vehicle_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ts_normalized_v1`: Normalize by time interval\n",
    "\n",
    "Idea: divide by difference between timesteps, to take out effect of irregular sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_ts_differentiated.copy()\n",
    "df_['time_diff'] = df_.groupby('vehicle_id')['time_step'].diff()\n",
    "df_['time_diff'] = np.where(df_['time_diff'].isna(), df_['time_step'], df_['time_diff'])\n",
    "meta_cols = ['vehicle_id', 'time_step', 'time_diff']\n",
    "\n",
    "# Process DataFrame in chunks\n",
    "chunk_size = 10000\n",
    "df_ts_normalized_v1 = pd.DataFrame()\n",
    "for start in tqdm(range(0, len(df_), chunk_size)):\n",
    "    end = start + chunk_size\n",
    "    df_chunk_normalized = df_.drop(columns=meta_cols).iloc[start:end].div(df_['time_diff'].iloc[start:end], axis=0)\n",
    "    df_ts_normalized_v1 = pd.concat([df_ts_normalized_v1, df_chunk_normalized])\n",
    "\n",
    "df_ts_normalized_v1[meta_cols] = df_[meta_cols]\n",
    "df_ts_normalized_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_ids = [0, 2, 100, 33641, 33643]\n",
    "fig, axes = plot_cumulative_timeseries(df_ts_normalized_v1, vehicle_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `df_ts_normalized_v2`: Normalize by maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## v3: Normalizing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) per bin over all trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler to each individual attribute\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# We assume that 'vehicle_id' and 'time_step' are not to be normalized\n",
    "# Select only the numeric columns to normalize\n",
    "numeric_columns = df_ts_limited.select_dtypes(include=['float64', 'int64']).columns\n",
    "columns_to_normalize = [col for col in numeric_columns if col not in ['vehicle_id', 'time_step']]\n",
    "\n",
    "# Normalize the numeric columns\n",
    "df_ts_limited[columns_to_normalize] = scaler.fit_transform(df_ts_limited[columns_to_normalize])\n",
    "\n",
    "# Display the head of the normalized DataFrame\n",
    "print(df_ts_limited.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_ids = [0, 2]\n",
    "unique_features = set(int(col.split('_')[0]) for col in ATTRIBUTE_COLUMNS)\n",
    "fig, axes = plt.subplots(figsize=(3*len(unique_features), 2*len(vehicle_ids)),\n",
    "                         nrows=len(vehicle_ids), ncols=len(unique_features), sharex=True)\n",
    "for vehicle_id, axes in zip(vehicle_ids, axes):\n",
    "    df_ = df_ts_limited[df_ts_limited.vehicle_id == vehicle_id]\n",
    "    df_ = df_.set_index('time_step')\n",
    "    for feature, ax in zip(unique_features, axes):\n",
    "        df_plot_ = df_[df_.columns[df_.columns.str.contains(str(feature))]]\n",
    "        #df_plot_.plot(ax=ax, marker='o')\n",
    "        ax.plot(df_plot_, marker='o')\n",
    "        ax.set_xlabel('timestamp')\n",
    "        ax.set_ylabel('Cumulative count')\n",
    "        ax.set_title(feature)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_limited[df_ts_limited.vehicle_id == 4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building decomposition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_group(group):\n",
    "    return pd.Series(\n",
    "        data={f\"{col}_{i}\": group.at[i, col] for col in group.columns if col != 'vehicle_id' for i in group.index},\n",
    "        name=group['vehicle_id'].iloc[0]\n",
    "    )\n",
    "\n",
    "\n",
    "flattened_df = pd.DataFrame([flatten_group(group.reset_index(drop=True)) \n",
    "                             for name, group in tqdm(df_ts_limited[['vehicle_id'] + ATTRIBUTE_COLUMNS].groupby('vehicle_id'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df = flattened_df.ffill() # !!! workaround\n",
    "flattened_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_train = flattened_df.loc[ALL_TRUCK_IDS_TRAIN]\n",
    "\n",
    "cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "V = feature_space_train.to_numpy()\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "nrows = V.shape[0]\n",
    "ncols = V.shape[1]\n",
    "title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "ax.set_title(title_, fontsize=None)\n",
    "# V.columns = BAND_COLUMNS\n",
    "im = ax.imshow(\n",
    "    V,\n",
    "    cmap=cmap,\n",
    "    aspect='auto',\n",
    "    interpolation='nearest',\n",
    "    norm=mpl.colors.LogNorm(vmin=-0.01, vmax=1.01),\n",
    "    # extent=[0.25,30.25,nrows,0]\n",
    ")\n",
    "ax.tick_params(axis='y', labelrotation=90)\n",
    "# ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_train = df_ts_limited.loc[TRUCK_IDS_TRAIN]\n",
    "\n",
    "cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "V = feature_space_train.to_numpy()\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "nrows = V.shape[0]\n",
    "ncols = V.shape[1]\n",
    "title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "ax.set_title(title_, fontsize=None)\n",
    "# V.columns = BAND_COLUMNS\n",
    "im = ax.imshow(\n",
    "    V,\n",
    "    cmap=cmap,\n",
    "    aspect='auto',\n",
    "    interpolation='nearest',\n",
    "    norm=mpl.colors.LogNorm(vmin=0, vmax=1),\n",
    "    # extent=[0.25,30.25,nrows,0]\n",
    ")\n",
    "ax.tick_params(axis='y', labelrotation=90)\n",
    "# ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "\n",
    "Take as input `df_ts_differentiated`. Start off with single sensor `397`. Construct decomposition per truck.\n",
    "\n",
    "- Normalize per sensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTRIBUTE_COLUMNS_167 = ['167_0', '167_1', '167_2', '167_3', '167_4', '167_5', '167_6', '167_7', '167_8', '167_9']\n",
    "ATTRIBUTE_COLUMNS_397 = [c for c in ATTRIBUTE_COLUMNS if '397' in c]\n",
    "df_ts_approach2 = df_ts_differentiated[['vehicle_id', 'time_step'] + ATTRIBUTE_COLUMNS_397]\n",
    "df_ts_approach2_train = df_ts_approach2[df_ts_approach2.vehicle_id.isin(ALL_TRUCK_IDS_TRAIN)]\n",
    "df_ts_approach2_test = df_ts_approach2[df_ts_approach2.vehicle_id.isin(ALL_TRUCK_IDS_TEST)]\n",
    "df_ts_approach2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train = df_ts_approach2_train[ATTRIBUTE_COLUMNS_397].to_numpy()\n",
    "V_test = df_ts_approach2_test[ATTRIBUTE_COLUMNS_397].to_numpy()\n",
    "ax = pd.Series({'train': len(V_train), 'test': len(V_test)}).plot.bar()\n",
    "ax.set_ylabel('Number of timestamps')\n",
    "ax.set_title('Size of training and test set')\n",
    "\n",
    "# Add the number of timestamps on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', \n",
    "                xytext=(0, 10), \n",
    "                textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_V(V):\n",
    "    cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    nrows = V.shape[0]\n",
    "    ncols = V.shape[1]\n",
    "    title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "    ax.set_title(title_, fontsize=20)\n",
    "    # V.columns = BAND_COLUMNS\n",
    "    im = ax.imshow(\n",
    "        V,\n",
    "        cmap=cmap,\n",
    "        aspect='auto',\n",
    "        interpolation='nearest',\n",
    "        norm=mpl.colors.LogNorm(vmin=V.min(), vmax=V.max()),\n",
    "        # extent=[0.25,30.25,nrows,0]\n",
    "    )\n",
    "    ax.tick_params(axis='y', labelrotation=90)\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_V(V_train)\n",
    "ax.set_ylabel('Readout', size=24)\n",
    "ax.set_xlabel('Sensor Bin', size=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "\n",
    "def extract_NMF(V, n_components=60, timestamps=None):\n",
    "    \"\"\"  Extracts statistics for non-negative matrix factorisation (NMF) on feature space provided by df.\n",
    "\n",
    "    Given a non-negative decomposition matrix V, NMF approximates V with two matrices W and H s.t. V = W x H.\n",
    "\n",
    "    :param V: Numpy array with (normalized) decomposition matrix V.\n",
    "    :param n_components: Integer with maximum number of components that should be extracted for NMF.\n",
    "    :return: Dictionary containing W and H, as well as some other model specific parameters.\n",
    "    \"\"\"\n",
    "    nmf = NMF(n_components=n_components, init='nndsvd', max_iter=1000, random_state=42)  # TODO: compare solvers\n",
    "    model = Pipeline([('nmf', nmf)])\n",
    "    W = model.fit_transform(V)\n",
    "    H = pd.DataFrame(model['nmf'].components_)\n",
    "    V_reconstructed = model['nmf'].inverse_transform(W)\n",
    "    model_dict = {\n",
    "        # number of components\n",
    "        'n_components': n_components,\n",
    "        # trained NMF-model\n",
    "        'nmf': model,\n",
    "        # decomposition matrix V, and approximation matrices W and H\n",
    "        #'V': V, 'W': W, \n",
    "        'H': H,\n",
    "        # timestamps for V\n",
    "        #'V_timestamps': timestamps,\n",
    "        # reconstructed decomposition matrix\n",
    "        #'V_reconstructed': V_reconstructed,\n",
    "        # coefficient of determination (average)\n",
    "        'R2_mean': r2_score(V, V_reconstructed, multioutput='uniform_average'),\n",
    "        # coefficient of determination (per sample)\n",
    "        'R2': r2_score(V.T, V_reconstructed.T, multioutput='raw_values'),\n",
    "        # coefficient of determination (per feature)\n",
    "        'R2_feature': r2_score(V, V_reconstructed, multioutput='raw_values'),\n",
    "        # mean squared error (average)\n",
    "        'MSE_mean': mean_squared_error(V, V_reconstructed),\n",
    "        # mean squared error (per sample)\n",
    "        'MSE': mean_squared_error(V.T, V_reconstructed.T, multioutput='raw_values'),\n",
    "        # reconstruction error expressed as Frobenius norm\n",
    "        'reconstruction_error': model['nmf'].reconstruction_err_\n",
    "    }\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def extract_nmf_per_number_of_component(df_V, n_components=60, timestamps=None, verbose=True):\n",
    "    \"\"\" Perform nmf with varying number of components.\n",
    "\n",
    "    :param df: Dataframe with (normalized) decomposition matrix V.\n",
    "    :param n_components: Integer with maximum number of components that should be extracted for NMF.\n",
    "    :param timestamps: Pandas series with timestamps corresponding to rows in feature space.\n",
    "    :return: Dataframe where each row corresponds to a different number of components and the columns contain\n",
    "        W and H, as well as some other model specific parameters.\n",
    "    \"\"\"\n",
    "    V = df_V.to_numpy()\n",
    "    range_components = range(1, n_components)\n",
    "    tqdm_description = 'Fitting NMF with varying number of components'\n",
    "    tqdm_range_components = tqdm(range_components, desc=tqdm_description, disable=(not verbose))\n",
    "    list_models = [extract_NMF(V, n_components=n_components, timestamps=timestamps.to_numpy()) for n_components in tqdm_range_components]\n",
    "    df_models = pd.DataFrame(list_models)\n",
    "    return df_models\n",
    "\n",
    "\n",
    "fpath = os.path.join('cache', 'df_nmf_models')\n",
    "RECOMPUTE = False         # only set to True, if NMF should be recomputed\n",
    "MAX_N_COMPONENTS = 15     # maximum number of components used to recompute\n",
    "df_V_train_ = pd.DataFrame(V_train)\n",
    "\n",
    "if not os.path.exists(fpath) or RECOMPUTE:\n",
    "    df_nmf_models_ = extract_nmf_per_number_of_component(\n",
    "        df_V_train_, n_components=MAX_N_COMPONENTS, timestamps=df_V_train_.index, verbose=True\n",
    "    )\n",
    "    #print(fpath)\n",
    "    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "    pickle.dump(df_nmf_models_, open(fpath, 'wb'))\n",
    "\n",
    "# load data from disk\n",
    "df_nmf_models = pickle.load(open(fpath, 'rb'))\n",
    "df_nmf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustrate_nmf_components_for_paper(V, explained_variance_ratio, df_nmf_models,\n",
    "                                        vmin=0.001, vmax=0.1,\n",
    "                                        xlims=(-1, 31), plot_x_ticks=7,\n",
    "                                        n_components=5):\n",
    "    \"\"\" Illustrate NMF components together with hyperparameter tuning and decomposition matrix.\n",
    "\n",
    "    Improved version of illustrate_nmf_components for paper.\n",
    "\n",
    "    :param V: Decomposition matrix as it is fed to NMF.\n",
    "    :param explained_variance_ratio: Numpy array with explained variance ratio extracted with PCA.\n",
    "    :param df_nmf_models: Dataframe with NMF-models for different number of components.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def get_n_components(x, threshold):\n",
    "        return (x > threshold).argmax() + 1\n",
    "\n",
    "    cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "    # mpl.rcParams['text.usetex'] = True\n",
    "    fig, ax_row = plt.subplots(figsize=(8, 7), ncols=3, nrows=1, sharex=False, sharey=False, constrained_layout=True)\n",
    "\n",
    "    # 1st column: show performance matrix V\n",
    "    ax = ax_row[0]\n",
    "    nrows = V.shape[0]\n",
    "    ncols = V.shape[1]\n",
    "    title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "    ax.set_title(title_, fontsize=None)\n",
    "    im = ax.imshow(\n",
    "        V,\n",
    "        cmap=cmap,\n",
    "        aspect='auto',\n",
    "        interpolation='nearest',\n",
    "        norm=mpl.colors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "    )\n",
    "    ax.tick_params(axis='y', labelrotation=90)\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xlabel('Sensor Bin')\n",
    "\n",
    "    # 2nd column: selection of number of components in two columns\n",
    "    ax = ax_row[1]\n",
    "    gridspec = ax.get_subplotspec().get_gridspec()\n",
    "    ax.remove()\n",
    "    subfig = fig.add_subfigure(gridspec[1])\n",
    "    sub_axes = subfig.subplots(2, 1, sharex=False, sharey=False)\n",
    "\n",
    "    # PCA\n",
    "    ax = sub_axes[0]\n",
    "    ax.set_ylim([30, 100])\n",
    "    explained_variance = 100 * explained_variance_ratio\n",
    "    ax.plot(pd.Series(dict(enumerate(explained_variance, start=1))), marker='o', markersize=4)\n",
    "    threshold = 95\n",
    "    n_components_95 = get_n_components(explained_variance, threshold)\n",
    "    ax.axhline(threshold, color='green', linestyle='dotted', label=f'> {threshold}% --> {n_components_95}')\n",
    "    threshold = 90\n",
    "    n_components_90 = get_n_components(explained_variance, threshold)\n",
    "    ax.axhline(threshold, color='red', linestyle='dashed', label=f'> {threshold}% --> {n_components_90}')\n",
    "    threshold = 75\n",
    "    n_components_75 = get_n_components(explained_variance, threshold)\n",
    "    ax.axhline(threshold, color='blue', linestyle='dotted', label=f'> {threshold}% --> {n_components_75}')\n",
    "    ax.set_ylabel('cumulative explained variance [%]')\n",
    "    ax.set_xlabel('number of components')\n",
    "    ax.set_title('Cumulative explained variance')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "\n",
    "    # NMF\n",
    "    ax = sub_axes[1]\n",
    "    x = df_nmf_models['n_components']\n",
    "    y = df_nmf_models['reconstruction_error']\n",
    "    ax.plot(pd.Series(dict(zip(x, y))), marker='o', markersize=4)\n",
    "    ax.set_title('Reconstruction error of NMF')\n",
    "    ax.set_xlabel('Number of components')\n",
    "    ax.set_ylabel('Frobenius norm')\n",
    "    ax.set_yscale('log')\n",
    "    # ax.set_ylim([0.00005, 0.0005])\n",
    "\n",
    "    # 3rd column\n",
    "    ax = ax_row[2]\n",
    "    gridspec = ax.get_subplotspec().get_gridspec()\n",
    "    ax.remove()\n",
    "    subfig = fig.add_subfigure(gridspec[2])\n",
    "    #n_components = get_n_components(explained_variance, min_explained_variance)\n",
    "    sub_axes = subfig.subplots(n_components, 1, sharex=True, sharey=True)\n",
    "    subfig.suptitle('components H')\n",
    "    row = df_nmf_models[df_nmf_models.n_components == n_components].iloc[0]\n",
    "    H = row.H\n",
    "    y_labels = list(range(len(H)))\n",
    "\n",
    "    for (row, x), ax, y_label in zip(H.iterrows(), sub_axes, y_labels):\n",
    "        x.plot(label=row, ax=ax, markersize=3)\n",
    "        # ax.set_title(y_label)\n",
    "        ax.set_xlabel('Sensor Bin')\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "    # ax.set_xlim(xlims)\n",
    "    return fig, ax_row\n",
    "\n",
    "# calculate explained variance\n",
    "print(f'- calculating explained variance...')\n",
    "pca = PCA(n_components=V_train.shape[1], random_state=42)\n",
    "pca.fit(V_train)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_.cumsum()\n",
    "explained_variance_ratio = explained_variance_ratio\n",
    "\n",
    "fig, _ = illustrate_nmf_components_for_paper(\n",
    "    V_train, explained_variance_ratio=explained_variance_ratio, df_nmf_models=df_nmf_models,\n",
    "    vmin=V_train.min(), vmax=V_train.max(), xlims=(-1,101), n_components=5\n",
    ")\n",
    "fig.savefig('V_v1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5, init='nndsvd', max_iter=2000, random_state=42)\n",
    "model = Pipeline([('nmf', nmf)])\n",
    "W_train = model.fit_transform(V_train)\n",
    "H_train = pd.DataFrame(model['nmf'].components_)\n",
    "V_reconstructed = model['nmf'].inverse_transform(W_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single fingerprint over all trucks\n",
    "- Assign weights by truck\n",
    "- Assign weights by hypergraph cluster\n",
    "- Assign weights by timerange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The weights are highly skewed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(W_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = {0: W_train.mean(axis=0)}\n",
    "fig, ax = plt.subplots(figsize=(1,5))\n",
    "sns.heatmap(pd.DataFrame(fingerprints[0]), annot=True, fmt=\".3f\", ax=ax, cmap='Blues', vmin=0, vmax=22, cbar=False)\n",
    "ax.set_ylabel('Component')\n",
    "ax.set_title('Fingerprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks_train = df_ts_approach2_train.vehicle_id\n",
    "trucks_test = df_ts_approach2_test.vehicle_id\n",
    "trucks_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = dict(zip(df_repair['vehicle_id'], df_repair['in_study_repair']))\n",
    "was_repaired = trucks_test.replace(replacement_dict)\n",
    "ax = pd.Series(was_repaired).value_counts().plot.bar()\n",
    "ax.set_xlabel('Repaired?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation 1: No grouping; Decomposition matrix v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = model.transform(V_test)\n",
    "W_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(dist, was_repaired)), columns=['distance', 'was_repaired']).astype({'was_repaired': bool})\n",
    "df.head()\n",
    "sns.boxplot(df, y='distance', x='was_repaired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(dist, was_repaired)), columns=['distance', 'was_repaired']).astype({'was_repaired': bool})\n",
    "df.head()\n",
    "ax = sns.boxplot(df, y='distance', x='was_repaired')\n",
    "ax.set_ylim(0, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation 2: Hypergraph clustering, Decomposition matrix v1\n",
    "\n",
    "1. Differentiate\n",
    "2. Forward-fill missing values\n",
    "3. Construct fingerprints per group\n",
    "\n",
    "no normalisation\n",
    "\n",
    "> GROUPS ARE CONSTRUCTED ON THE WHOLE TRAINING SET (NOT ONLY ON HEALTHY DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation 2 (Construct fingerprints per group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select clustering solutions for training set\n",
    "df_clustering_solution_train = pd.DataFrame([df_clustering_solution.loc[truck_id] for truck_id in tqdm(trucks_train)])\n",
    "\n",
    "# select all rows from V and W for the corresponding operating mode\n",
    "V_train_per_om = {om: V_train[df_clustering_solution_train[om]] for om in df_clustering_solution_train.columns}\n",
    "W_train_per_om = {om: W_train[df_clustering_solution_train[om]] for om in df_clustering_solution_train.columns}\n",
    "\n",
    "# construct fingerprints per operating mode\n",
    "fingerprints = {om: W_.mean(axis=0) for om, W_ in W_train_per_om.items()}\n",
    "df_fingerprints = pd.DataFrame(fingerprints).T\n",
    "df_fingerprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 4))\n",
    "ax = sns.heatmap(df_fingerprints, cmap='Blues', ax=ax)\n",
    "ax.set_title('Fingerprints')\n",
    "ax.set_xlabel('component')\n",
    "ax.set_ylabel('Fingerprint');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering_solution_test = pd.DataFrame([df_clustering_solution.loc[truck_id] for truck_id in tqdm(trucks_test)])\n",
    "W_test = model.transform(V_test)\n",
    "W_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 12))\n",
    "ax = sns.heatmap(W_test, cmap='Blues', ax=ax, vmax=50)\n",
    "ax.set_title('$W_{test}$')\n",
    "ax.set_xlabel('component')\n",
    "ax.set_ylabel('Fingerprint');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_per_fingerprints = []\n",
    "for w, clus_sol in tqdm(zip(W_test, df_clustering_solution_test.iterrows()), total=len(df_clustering_solution_test)):\n",
    "    distances_per_fingerprints_this_row = {om: np.linalg.norm(w-f) for om, f in fingerprints.items()}\n",
    "    distances_per_fingerprints.append(distances_per_fingerprints_this_row)\n",
    "\n",
    "df_distances_per_fingerprints = pd.DataFrame(distances_per_fingerprints)\n",
    "df_distances_per_fingerprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 6\n",
    "nrows = math.ceil(len(df_clustering_solution_test.columns) / ncols)\n",
    "fig, axes = plt.subplots(figsize=(15,15), nrows=nrows, ncols=ncols, sharex=True, sharey=False)\n",
    "for col, ax in zip(df_clustering_solution_test.columns, axes.flat):\n",
    "    df_ = df_distances_per_fingerprints[col]\n",
    "    df_ = df_[df_clustering_solution_test[col].reset_index(drop=True) & ~was_repaired.astype(bool)]\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        sns.histplot(df_, bins=np.arange(0, 160, 2), ax=ax)\n",
    "        median = df_.median()\n",
    "        ax.axvline(median, label='median distance', color='red', linestyle='dashed')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.9, s=f'median={round(median, 2)}', color='red')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.8, s=f'p99={round(df_.quantile(0.99), 2)}', color='red')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.7, s=f'p95={round(df_.quantile(0.95), 2)}', color='red')\n",
    "    ax.set_xlim([0, 160])\n",
    "    ax.set_xlabel('Distance')\n",
    "    ax.set_title(col)\n",
    "fig.suptitle('Distribution of distances per operating mode (healthy)', size=18, color='blue')\n",
    "fig.tight_layout()\n",
    "fig.savefig('distribution_healthy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 6\n",
    "nrows = math.ceil(len(df_clustering_solution_test.columns) / ncols)\n",
    "fig, axes = plt.subplots(figsize=(15,15), nrows=nrows, ncols=ncols, sharex=True, sharey=False)\n",
    "for col, ax in zip(df_clustering_solution_test.columns, axes.flat):\n",
    "    df_ = df_distances_per_fingerprints[col]\n",
    "    df_ = df_[df_clustering_solution_test[col].reset_index(drop=True) & was_repaired.astype(bool)]\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        sns.histplot(df_, bins=np.arange(0, 160, 2), ax=ax)\n",
    "        median = df_.median()\n",
    "        ax.axvline(median, label='median distance', color='red', linestyle='dashed')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.9, s=f'median={round(median, 2)}', color='red')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.8, s=f'p99={round(df_.quantile(0.99), 2)}', color='red')\n",
    "        ax.text(x=median + 0.05 * median, y=ax.get_ylim()[1] * 0.7, s=f'p95={round(df_.quantile(0.95), 2)}', color='red')\n",
    "    ax.set_xlim([0, 160])\n",
    "    ax.set_xlabel('Distance')\n",
    "    ax.set_title(col)\n",
    "fig.suptitle('Distribution of distances per operating mode (repaired)', size=18, color='blue')\n",
    "fig.tight_layout()\n",
    "fig.savefig('distribution_repaired.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3  # TODO: replace with elbow method\n",
    "\n",
    "# construct fingerprints per operating mode\n",
    "for om, V_test_ in V_test_per_om.items():\n",
    "    nmf = NMF(n_components=n_components, init='nndsvd', max_iter=1000, random_state=42)\n",
    "    model = Pipeline([('nmf', nmf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = dict(zip(df_repair['vehicle_id'], df_clustering_solution.loc[]))\n",
    "#df_spec_clustering_per_timestamp = df_clustering_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_number_of_timestamps(group):\n",
    "    return group.iloc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THROWN_AWAY_IDS = []\n",
    "sol = {}\n",
    "n_thrown_away = 0\n",
    "unique_vehicle_ids = df_ts.vehicle_id.unique()\n",
    "LEN_TS = 25\n",
    "for id_ in tqdm(unique_vehicle_ids):\n",
    "    df_ = df_ts[df_ts.vehicle_id == id_][ATTRIBUTE_COLUMNS]\n",
    "    # print(df_.shape)\n",
    "    matrix = df_.iloc[:LEN_TS]\n",
    "    normalized_matrix = MinMaxScaler().fit_transform(matrix)\n",
    "    s_ = pd.Series(normalized_matrix.flatten())\n",
    "    # why only so few samples?\n",
    "    if len(s_) >= len(ATTRIBUTE_COLUMNS) * LEN_TS:\n",
    "        sol[id_] = s_\n",
    "    else:\n",
    "        n_thrown_away += 1\n",
    "        THROWN_AWAY_IDS.append(id_)\n",
    "\n",
    "print(f'Threw away {n_thrown_away} trucks')\n",
    "\n",
    "feature_space = pd.DataFrame(sol).T\n",
    "feature_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUCK_IDS_TRAIN = np.array(list(set(ALL_TRUCK_IDS_TRAIN) - set(THROWN_AWAY_IDS)))\n",
    "TRUCK_IDS_TEST = np.array(list(set(ALL_TRUCK_IDS_TEST) - set(THROWN_AWAY_IDS)))\n",
    "\n",
    "print(f\"Length of TRUCK_IDS_TRAIN={len(TRUCK_IDS_TRAIN)}\")\n",
    "print(f\"Length of TRUCK_IDS_TEST={len(TRUCK_IDS_TEST)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_train = feature_space.loc[TRUCK_IDS_TRAIN]\n",
    "feature_space_test = feature_space.loc[TRUCK_IDS_TEST]\n",
    "feature_space_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "V = feature_space_train.to_numpy()\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "nrows = V.shape[0]\n",
    "ncols = V.shape[1]\n",
    "title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "ax.set_title(title_, fontsize=None)\n",
    "# V.columns = BAND_COLUMNS\n",
    "im = ax.imshow(\n",
    "    V,\n",
    "    cmap=cmap,\n",
    "    aspect='auto',\n",
    "    interpolation='nearest',\n",
    "    norm=mpl.colors.LogNorm(vmin=0, vmax=1),\n",
    "    # extent=[0.25,30.25,nrows,0]\n",
    ")\n",
    "ax.tick_params(axis='y', labelrotation=90)\n",
    "# ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "cmap = mpl.cm.get_cmap(\"Blues\")\n",
    "V = feature_space_train.to_numpy()\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "nrows = V.shape[0]\n",
    "ncols = V.shape[1]\n",
    "title_ = \"Performance matrix V\" + f\" ({nrows} x {ncols})\"\n",
    "ax.set_title(title_, fontsize=None)\n",
    "# V.columns = BAND_COLUMNS\n",
    "im = ax.imshow(\n",
    "    V,\n",
    "    cmap=cmap,\n",
    "    aspect='auto',\n",
    "    interpolation='nearest',\n",
    "    norm=mpl.colors.LogNorm(vmin=-0.01, vmax=1.01),\n",
    "    # extent=[0.25,30.25,nrows,0]\n",
    ")\n",
    "ax.tick_params(axis='y', labelrotation=90)\n",
    "# ax.xaxis.set_minor_locator(ticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_grouping = df_trucks_clustering1[0]\n",
    "first_grouping = first_grouping[first_grouping].index\n",
    "first_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks = feature_space[feature_space.index.isin(first_grouping)]\n",
    "trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_healthy_trucks = df_trucks_clustering1['in_study_repair'][df_trucks_clustering1['in_study_repair'] == 1].index\n",
    "unhealthy_trucks_first_grouping = trucks[trucks.index.isin(idx_healthy_trucks)]\n",
    "healthy_trucks_first_grouping = trucks[~trucks.index.isin(idx_healthy_trucks)]\n",
    "healthy_trucks_first_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_trucks_first_grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues to handle (see slides):\n",
    "\n",
    "- Handling of very short read-outs (e.g. less than 25 time stamps)\n",
    "- Handling of missing values\n",
    "- Differencing of time-series\n",
    "- Normalization of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compact",
   "language": "python",
   "name": "compact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

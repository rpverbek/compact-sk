{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: compact 0.0.1\n",
      "Uninstalling compact-0.0.1:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.11/dist-packages/compact-0.0.1.dist-info/*\n",
      "    /usr/local/lib/python3.11/dist-packages/compact/*\n",
      "Proceed (Y/n)?   Successfully uninstalled compact-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | pip uninstall compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Kit: Contextual Performance Profiling and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting compact@ git+https://github.com/rpverbek/compact-sk.git (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 14))\n",
      "  Cloning https://github.com/rpverbek/compact-sk.git to /tmp/pip-install-gpuvf24f/compact_f56ad6ac9e294c45ac348238776db999\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rpverbek/compact-sk.git /tmp/pip-install-gpuvf24f/compact_f56ad6ac9e294c45ac348238776db999\n",
      "  Resolved https://github.com/rpverbek/compact-sk.git to commit 9649668756c46753c2b1eda3b5472525d2a4e957\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (3.8.2)\n",
      "Requirement already satisfied: nbformat==5.9.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.9.2)\n",
      "Requirement already satisfied: numpy==1.26.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: pandas==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: plotly==5.18.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 6)) (5.18.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: seaborn==0.13.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 9)) (0.13.0)\n",
      "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.16.1)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 11)) (4.66.1)\n",
      "Requirement already satisfied: kneed==0.8.5 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 12)) (0.8.5)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (8.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly==5.18.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 6)) (9.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (70.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.37.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (8.26.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (3.0.11)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.43.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (0.19.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (4.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '/compact/requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not running in Google Colab, skipping the Colab-specific code.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt\n",
    "try:\n",
    "    # running on colab\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ModuleNotFoundError:\n",
    "    # running locally\n",
    "    display(\"Not running in Google Colab, skipping the Colab-specific code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The autoreload can be removed at the end?\n",
    "from compact.data.phm_data_handler import fetch_and_unzip_data\n",
    "from compact.nmf_profiling import extract_nmf_incremental, get_df_W_offline_and_online\n",
    "from compact.nmf_profiling import get_pivot_table\n",
    "from compact.viz.viz import illustrate_nmf_components_interactive, show_fingerprints, plot_example_interactive\n",
    "from compact.viz.viz import plot_ROC_curve, plot_weights_interactive\n",
    "from compact.util import get_operating_modes\n",
    "from compact.preprocessing import get_and_preprocess_healthy_data, get_and_preprocess_unhealthy_data\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "# ignore convergence warnings (1000 iterations reached by NMF)\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> ToDo: make clear that Anomaly Detection and Performance Profiling has their own code section.\n",
    "\n",
    "\n",
    "## Business context\n",
    "Industry 4.0 leverages on the advanced AI technologies to enable anomaly detection and performance profiling of industrial assets operating in different contexts. Context is defined by both internal and external factors, such as operational conditions, environmental variables, and usage patterns. For this reason, context-aware methods are fundamental to identify anomalies and to ensure accurate and reliable asset profiling. These methods allow for real-time monitoring and enable enhanced performance and reduce downtime of assets.\n",
    "\n",
    "## Business goal\n",
    "\n",
    "The business goal related to this Starter Kit (SK) is to illustrate a data-driven methodology to identify anomalies and profile the performance of assets operating in different contexts, i.e. in terms of process measurements reflecting the internal operations of the asset.\n",
    "As data-driven methodology, this SK focuses on the methodology developed by Fingerhut et al. [[1](#fingerhut2023), [2](fingerhut2024)]. Conventional anomaly detection methods often detect anomalies when the operating conditions change, rendering them less applicable for real-world dynamic industrial use cases. In contrast, the methodology presented in this starterkit is suitable for these scenarios, because it considers the dynamic nature of operating conditions.\n",
    "\n",
    "\n",
    "## Application contexts\n",
    "\n",
    "Contextual anomaly detection and performance profiling play a relevant role in a variety of industrial contexts such as:\n",
    "\n",
    "- Raise warnings to anticipate and avoid safety-critical conditions\n",
    "\n",
    "- Alert the need for inspection to avoid possible downtime and cost corrective maintenance\n",
    "\n",
    "- Performance benchmarking\n",
    "\n",
    "**FFNG: Do we use double spacing between some bullet points (like the one above) on purpose.**\n",
    "\n",
    "## Data characteristics and requirements\n",
    "To showcase the SK, a dataset is required that includes:\n",
    "\n",
    "- The vibration frequency captured by asset sensors. The captured vibrations need to be related to the health state of the assets.\n",
    "- Parameters related to the internal operations of the asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Kit outline\n",
    "\n",
    "\n",
    "The SK is organized in four main sections. First, the required background knowledge is provided to understand the terminology used in the rest of the document. Second, a description of data generated generated by the assets along with its preprocessing is reported. Third, the methodology introduced by [Fingerhut et al. [1]](#fingerhut2023) is illustrated highlighting how it can be used for performance profiling and anomaly detection of assets. Finally, conclusions are drawn.\n",
    "\n",
    "At the end of the SK you will know how to:\n",
    "\n",
    "- Develop a model for anomaly detection and performance profiling\n",
    "\n",
    "- Experimentally validate the resulting model and objectively compare it with other approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "  \n",
    "- **anomaly**: A data point that deviates significantly from what is expected under the given conditions.\n",
    "- **operating mode**. An asset can operate in different contexts which can influence its behavior. We refer to these context-dependent behaviors as operating modes. As an example we can imagine a gearbox that operates in two modes: normal and throttle. These two modes can be isolated by analzying the context, specifically the rotation speed of the gears. In throttle mode, the gearbox would operate under a reduced load, which would be indicated by a low rotational speed.\n",
    "- **operating and performance views**. The operating view is composed of the parameters capturing the operating context (e.g. rotation speed, torque). The performance view is composed of the parameters monitoring the performance behaviour (e.g. vibrations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data understanding\n",
    "\n",
    "### 2.1 Data description\n",
    "\n",
    "The dataset we will use in this Starter Kit comes from the [PHM North America challenge '23](https://data.phmsociety.org/phm2023-conference-data-challenge/). This dataset collect the time series data from a gearbox subject to pitting, i.e. a fatigue failure of the gear tooth along with metadata. This dataset includes measurements under varied operating conditions from a healthy state as well as six known fault levels. The training data are collected from a range of different operating conditions under 15 different rotational speeds and 6 different torque levels.  For each operating condition, 5 vibration measurements were collected.\n",
    "The vibration data is given in the time domain with a sampling rate of 20480Hz. The sampling duration differs between 3 seconds and 12 seconds. For each vibration measurement there are triaxial time-domain vibration measurements available (x, y and z). The vibrations are collected at different rotation per minute (rpm) and different runs.  Below, the user can get acquainted with the [[dataset](https://data.phmsociety.org/phm2023-conference-data-challenge/)] by visualizing the vibration measurements in the three directions for different rpm and runs. [**FFNG: In its current form, it might not be entirely clear how different vibration directions are handled. --> Need to go over that. AMUR: I did an iteration. Looks better? --> FFNG: Yes, thanks!**]\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://data.phmsociety.org/wp-content/uploads/sites/9/2023/06/PHM2023dc_fig1.png\" alt=\"MarineGEO circle logo\" style=\"height: 375px; width:800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping data...\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch the data if necessary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfetch_and_unzip_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/compact/data/phm_data_handler.py:101\u001b[0m, in \u001b[0;36mfetch_and_unzip_data\u001b[0;34m(fname, force)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(local_path_zipped), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path_zipped\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnzipping data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path_zipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m    102\u001b[0m         zip_ref\u001b[38;5;241m.\u001b[39mextractall(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_path_unzipped\n",
      "File \u001b[0;32m/usr/lib/python3.11/zipfile.py:1299\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1299\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1301\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/zipfile.py:1366\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Fetch the data if necessary\n",
    "fetch_and_unzip_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "Splitting the dataset into training and testing sets is a common activity for validating the machine learning models. The training dataset is used to prepare a model, i.e. to train it, whereas the testing dataset is used to evaluate the performance of the model.\n",
    "In our case, training and a testing set are based on splitting the vibration measurements. More specifically, a random sample of 75% of the original data recorded under normal (= healthy) condition is used for training whereas the remaining 25% is used for testing.  In the rest of the document, we will only refer to this split to analyze the performance of the methodology for the sake of the computational time. However, to have more reliable results, multiple splits should be performed, i.e. multiple random samples should be extracted for the training and testing sets. The interested reader can refer to [Fingerhut et al. [2]](#fingerhut2024) to see the results when 100 random splits are generated.\n",
    "\n",
    "\n",
    "\n",
    "The test set is then created by combining:\n",
    "1. **Normal condition**: The 25% healthy data that was held back (not used in the training set)\n",
    "2. **Anomaly condition**: Vibration data characterized by pitting level 1-8 (two more than in the training set). For each level of pitting, there are between 267 and 304 samples in the test set that were recorded at different speeds and torques.\n",
    "\n",
    "The figure below illustrates the train-test split.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_train-test-split.png?raw=1\" alt=\"Overview train-test split\" style=\"width:1000px;\"/>\n",
    "\n",
    "Furthermore, combinations of rotational speed X and torque Y are excluded from the test set, if they do not occur in the training set. This situation occurs, if the random train-test split did by chance assign all measurements with the same operating parameters to the test set.  [**FFNG: Added this sentence. (21.08.)**]. **AMUR: I don't get this sentece. I could be possible that they do not occur? Are there sample where X and Y are not recorded? If this is the case, should they not be removed even from the training set? --> FFNG: As a result of the random train-test split, some of the operating modes only occur in the test set (let's take for instance all measurements at 500 rpm and 100 Nm). If they cannot be found in the training set, then no vibration fingerprints would be constructed in the training set. I added this explanation in the sentence before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, the original data is prepared in the training set for the analysis.\n",
    "- The **time series data** is transformed into frequency-bands. Typically, a frequency band expresses vibration behaviour in a specific range of frequencies (measured in Hertz [Hz]). We define a frequency-band in terms of \"orders\". It is worth to notice that a frequency measure captures the number of events per second whereas the \"orders\" capture the number of events per revolution of the rotating element. Finally, a order-transformation is applied to standardize the data. This step is important and a common preprocessing step in prognostics and health management. For details, the interested reader is referred to the vibration alignment section of [Fingerhut et al. [1]](#fingerhut2023). All order-transformed vibration measurements are organized in a matrix which we call **performance matrix V**.\n",
    "- Metadata is created from the original dataset. The metadata contains the parameters at which the vibrations were measured:\n",
    "    - The torque expresses the rotational force in terms of Newton meter.\n",
    "    - The rotation speed expresses, how fast the gearbox is rotating in terms of revolutions per minute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief excerpt of the **performance matrix V** is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need something to showcase how the training and testing dataset looks like (columns, their meaning, etc)\n",
    "df_V_train, meta_data_train, df_data_healthy_test, f = get_and_preprocess_healthy_data()\n",
    "df_orders_test, meta_data_test = get_and_preprocess_unhealthy_data(df_data_healthy_test, f)\n",
    "\n",
    "# extract list of frequency band columns for later usage\n",
    "cols_ = df_V_train.columns\n",
    "BAND_COLS = cols_[cols_.str.contains('band')].tolist()\n",
    "\n",
    "df_V_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above has n columns which represent vibration measurements and m rows which represent the frequency bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we illustrate the corresponding metadata, consisting of the previously described process parameters `rotational speed [RPM]` and `torque [Nm]`, and some additional information regarding the vibration measurement direction (`direction`) and sample ids (`sample_id` and `unique_sample_id`). Each row corresponds to the same row in the performance matrix. For instance, the very first measurement contains vibrations recorded at 100 RPM and 300 NM, for the vibration measurement direction x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of operating modes is given by the unique combinations of `rotational speed [RPM]` and `torque [Nm]`. There are 76 unique operating modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_oms = len(meta_data_train['unique_sample_id'].str[:-2].unique())\n",
    "display(md(f'There are {n_oms} unique operating modes.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "\n",
    "The methodology can be divided into the offline and online phases. The offline phase focuses on performance profiling. More specifically, the operating modes of healthy assets are extracted and mapped to the expected performances. The online phase focuses on anomaly detection by exploiting the performance profiles extracted in the previous phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Offline phase\n",
    "\n",
    "The general workflow of the offline phase is reported in the figure below.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_offline-phase.png?raw=1\" alt=\"Overview offline phase\" style=\"width:800px;\"/>\n",
    "\n",
    "\n",
    "This phase can be divided into 3 steps (See Figure above). The main goals of these phases can be summarized as follow:\n",
    "1. Characterizing performance  behaviour. High-dimensional vibration signals are characterized in terms of a few vibration components that capture the fundamental vibration behaviour.\n",
    "2. Extracting operating modes. Based on the context, the extracted operating modes are associated with each timestamp.\n",
    "\n",
    "3. Linking operating modes to performance behaviour. For each operating mode a performance fingerprint is associated.\n",
    "\n",
    "\n",
    "\n",
    "#### Characterizing performance behaviour\n",
    "\n",
    "In this step, we extract for each asset its characteristic **performance behaviour**. These are extracted by applying a _non-negative matrix factorization_ (NMF) [[3](#nmf)] to the performance matrix V. The NMF method approximates a given matrix with two lower-dimensional matrices. The approximation contains the essential information about the asset's performance. Hence, NMF reveals underlying patterns in the performance data by representing it in a simpler form.\n",
    "\n",
    "We apply NMF to decompose the matrix V into a separate weight matrix $\\mathbf{W}$ and a component matrix $\\mathbf{H}$, i.e., $\\mathbf{V} \\approx \\mathbf{W} \\times \\mathbf{H}$.\n",
    "Compared to other decomposition methods, NMF has the advantage that due to the entirely positive values, $\\mathbf{H}$ is interpretable.\n",
    "\n",
    "The component matrix $\\mathbf{H}$ contains a set of $h$ representative components allowing to express performance behaviour in a standardized way and thus allowing to compare performance across operating modes and across assets. The matrix $\\mathbf{W}$ contains the weights for reconstructing the original performance matrix $\\mathbf{V}$. Each element in $\\mathbf{W}$ can be interpreted as the weights of the building blocks in $\\mathbf{H}$ needed to reconstruct a vibration signal encoded in the performance matrix $\\mathbf{V}$.\n",
    "\n",
    "To decide the value $h$ of components we decompose the matrix into an increasing number of components. In the next code section, we apply the NMF to up to 50 components. Unlike PCA, the decompositions cannot be derived from each other. Therefore, we need to recalculate them for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_N_COMPONENTS = 50    # maximum number of components used to recompute\n",
    "\n",
    "df_nmf_models = extract_nmf_incremental(df_V_train, max_n_components=MAX_N_COMPONENTS, timestamps=df_V_train.index, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we illustrate the decomposition of the vibration data.\n",
    "\n",
    "The identification of the number of components to describe the operating context is not trivial. To facilitate this process, in the plot below we provide the following plots.\n",
    "- The top two plots visualize how well the performance matrix $\\mathbf{V}$ can be approximated\n",
    "    - The left-top plot illustrates the cumulative explained variance of a principal component analysis (PCA) of the performance matrix $\\mathbf{V}$. It merely serves as an indication of an upper bound for how well the signal could be expressed using PCA.\n",
    "    - The top-righ plot illustrates the reconstruction error of the performance matrix $\\mathbf{V}$ using the NMF components. The reconstruction error is calculated as the Frobenius norm of the difference between the original matrix and the reconstructed matrix.\n",
    "- The bottom lineplots illustrate the NMF components.\n",
    "\n",
    "In the interactive widget below, the reader can select how the number of components are determined. This number can be based on:\n",
    "- the reconstruction error plot. In this case, the _knee point_ is identified and used to determine the number of components. The identification of this knee point is an extension of the methodology presented in [2] for automated hyperparameter tuning for operative context detection.\n",
    "\n",
    "- a threshold for the cumulative _explained variance_ from PCA (e.g., 95%). In this case, the smallest number of components for which this threshold is exceeded, is selected.\n",
    "- both the aforementioned methods. In this case the highest of the two values is used as the number of components.\n",
    "\n",
    "Each of the $h$ components from $\\mathbf{H}$ is illustrated in a separate lineplot. The components serve as building blocks of the observed vibration signals, revealing common patterns in the vibration measurements.\n",
    "\n",
    "For the suggested settings, it can be observed that components 1 and 2 form the basis vectors for peaks that are observed at 40 and 80 orders. These peaks are expected, as the driving gear has 40 teeth. It is common to see peaks for multiples of the number of teeth, hence there is also a peak at 80 orders. Components 3, 4 and 5 encode smaller peaks which model the noise floor.\n",
    "\n",
    "The reader is invited to experiment with multiple parameters below to verify how they affect the number of components extracted. The domain expert can use this code section to incorporate their domain knowledge to make the extraction and representation of implicit contexts more data-efficient .\n",
    "\n",
    "The interactive widget allows to go beyond the parameters selected in the original methodology [[2](#fingerhut2024)]. By increasing the range of acceptable parameters or by increasing the selection criterion (e.g., the explained variance), more components will be used in the decomposition. This makes the vibration fingerprints more granular, but also more prone to noise. In contrast, decreasing the accepted range or the selection criterium will yield less components, making the resulting fingerprints less informative, but also less prone to noise.\n",
    "\n",
    "The number of components should be neither too low, which would lead to inaccurate, too general vibration fingerprints, nor should it be too high, which would lead to too specific, fingerprints prone to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_values = illustrate_nmf_components_interactive(df_V_train, df_nmf_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the number of components determined in the interactive figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = saved_values['n_components']\n",
    "COMPONENT_COLUMNS = list(range(N_COMPONENTS))  # used later\n",
    "model = df_nmf_models[(df_nmf_models.n_components == N_COMPONENTS)].iloc[0]\n",
    "display(md(f'We are using **{N_COMPONENTS} components**.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of this section, the reader played with the number of components. Did we test multiple options? Should we not put a limit to avoid that the notebook breaks?\n",
    "\n",
    "While the starterkit is able to cope with different hyperparameters, in the text that follows, we assume that at least 5 components were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting operating modes.\n",
    "\n",
    "> AMUR: In your opinion Fabian, is this the best place where I can write: here we perform \"Context segmentation\". I need to repor these two words somewhere. FFNG: Yes: \"context\" expressed in \"operating modes\"; \"context segmentation\" $\\approx$ \"extraction of operating modes\". Let's write it in the title such that it is clear?\n",
    "\n",
    "In this step, we extract the operating modes of the assets. In the use case, a finite number of speed and torque values are tested.\n",
    "For clarity, in the text that follows, the operating modes are reported using the pattern `@ X rpm, Y Nm`, where `X` corresponds to the rotational speed and `Y` corresponds to the torque. All measurements with the same rotational speed and torque are summarized in the same operating mode (OM). There are as many operating modes, as there are unique combinations of `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_operating_modes = get_operating_modes()\n",
    "df_operating_modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot table above depicts the names of the extracted operating modes in function of the two operating parameters speed and torque: An asset running in `OM 1` (top left) runs at a rotational speed of 100 rpm and a torque of 50 Nm, whereas an asset running in `OM 77` (bottom right) runs at 1200 rpm and 500 Nm. For this train-test split, there is a total of 77 operating modes.\n",
    "> AMUR: Fabian can you continue? --> FFNG: Adapted description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='Link-operating-modes-to-performance-behaviour'>Linking operating modes to performance</a>\n",
    "In this step, the performance behaviour is linked with the operating modes. This allows to derive **context-sensitive performance fingerprints**. As each vibration measurement is assigned to an operating mode, it is possible to derive fingerprints by aggregating all rows in $\\mathbf{W}$ annotated with the same operating mode.\n",
    "In the following, for each of the 77 operating modes, the reader can visualize the performance behaviour for each individual measurement based on the selected statistics (e.g. mean).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fingerprints(model, df_V_train, meta_data_train, df_operating_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AMUR: Fabian can introduce in the text that follow psd? I don't know what it means. If you could mention it, it would help. Please complete the next sentence\n",
    "\n",
    "The table reports in x-axis ..., y-axis, the color describes ...  [complete sentence].\n",
    "From the analysis of the operating modes, it is possible to observe that they present distinct performance fingerprints, as can be observed for instance when comparing e.g., operating mode (OM) 1 with OM 50. Whereas OM 1 predominantly expresses vibrations in the third component related to the noise floor for all vibration directions, OM 50 predominantly expresses vibrations in the first two components related to 40 and 80 orders. At the same time, operating modes with similar operating conditions show similar vibration fingerprints. For instance, OM 1 and OM 16 only differ slightly in the torque, which results (as expected) in similar vibration fingerprints.\n",
    "\n",
    "At the end of this offline phase, each operating mode has its own performance profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Online phase\n",
    "\n",
    "The general workflow of the online phase is reported in the figure below. Note that due to the nature of the data used in this use case, some steps are no longer necessary or become very simplified. These steps are still included for the sake of completeness.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_online-phase.png?raw=1\" alt=\"Overview online phase\" style=\"width:800px;\"/>\n",
    "\n",
    "> Part of the text below should be simplified [I need a lot of help from Fabian for this job]. --> FFNG: Is it simplified enough? \n",
    "\n",
    "This phase can be divided in five steps (See Figure above). The main goals of these phases can be summarized as follows:\n",
    "\n",
    "4. **Windowing incoming streaming**. Streaming data is usually divided into batches. This activity is generally performed to avoid processing each new received data point as it arrives. However, since the data isn't streaming in this case, windowing isn't needed, so this step is skipped.\n",
    "\n",
    "5. **Detecting of the operating context**. Each time stamp is associated with an operating mode which was identified during the offline phase. In this use case, operating modes are linked to vibration measurements without time stamps.\n",
    "\n",
    "6. **Deriving the performance profiles**. Each vibration measurement is characterized in terms of the vibration components that were extracted in step 1 during the offline phase.\n",
    "\n",
    "7. **Estimating the fingerprint offset**. For each vibration measurement, the _offset_ is calculated between the online profiles and the offline fingerprints. The offset quantifies to what extent the observed vibration behaviour differs from the expected vibration behaviour expressed through the fingerprints.\n",
    "\n",
    "8. **Deriving alarms**. In the original paper [[2](#fingerhut2024)], based on the offset, an anomaly score is computed for each time stamp. This score is monitored over time raise alarms. In this use case, the monitoring of the anomaly score is not possible since the timestamps in the test set are not ordered. For this reason, this step is skipped.\n",
    "\n",
    "\n",
    "#### Detecting of the operating context and deriving the performance profiles.\n",
    "\n",
    "As in the offline phase, the operating modes are extracted from each timestamp present in the testing dataset. Then, the performance profiles are derived for each vibration measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of extracting the performance profiles is to capture and represent the key characteristics of the vibration signal in a way that reflects the asset's vibration behavior. Therefore, this step essentially mirrors the step 1 of the offline phase, with the exception that the vibration components are not derived again.\n",
    "\n",
    "The performance profiles approximate the vibration signal in terms of the representative vibration components that were extracted during step 1 of the offline phase. Similarly to this step, the approximation contains the essential information about the asset's performance.\n",
    "These profiles are extracted by applying NMF to the processed vibration measurements from the online phase, denoted as $\\mathbf{v}'$.\n",
    "The processed measurements $\\mathbf{v}'$ are a vector with the same number of elements as the columns in the performance matrix $\\mathbf{V}$. Since the step is applied to a data stream, multiple samples are no longer stacked together as in $\\mathbf{V}$ during the offline phase.\n",
    "> AMUR; Fabian is not clear the last sentence. What is denoted as $\\mathbf{v}'$? Could rephrase the sentence? --> FFNG: I added some explanation. Is it now more clear?\n",
    "\n",
    "In the online phase, NMF now uses the $h$ components from matrix $\\mathbf{H}$, which were already identified during the offline phase. Specifically, NMF decomposes $\\mathbf{v}'$ into a weight vector $\\mathbf{w}'$ using the fixed component matrix $\\mathbf{H}$, such that $\\mathbf{v}' \\approx \\mathbf{w}' \\times \\mathbf{H}$.\n",
    "\n",
    "Since the same components are used from the offline phase, the profiles match the format of the vibration fingerprints, making them suitable for subsequent anomaly detection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_offline, df_W_online, fingerprints, test_vibration_measurement_periods_meta_data = get_df_W_offline_and_online(df_V_train, meta_data_train, meta_data_test, model, df_orders_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the derived weights $\\mathbf{w}'$, below the derived weights for a single measurement are illustrated\n",
    "A vibration measurement consists of the three measurements directions (namely the X-, Y- and Z-axis, corresponding to the rows in the colormap below).\n",
    "The weights that are illustrated below consist of $h$ columns each, where $h$ represents the number of components determined in the offline phase.\n",
    "The format of the weights is the same as that of the fingerprint shown in the [previous section](#Link-operating-modes-to-performance-behaviour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(df_W_online, meta_data_test, df_operating_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we select measurement 10 from the test set, which was taken at 1000 rpm and 50 Nm and therefore corresponds to operating mode 10. It can be observed that components 1 and 4 have high values in all 3 measurement directions.\n",
    "> AMUR: Fabian what is a strong presence?. I don't think the reader even knows what is the meaning of code color (ie. the meseaurement range).\n",
    ">\n",
    "> RVEB: I changed it to \"high values\" instead of high presence.\n",
    "\n",
    "When comparing this to the vibration fingerprint at OM 10 (see the [interactive widget for showing the fingerprints](#Link-operating-modes-to-performance-behaviour) and choose operating mode 10), we observe a similar behaviour. Hence, for this example, the observed beahviour is similar to the expected vibration behaviour, which is indicative that this vibration measurement can be considered as non-anomalous. This comparison is a representative example on how to check whether the vibration measurement is indicative of an anomaly.\n",
    "\n",
    "At the end of this step, for each online vibration measurement, a performance profile is extracted, which is of the same format as the vibration fingerprints.\n",
    "> AMUR: Fabian what does it mean: \"processed to a corresponding performance\". Can you be more clear? --> FFNG: I changed the sentence. Is it now more clear?\n",
    "\n",
    "The performance profile contains the essential information of the vibration signal, while being of the same format as the vibration fingerprints.\n",
    "> AMUR: Fabian do we need the sentence above? --> FFNG: I added the information in the previous sentence.\n",
    "\n",
    "In the next section, the performance profiles are compared to the expected vibrations from the fingerprints in an automated fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the fingerprint offset\n",
    "\n",
    "In order to assess\n",
    "whether the performance of asset $i$ ($i = 1, . . . , n$) is normal or anomalous, it is necessary to quantify the distance - offset - between the observed and expected fingerprints. For this reason, for each performance measurement, the cosine distance between the derived weights $\\mathbf{w}'$ and the fingerprint $f_{ij} \\in F_i$, corresponding to the detected operating mode $O_{ij}$, ($j = 1, . . . , k_i$), is used to estimate the offset:\n",
    "\n",
    "$$ d_{cos}(\\mathbf{w}_i', \\mathbf{f}_{ij}) = 1 - \\dfrac{\\mathbf{w}_i' \\cdot \\mathbf{f}_{ij}}{\\|\\mathbf{w}_i'\\| \\ \\|\\mathbf{f}_{ij}\\|} $$\n",
    "\n",
    "where ||  || is the magnitude of the corresponding vector. In this use case, being present only one asset, $n$=1.\n",
    "\n",
    "The derived weight vectors represent individual direction measurements as a result of decomposing per individual direction. The weight vectors are appended into a single ($3\\times h$)-dimensional vector $\\mathbf{w}_i'$  of rank 1 that is compared to the fingerprint $f_{1j}$ of the corresponding operating mode $O_{1j}$.\n",
    "\n",
    "Once the offset are extracted, it is possible to evaluate to what extent they can be exploited for anomaly detection. For this reason in next code section, for each operating mode are computed the offset of the derived weights to the corresponding fingerprint. More specifically, a pivot table is created below showing the cosine distance in order to compare distances between measurements and their corresponding fingerprints (left) and other fingerprints (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine = get_pivot_table(df_W_online, fingerprints, test_vibration_measurement_periods_meta_data)\n",
    "\n",
    "print(f'Pivot table with distances to all fingerprints (0 - {len(df_cosine.columns) - 7}), corresponding rpm and torque values, and additional information on the anomaly condition:')\n",
    "display(df_cosine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As next step, it is evaluated whether the performance offset can be exploited for detecting anomalies. In the plot below is analzed the relation between offset distance and pitting level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_cosine, y='distance_to_own_cluster_center', x='pitting_level')\n",
    "ax.set_ylabel('Distance to context sensitive fingerprint')\n",
    "ax.set_title(f'Distance to own cluster center per pitting level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplots above, a clear difference in terms of offset between healthy and faulty gearboxes can be observed. Indeed, healthy gearboxes, the one with pitting level = 0, have offset close to 0 wherease faulty gearbox, the one with pitting level above 0, have higher offset. This is expected, as the context-sensitive fingerprint is derived from healthy data without pitting and data with pitting is likely to have vibration patterns not present in the healthy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot provides only a partial view of the ability of the methodology to identify anomalies. To fully appreciate the performance of the methodology, it is necessary to adequately detect faulty gearboxes as anomalies.\n",
    "Whether a datapoint is labelled as anomalous depends on a predefined distance threshold.\n",
    "If the predefined distance threshold is exceeded, an anomaly is raised.\n",
    "To make our validation independent of the choice of this hyperparameter, we construct a ROC-curve by varying this distance threshold.\n",
    "\n",
    "A ROC curve is a visualisation for evaluating the performance of the anomaly detection. It plots the *true positive rate* (TPR) against the *false positive rate (FPR)* at different distance thresholds. In this use case, the TPR represents the rate of measurements exposed to pitting that were actually detected as anomaly whereas the FPR is the rate of measurements that were not exposed to pitting and still detected as anomaly.\n",
    "The higher is the area under the ROC-curve, the better the model is at detecting anomalies.\n",
    "\n",
    "Below the ROC-curve is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(df_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above it can be observed that the anomaly detection generally performs well with an area under the curve (AUC) of $0.979$.\n",
    "Operators aim for a high TPR while minimizing false alarms (keeping the FPR low). Therefore, we additionally tracked the TPR at a stable FPR of 0.1 (TPR@FPR=0.1, red dashed line), which represents the TPR when there are 10% false positives. In this scenario, the TPR would be 96%. An alternatve objective criterium is to keep the FPR as low as possible at a high TPR.\n",
    "The plot above therefore also illustrates the FPR@TPR=0.9 (green dashed line).In that scenario, we check how many false alarms would be triggered if we want to guarantee that 90% of the gear pitting is detected. In this use case, FPR@TPR=0.9 is 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "In this SK is illustrated a data-driven methodology for contextual performance profiling and anomaly detection. The SK focuses on how to set up methodology parameters and correctly interpret its results. The methodology, validated on a gearbox that is subject to pitting, explains how to extract the contexts from an asset and how to use them to profile its performance. Furthermore, it proves that the performance profiles can be used to identify anomalies. For the latter, it has been shown that our method is able to detect most anomalies, while throwing few false alarms, as demonstrated through the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional information\n",
    "The methodology presented in this notebook is based on the papers [[1]](#fingerhut2023) and [[2]](#fingerhut2024).\n",
    "\n",
    "This Starter Kit was developed in the context of the Compact project [ToDo: Find an online reference]. For more information, please contact [info@elucidata.be](mailto:info@elucidata.be).\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Notebook\"), to deal in the Notebook without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Notebook, and to permit persons to whom the Notebook is provided to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies of the Notebook and/or copies of substantial portions of the Notebook.\n",
    "\n",
    "THE NOTEBOOK IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL SIRRIS, THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, DIRECT OR INDIRECT, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE NOTEBOOK OR THE USE OR OTHER DEALINGS IN THE NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id='fingerhut2023'>[1]</a> F. Fingerhut, S. Klein, M. Verbeke, S. Rajendran and E. Tsiporkova, \"Multi-view contextual performance profiling in rotating machinery,\" 2023 IEEE International Conference on Prognostics and Health Management (ICPHM), Montreal, QC, Canada, 2023, pp. 343-350, [doi: 10.1109/ICPHM57936.2023.10194172](https://ieeexplore.ieee.org/document/10194172).\n",
    "\n",
    "<a id='fingerhut2024'>[2]</a> F. Fingerhut, M. Verbeke and E. Tsiporkova, \"Unsupervised context-sensitive anomaly detection on streaming data relying on multi-view profiling,\" 2024 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS), Madrid, Spain, 2024, pp. 1-10, [doi: 10.1109/EAIS58494.2024.10569106](https://ieeexplore.ieee.org/document/10569106).\n",
    "\n",
    "<a id='nmf'>[3]</a> D. Lee, S. Seung, [Learning the parts of objects by non-negative matrix factorization](https://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf), 1999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### , 2024, Sirris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

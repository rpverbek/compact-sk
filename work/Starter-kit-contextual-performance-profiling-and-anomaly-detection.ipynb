{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: compact 0.0.1\n",
      "Uninstalling compact-0.0.1:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.11/dist-packages/compact-0.0.1.dist-info/*\n",
      "    /usr/local/lib/python3.11/dist-packages/compact/*\n",
      "Proceed (Y/n)?   Successfully uninstalled compact-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!echo \"Y\" | pip uninstall compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Kit: Contextual Performance Profiling and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting compact@ git+https://github.com/rpverbek/compact-sk.git (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 14))\n",
      "  Cloning https://github.com/rpverbek/compact-sk.git to /tmp/pip-install-q5m9slgq/compact_0bc0ed5671ab4429924a557b51b7c49b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rpverbek/compact-sk.git /tmp/pip-install-q5m9slgq/compact_0bc0ed5671ab4429924a557b51b7c49b\n",
      "  Resolved https://github.com/rpverbek/compact-sk.git to commit 9c097423a616c1ed1c759b3ebe0f6df5258aede5\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (3.8.2)\n",
      "Requirement already satisfied: nbformat==5.9.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.9.2)\n",
      "Requirement already satisfied: numpy==1.26.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 4)) (1.26.2)\n",
      "Requirement already satisfied: pandas==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: plotly==5.18.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 6)) (5.18.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: seaborn==0.13.0 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 9)) (0.13.0)\n",
      "Requirement already satisfied: tensorflow==2.16.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.16.1)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 11)) (4.66.1)\n",
      "Requirement already satisfied: kneed==0.8.5 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 12)) (0.8.5)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /usr/local/lib/python3.11/dist-packages (from -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (8.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (5.14.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly==5.18.0->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 6)) (9.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (70.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.37.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (8.26.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (3.0.11)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.43.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (0.19.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->nbformat==5.9.2->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 3)) (4.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1->-r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt (line 10)) (0.1.2)\n",
      "Building wheels for collected packages: compact\n",
      "  Building wheel for compact (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for compact: filename=compact-0.0.1-py3-none-any.whl size=18743 sha256=9b7bfe5d441de34265dc4d39b6fc6df5d77b0072d92eb2210c890c4603f21754\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7f613kmq/wheels/48/d8/b8/17146e928a2f52859d3e797229cc8b3b1ec39d5ba9593a668b\n",
      "Successfully built compact\n",
      "Installing collected packages: compact\n",
      "Successfully installed compact-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Not running in Google Colab, skipping the Colab-specific code.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/rpverbek/compact-sk/main/compact/requirements.txt\n",
    "try:\n",
    "    # running on colab\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ModuleNotFoundError:\n",
    "    # running locally\n",
    "    display(\"Not running in Google Colab, skipping the Colab-specific code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# The autoreload can be removed at the end?\n",
    "from compact.data.phm_data_handler import fetch_and_unzip_data\n",
    "from compact.nmf_profiling import extract_nmf_incremental, get_df_W_offline_and_online\n",
    "from compact.nmf_profiling import get_pivot_table\n",
    "from compact.viz.viz import illustrate_nmf_components_interactive, show_fingerprints, plot_example_interactive\n",
    "from compact.viz.viz import plot_ROC_curve, plot_weights_interactive\n",
    "from compact.util import get_operating_modes\n",
    "from compact.preprocessing import get_and_preprocess_healthy_data, get_and_preprocess_unhealthy_data\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "# ignore convergence warnings (1000 iterations reached by NMF)\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> ToDo: make clear that Anomaly Detection and Performance Profiling has their own code section.\n",
    "\n",
    "\n",
    "## Business context\n",
    "Industry 4.0 leverages on the advanced AI technologies to enable anomaly detection and performance profiling of industrial assets operating in different contexts. Context is defined by both internal and external factors, such as operational conditions, environmental variables, and usage patterns. For this reason, context-aware methods are fundamental to identify anomalies and to ensure accurate and reliable asset profiling. These methods allow for real-time monitoring and enable enhanced performance and reduce downtime of assets. <mark> FFNG: We are introducing the term \"anomaly\" only in the background section, but mention it already in here. Shouldn't we explain it already briefly beforehand? </mark>\n",
    "\n",
    "## Business goal\n",
    "\n",
    "The business goal related to this Starter Kit (SK) is to illustrate a data-driven methodology to identify anomalies and profile the performance of assets operating in different contexts, i.e., in terms of process measurements reflecting the internal operations of the asset.\n",
    "As data-driven methodology, this SK focuses on the methodology developed by Fingerhut et al. [[1](#fingerhut2023), [2](fingerhut2024)]. Conventional anomaly detection methods often detect anomalies when the operating conditions change, rendering them less applicable for real-world dynamic industrial use cases. In contrast, the methodology presented in this starterkit is suitable for these scenarios, because it considers the dynamic nature of operating conditions.\n",
    "\n",
    "\n",
    "## Application contexts\n",
    "\n",
    "Contextual anomaly detection and performance profiling play a relevant role in a variety of industrial contexts such as:\n",
    "\n",
    "- Raise warnings to anticipate and avoid safety-critical conditions\n",
    "- Alert the need for inspection to avoid possible downtime and cost corrective maintenance\n",
    "- Performance benchmarking\n",
    "\n",
    "## Data characteristics and requirements\n",
    "To showcase the SK, a dataset is required that includes:\n",
    "\n",
    "- The vibration frequency captured by asset sensors. The captured vibrations need to be related to the health state of the assets.\n",
    "- Parameters related to the internal operations of the asset that influence the vibrations but are not necessarily directly related to the health state.  <mark> FFNG: Added \"that influence [...]\" </mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Kit outline\n",
    "\n",
    "\n",
    "The SK is organized in five main sections. First, the required background knowledge is provided to understand the terminology used in the rest of the document. Second, a description of data generated by the assets along with its preprocessing is reported. Third, the methodology introduced by [Fingerhut et al. [1]](#fingerhut2023) is illustrated highlighting how it can be used for performance profiling. Fourth, the validation of the methodology for anomaly detection is provided. Finally, conclusions are drawn.\n",
    "\n",
    "At the end of the SK you will know how to:\n",
    "\n",
    "- Develop a model for anomaly detection and performance profiling\n",
    "\n",
    "- Experimentally validate the resulting model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "  \n",
    "- **anomaly**: A data point that deviates significantly from what is expected under the given conditions. Anomalies give an indication about potential failures of the asset. <mark> FFNG: Added some explanation in potential failures </mark>\n",
    "- **operating mode**. An asset can operate in different contexts which can influence its behavior. We refer to these context-dependent behaviors as operating modes. As an example we can imagine a gearbox that operates in two modes: normal and throttle. These two modes can be isolated by analzying the context, specifically the rotation speed of the gears. In throttle mode, the gearbox would operate under a reduced load, which would be indicated by a low rotational speed.\n",
    "- **operating and performance views**. The operating view is composed of the parameters capturing the operating context (e.g. rotation speed, torque). The performance view is composed of the parameters monitoring the performance behaviour (e.g. vibrations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data understanding\n",
    "\n",
    "### 2.1 Data description\n",
    "\n",
    "The dataset we will use in this Starter Kit comes from the [PHM North America challenge '23](https://data.phmsociety.org/phm2023-conference-data-challenge/). This dataset collects the time series data from a gearbox subject to pitting, i.e. a fatigue failure of the gear tooth along with metadata. This dataset includes measurements under varied operating conditions, defined in terms of rotational speed and torque, from a healthy state as well as six known fault levels. The training data are collected from a range of different operating conditions under 15 different rotational speeds and 6 different torque levels.  For each operating condition, 5 vibration measurements were collected.\n",
    "The vibration data is given in the time domain with a sampling rate of 20480Hz. The sampling duration differs between 3 and 12 seconds. For each vibration measurement there are tri-axial time-domain vibration measurements available (x, y and z). The vibrations are collected at different rotation per minute (rpm) and different runs.  Below, the user can get acquainted with the dataset by visualizing the vibration measurements in the three directions (x, y and z) for different rpm and runs. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://data.phmsociety.org/wp-content/uploads/sites/9/2023/06/PHM2023dc_fig1.png\" alt=\"MarineGEO circle logo\" style=\"height: 375px; width:800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/Data_Challenge_PHM2023_training_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the data if necessary\n",
    "fetch_and_unzip_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/Data_Challenge_PHM2023_training_data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the data if necessary\n",
    "fetch_and_unzip_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7033657409ff464394ea5e4637bb4ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Chose the context', layout=Layout(width='60%'), options=(('RMP 100…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "Splitting the dataset into training and testing sets is a common activity for validating the machine learning models. The training dataset is used to prepare a model, i.e. to train it, whereas the testing dataset is used to evaluate the performance of the model.\n",
    "In our case, the training and the testing set are based on splitting the vibration measurements. More specifically, a random sample of 75% of the original data recorded under normal (= healthy) condition is used for training whereas the remaining 25% is used for testing.  In the rest of the document, we will only refer to this split to analyze the performance of the methodology for the sake of the computational time. However, to have more reliable results, multiple splits should be performed, i.e. multiple random samples should be extracted for the training and testing sets. The interested reader can refer to [Fingerhut et al. [2]](#fingerhut2024) to see the results when 100 random splits are generated.\n",
    "\n",
    "\n",
    "\n",
    "The testing set is then created by combining:\n",
    "1. **Normal condition**: The 25% healthy data that was held back (not used in the training set)\n",
    "2. **Anomaly condition**: Vibration data characterized by pitting level 1-8 (two more than in the training set). For each level of pitting, there are between 267 and 304 samples in the test set that were recorded at different speeds and torques.\n",
    "\n",
    "The figure below illustrates the train-test split.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_train-test-split.png?raw=1\" alt=\"Overview train-test split\" style=\"width:1000px;\"/>\n",
    "\n",
    "Furthermore, from the test set are removed operating conditions (i.e. combinations of rotational speed and torque) which did not appear in the training set (this can happen if during the random train-test split all measurements with the same operating conditions end up only in the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, the original data in the training set is prepared for the analysis.\n",
    "- The **time series data** is transformed into frequency-bands. Typically, a frequency-band characterize the vibration behaviour in a specific range of frequencies (measured in Hertz [Hz]). We refer to a frequency-band in terms of \"order\". It is worth to notice that a frequency measure captures the number of events per second whereas the \"order\" captures the number of events per revolution of the rotating element. Finally, a order-transformation is applied to standardize the data. This step is important and a common preprocessing step for prognostics and health management analysis. For details, the interested reader is referred to the vibration alignment section of [Fingerhut et al. [1]](#fingerhut2023). All order-transformed vibration measurements are organized in a matrix which we call **performance matrix V**.\n",
    "- Metadata is created from the original dataset. The metadata contains the parameters at which the vibrations were measured:\n",
    "    - The **torque** expresses the rotational force in terms of Newton meter.\n",
    "    - The **rotation speed** expresses, how fast the gearbox is rotating in terms of revolutions per minute.\n",
    "    - It additionally hosts information aboute the **measurement direction** (x, y and z) and the **sample ids**, to be able to differentiate beteween multiple measurements at the same rotation speed and torque.\n",
    "\n",
    "    <mark> AMUR > Fabian, the metadata table hosts more information than the one reported in the bullet points above FFNG: Added bulletpoint with additional information. </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief excerpt of the **performance matrix V** is shown below.\n",
    "\n",
    "In the performance matrix V, all order-transformed vibration measurements are organized in a matrix, where each column contains a frequency band which is two orders long. The exact lenght of the order-transformed frequency band was determined beforehand based on the smotheness of the signal, but it can be adapted as a hyperparameter. The first column gives the total amplitude between 0.5 and 2.5 orders. <mark> FFNG: Added explanation of V here.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_0.5-2.5</th>\n",
       "      <th>band_2.5-4.5</th>\n",
       "      <th>band_4.5-6.5</th>\n",
       "      <th>band_6.5-8.5</th>\n",
       "      <th>band_8.5-10.5</th>\n",
       "      <th>band_10.5-12.5</th>\n",
       "      <th>band_12.5-14.5</th>\n",
       "      <th>band_14.5-16.5</th>\n",
       "      <th>band_16.5-18.5</th>\n",
       "      <th>band_18.5-20.5</th>\n",
       "      <th>...</th>\n",
       "      <th>band_80.5-82.5</th>\n",
       "      <th>band_82.5-84.5</th>\n",
       "      <th>band_84.5-86.5</th>\n",
       "      <th>band_86.5-88.5</th>\n",
       "      <th>band_88.5-90.5</th>\n",
       "      <th>band_90.5-92.5</th>\n",
       "      <th>band_92.5-94.5</th>\n",
       "      <th>band_94.5-96.5</th>\n",
       "      <th>band_96.5-98.5</th>\n",
       "      <th>band_98.5-100.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.005449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.015687</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.018125</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.018651</td>\n",
       "      <td>0.012879</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.026134</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.017948</td>\n",
       "      <td>0.095809</td>\n",
       "      <td>0.018065</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039877</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.007278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.003673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       band_0.5-2.5  band_2.5-4.5  band_4.5-6.5  band_6.5-8.5  band_8.5-10.5   \n",
       "index                                                                          \n",
       "0          0.000083      0.001135      0.000332      0.000749       0.001604  \\\n",
       "1          0.000030      0.000419      0.000701      0.000212       0.000586   \n",
       "2          0.000092      0.000264      0.000134      0.000106       0.001203   \n",
       "3          0.001093      0.000285      0.000084      0.000135       0.000072   \n",
       "4          0.001949      0.000557      0.000211      0.000272       0.000150   \n",
       "\n",
       "       band_10.5-12.5  band_12.5-14.5  band_14.5-16.5  band_16.5-18.5   \n",
       "index                                                                   \n",
       "0            0.001779        0.002436        0.000905        0.003705  \\\n",
       "1            0.000226        0.000414        0.000210        0.000723   \n",
       "2            0.000446        0.018651        0.012879        0.029916   \n",
       "3            0.000105        0.000596        0.002880        0.000250   \n",
       "4            0.000162        0.000396        0.009223        0.000645   \n",
       "\n",
       "       band_18.5-20.5  ...  band_80.5-82.5  band_82.5-84.5  band_84.5-86.5   \n",
       "index                  ...                                                   \n",
       "0            0.000203  ...        0.002588        0.004577        0.005826  \\\n",
       "1            0.000209  ...        0.003337        0.007597        0.015687   \n",
       "2            0.006926  ...        0.010886        0.011106        0.026134   \n",
       "3            0.000562  ...        0.039877        0.013978        0.009773   \n",
       "4            0.002202  ...        0.016658        0.007494        0.015543   \n",
       "\n",
       "       band_86.5-88.5  band_88.5-90.5  band_90.5-92.5  band_92.5-94.5   \n",
       "index                                                                   \n",
       "0            0.001566        0.003492        0.019335        0.006759  \\\n",
       "1            0.003851        0.010378        0.018125        0.005149   \n",
       "2            0.009216        0.017948        0.095809        0.018065   \n",
       "3            0.006686        0.010765        0.014013        0.009631   \n",
       "4            0.010917        0.013567        0.012068        0.006568   \n",
       "\n",
       "       band_94.5-96.5  band_96.5-98.5  band_98.5-100.5  \n",
       "index                                                   \n",
       "0            0.009075        0.004589         0.005449  \n",
       "1            0.002389        0.001015         0.001774  \n",
       "2            0.016754        0.004955         0.003817  \n",
       "3            0.006961        0.002484         0.007278  \n",
       "4            0.004345        0.002025         0.003673  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we need something to showcase how the training and testing dataset looks like (columns, their meaning, etc)\n",
    "df_V_train, meta_data_train, df_data_healthy_test, f = get_and_preprocess_healthy_data()\n",
    "df_orders_test, meta_data_test = get_and_preprocess_unhealthy_data(df_data_healthy_test, f)\n",
    "\n",
    "# extract list of frequency band columns for later usage\n",
    "cols_ = df_V_train.columns\n",
    "BAND_COLS = cols_[cols_.str.contains('band')].tolist()\n",
    "\n",
    "df_V_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above has n columns which represent vibration measurements and m rows which represent the order-transformed frequency bands.\n",
    "\n",
    "    AMUR > Fabian, the refernece to \"frequency bins\" just falls from the sky. Is this the same as frequency bands?. Introduce in the section above this term. E.g. \"All order-transformed vibration measurements are organized in a matrix\" where XXX is organized in frequency bins and YYY is organized in band ranges. Or something similar. FFNG: Added information before cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we illustrate the corresponding metadata, consisting of the previously described process parameters `rotational speed [RPM]` and `torque [Nm]`, and some additional information regarding the vibration measurement direction (`direction`). Each row corresponds to the same row in the performance matrix. For instance, the very first measurement contains vibrations recorded at 3000 RPM and 50 NM, for the vibration measurement direction x.\n",
    "\n",
    "    AMUR > Fabian, the sentence above is not true right? I don't see 300 NM. FFNG: the order must have in the meantime? I adapted.\n",
    "    AMUR > Fabian, what\\s the point of showing sample id and unique sample id? Does it carry information useful for the SK which follows? or conversely, why they were created? To map operational parameters with keywords which you eventually use to map with the matrix V? FFNG: It was previously used for the mapping. At the moment I am not sure anymore. I hide it from the output for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotational speed [RPM]</th>\n",
       "      <th>torque [Nm]</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>50</td>\n",
       "      <td>psd_x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000</td>\n",
       "      <td>50</td>\n",
       "      <td>psd_y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>50</td>\n",
       "      <td>psd_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>psd_x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>psd_y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rotational speed [RPM]  torque [Nm] direction\n",
       "0                    3000           50     psd_x\n",
       "1                    3000           50     psd_y\n",
       "2                    3000           50     psd_z\n",
       "3                     400          100     psd_x\n",
       "4                     400          100     psd_y"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data_train.drop(columns=['sample_id', 'unique_sample_id']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the use case, a finite number of combinations of speed and torque values are present. These combinations of process parameters, characterizing the behavior of the gearbox, describe the **operating modes** of the asset. ~~In this use case, the operating modes are 76.~~ <mark> Fabian: **Can we remove the last sentence that I just striked through and the following code cell?** Later we repeat this discussion in \"Extracting operating modes\" stating that there are 77 instead of 76 operating modes. This is due to one operating mode not containing any measurements, which is counted when we state that there are 77 operating mode and not counted when there are 76 operating modes. </mark>\n",
    "<a id='number-of-oms'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are 76 unique operating modes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_oms = len(meta_data_train['unique_sample_id'].str[:-2].unique())\n",
    "display(md(f'There are {n_oms} unique operating modes.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "\n",
    "The methodology can be divided into the offline and online phases. The offline phase focuses on performance profiling. More specifically, the operating modes of healthy assets are extracted and mapped to the expected performances. The online phase focuses on anomaly detection by exploiting the performance profiles extracted in the previous phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Offline phase\n",
    "\n",
    "The general workflow of the offline phase is reported in the figure below.\n",
    "\n",
    "    AMUR> Fabian, the figure mentions: process sensor meausurement.  This name is never used before. What is it? What you call process parameters? If the latter, update the figure accordingly. Same applies to performance sensor measurement and vibration amplitude. What is it? Please, check that there is a mapping with the terms in the figure and text (ctrl+F, ctrl+C and ctrl+V for each term you are not sure is actually used or meaningful) FFNG: Replaced figure. Need to push to github.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_offline-phase.png?raw=1\" alt=\"Overview offline phase\" style=\"width:800px;\"/>\n",
    "\n",
    "\n",
    "This phase can be divided into 3 steps (See Figure above). The main goals of these phases can be summarized as follow:\n",
    "1. Characterizing performance  behaviour. High-dimensional vibration signals are characterized in terms of a few vibration components that capture the fundamental vibration behaviour.\n",
    "2. Extracting operating modes. Based on the process parameters, the operating modes are extracted and associated with each measurement. In the original methodology, a measurement is taken at a specific timestamp. The gearbox dataset does not contain information about the point in time when a measurement was taken.\n",
    "\n",
    "    AMUR > Fabian, in section Extracting operating modes there is no reference to 'timestamp'. Can you go to that section and add at least one sentence which mentions them? FFNG: Changed \"timestamp\" to \"measurement\" and added clarification later in the text, that this part can be adapted to timeseries data.\n",
    "\n",
    "3. Linking operating modes to performance behaviour. For each operating mode a performance fingerprint is associated.\n",
    "\n",
    "\n",
    "\n",
    "#### Characterizing performance behaviour\n",
    "\n",
    "In this step, we extract for each asset its characteristic **performance behaviour**. These behaviours are extracted by applying a _non-negative matrix factorization_ (NMF) [[3](#nmf)] to the performance matrix V. \n",
    "We apply NMF to decompose the matrix V into a separate weight matrix $\\mathbf{W}$ and a component matrix $\\mathbf{H}$, i.e., $\\mathbf{V} \\approx \\mathbf{W} \\times \\mathbf{H}$. The latter two matrices are lower-dimensional matrices and the approximation contains the essential information about the asset's performance. Hence, NMF reveals underlying patterns in the performance data by representing it in a simpler form.\n",
    "\n",
    "Compared to other decomposition and dimensionality reduction methods (such as PCA or auto-encoders), NMF has the advantage that it decomposes the vibrations into entirely positive values. This is important for interpretability, because non-negative factors correspond to real-world quantities that cannot take negative values, such as the frequency transformed vibrations. As a result, the components in $\\mathbf{H}$ can be understood in combination with the weights in $\\mathbf{W}$ as additive combinations. This makes it easier to understand how the vibrations contribute to the overall performance behavior.\n",
    "    \n",
    "> AMUR> Fabian, I don't see why a matrix with only positive values is 'interpretable'. Or I don't understand what make a matrix interpretable. Either remove the sentence, either explain what you meant.\n",
    ">\n",
    "> FFNG: Added some explanation. If you don't agree, we can delete it. But I think it has certain advantages over for instance PCA or auto-encoders.\n",
    ">\n",
    "> RVEB: the explanation makes sense to me.\n",
    "\n",
    "The component matrix $\\mathbf{H}$ contains a set of $h$ representative components allowing to express performance behaviour in a standardized way and thus allowing to compare performance across operating modes and across assets. The matrix $\\mathbf{W}$ contains the weights for reconstructing the original performance matrix $\\mathbf{V}$. Each element in $\\mathbf{W}$ can be interpreted as the weights of the building blocks in $\\mathbf{H}$ needed to reconstruct a vibration signal encoded in the performance matrix $\\mathbf{V}$.\n",
    "\n",
    "To decide the value $h$ of components we decompose the matrix multiple times using an increasing number of components. In the next code section, we apply the NMF to up to 50 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting NMF with varying number of components: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_N_COMPONENTS = 50    # maximum number of components used to recompute\n",
    "\n",
    "df_nmf_models = extract_nmf_incremental(df_V_train, max_n_components=MAX_N_COMPONENTS, timestamps=df_V_train.index, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we illustrate the decomposition of the performance matrix V.\n",
    "\n",
    "    AMUR> Fabian, what is vibration data? Can you just refer to the matrix you want to decompose? FFNG: Changed. The following sentences further describe the visualisation.\n",
    "\n",
    "The identification of the number of components to describe the operating context is not trivial. To facilitate this process, in the plot below we provide the following plots.\n",
    "- The top two plots visualize how well the performance matrix $\\mathbf{V}$ can be approximated\n",
    "    - The left-top plot illustrates the cumulative explained variance of a principal component analysis (PCA) of the performance matrix $\\mathbf{V}$. It merely serves as an indication of an upper bound for how well the signal could be expressed using PCA.\n",
    "    - The top-righ plot illustrates the reconstruction error of the performance matrix $\\mathbf{V}$ using the NMF components. The reconstruction error is calculated as the Frobenius norm of the difference between the original matrix and the reconstructed matrix.\n",
    "- The bottom lineplots illustrate the NMF components.\n",
    "\n",
    "In the interactive widget below, the reader can select how the number of components are determined. This number can be based on:\n",
    "\n",
    "- a threshold for the cumulative _explained variance_ from PCA (e.g., 95%). In this case, the smallest number of components for which this threshold is exceeded, is selected.\n",
    "- the reconstruction error plot. In this case, the _knee point_ is identified and used to determine the number of components. The identification of this knee point is an extension of the methodology presented in [2] for automated hyperparameter tuning for operative context detection.\n",
    "- both the aforementioned methods. In this case the highest of the two values is used as the number of components.\n",
    "\n",
    "Each of the $h$ components from $\\mathbf{H}$ is illustrated in a separate lineplot. The components serve as building blocks of the observed vibration signals, revealing common patterns in the vibration measurements.\n",
    "\n",
    "For the suggested settings, it can be observed that components 1 and 2 form the basis vectors for peaks that are observed at 40 and 80 orders. These peaks are expected, as the driving gear has 40 teeth. It is common to see peaks for multiples of the number of teeth, hence there is also a peak at 80 orders. Components 3, 4 and 5 encode smaller peaks which model the noise floor.\n",
    "\n",
    "The reader is invited to experiment with multiple parameters below to verify how they affect the number of components extracted. The domain expert can use this code section to incorporate their domain knowledge to make the extraction and representation of implicit contexts more data-efficient .\n",
    "\n",
    "The interactive widget allows to go beyond the analysis performed in the original methodology [[2](#fingerhut2024)]. By increasing the range of acceptable components or by increasing the threshold (in case of using the explained variance as criterium)\n",
    "    \n",
    "> AMUR: Robbert, what does it mean 'increase' the selection criteria?! The selection criteria are in drp down list. What does it mean: increase?\n",
    ">\n",
    "> RVEB: fixed.\n",
    "\n",
    ", more components will be used in the decomposition. This makes the vibration fingerprints more granular, but also more prone to noise. In contrast, decreasing the accepted range or the selection criterium will yield less components, making the resulting fingerprints less informative, but also less prone to noise.\n",
    "\n",
    "The number of components should be neither too low, which would lead to inaccurate and too general vibration fingerprints, nor should it be too high, which would lead to too specific fingerprints which could model the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03381ba32e204fb4b077cae53824e40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='How to select the number of components for NMF', layout=Layout(wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "saved_values = illustrate_nmf_components_interactive(df_V_train, df_nmf_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the number of components determined in the interactive figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We are using **5 components**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_COMPONENTS = saved_values['n_components']\n",
    "COMPONENT_COLUMNS = list(range(N_COMPONENTS))  # used later\n",
    "model = df_nmf_models[(df_nmf_models.n_components == N_COMPONENTS)].iloc[0]\n",
    "plural = 's' if N_COMPONENTS > 1 else ''\n",
    "display(md(f\"We are using **{N_COMPONENTS} component{plural}**.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AMUR: Robbert: At the end of this section, the reader played with the number of components. Did we test multiple options? Should we not put a limit to avoid that the notebook breaks?\n",
    ">\n",
    "> RVEB: Tested for both extreme cases (n_components = 1 and very large). Figures also adapt automatically to remain as readable as possible.\n",
    "\n",
    "While the starterkit is able to cope with different hyperparameters, in the text that follows, we assume that 5 components were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting operating modes.\n",
    "\n",
    "This step focuses in context-segmentation of context-dependent behaviors of the assets; in other words, this section highlights how the operating modes of the asset are extracted. In this use case, the extraction of the operating modes is straightforward since it maps with the possible combinations of rotational speed and torque.  \n",
    "For clarity, in the text that follows, the operating modes are reported using the pattern `@ X rpm, Y Nm`, where `X` corresponds to the rotational speed and `Y` corresponds to the torque. All measurements with the same rotational speed and torque are summarized in the same operating mode (OM). There are as many operating modes, as there are unique combinations of `X` and `Y`.\n",
    "\n",
    "Note that, while in the starter kit, operating modes are extracted independently of the time, it is possible to adapt the operating mode extraction to time-series data, where time spans are assigned a specific operating mode, instead of single measurements. <mark> FFNG: Added this brief clarification. </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>400</th>\n",
       "      <th>500</th>\n",
       "      <th>600</th>\n",
       "      <th>700</th>\n",
       "      <th>800</th>\n",
       "      <th>900</th>\n",
       "      <th>1000</th>\n",
       "      <th>1200</th>\n",
       "      <th>2100</th>\n",
       "      <th>2700</th>\n",
       "      <th>3000</th>\n",
       "      <th>3600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>OM 1</td>\n",
       "      <td>OM 2</td>\n",
       "      <td>OM 3</td>\n",
       "      <td>OM 4</td>\n",
       "      <td>OM 5</td>\n",
       "      <td>OM 6</td>\n",
       "      <td>OM 7</td>\n",
       "      <td>OM 8</td>\n",
       "      <td>OM 9</td>\n",
       "      <td>OM 10</td>\n",
       "      <td>OM 11</td>\n",
       "      <td>OM 12</td>\n",
       "      <td>OM 13</td>\n",
       "      <td>OM 14</td>\n",
       "      <td>OM 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>OM 16</td>\n",
       "      <td>OM 17</td>\n",
       "      <td>OM 18</td>\n",
       "      <td>OM 19</td>\n",
       "      <td>OM 20</td>\n",
       "      <td>OM 21</td>\n",
       "      <td>OM 22</td>\n",
       "      <td>OM 23</td>\n",
       "      <td>OM 24</td>\n",
       "      <td>OM 25</td>\n",
       "      <td>OM 26</td>\n",
       "      <td>OM 27</td>\n",
       "      <td>OM 28</td>\n",
       "      <td>OM 29</td>\n",
       "      <td>OM 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>OM 31</td>\n",
       "      <td>OM 32</td>\n",
       "      <td>OM 33</td>\n",
       "      <td>OM 34</td>\n",
       "      <td>OM 35</td>\n",
       "      <td>OM 36</td>\n",
       "      <td>OM 37</td>\n",
       "      <td>OM 38</td>\n",
       "      <td>OM 39</td>\n",
       "      <td>OM 40</td>\n",
       "      <td>OM 41</td>\n",
       "      <td>OM 42</td>\n",
       "      <td>OM 43</td>\n",
       "      <td>OM 44</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>OM 45</td>\n",
       "      <td>OM 46</td>\n",
       "      <td>OM 47</td>\n",
       "      <td>OM 48</td>\n",
       "      <td>OM 49</td>\n",
       "      <td>OM 50</td>\n",
       "      <td>OM 51</td>\n",
       "      <td>OM 52</td>\n",
       "      <td>OM 53</td>\n",
       "      <td>OM 54</td>\n",
       "      <td>OM 55</td>\n",
       "      <td>OM 56</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>OM 57</td>\n",
       "      <td>OM 58</td>\n",
       "      <td></td>\n",
       "      <td>OM 59</td>\n",
       "      <td>OM 60</td>\n",
       "      <td>OM 61</td>\n",
       "      <td>OM 62</td>\n",
       "      <td>OM 63</td>\n",
       "      <td>OM 64</td>\n",
       "      <td>OM 65</td>\n",
       "      <td>OM 66</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>OM 67</td>\n",
       "      <td>OM 68</td>\n",
       "      <td>OM 69</td>\n",
       "      <td>OM 70</td>\n",
       "      <td>OM 71</td>\n",
       "      <td>OM 72</td>\n",
       "      <td>OM 73</td>\n",
       "      <td>OM 74</td>\n",
       "      <td>OM 75</td>\n",
       "      <td>OM 76</td>\n",
       "      <td>OM 77</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      100    200    300    400    500    600    700    800    900    1000   \n",
       "50    OM 1   OM 2   OM 3   OM 4   OM 5   OM 6   OM 7   OM 8   OM 9  OM 10  \\\n",
       "100  OM 16  OM 17  OM 18  OM 19  OM 20  OM 21  OM 22  OM 23  OM 24  OM 25   \n",
       "200  OM 31  OM 32  OM 33  OM 34  OM 35  OM 36  OM 37  OM 38  OM 39  OM 40   \n",
       "300  OM 45  OM 46  OM 47  OM 48  OM 49  OM 50  OM 51  OM 52  OM 53  OM 54   \n",
       "400  OM 57  OM 58         OM 59  OM 60  OM 61  OM 62  OM 63  OM 64  OM 65   \n",
       "500  OM 67  OM 68  OM 69  OM 70  OM 71  OM 72  OM 73  OM 74  OM 75  OM 76   \n",
       "\n",
       "      1200   2100   2700   3000   3600  \n",
       "50   OM 11  OM 12  OM 13  OM 14  OM 15  \n",
       "100  OM 26  OM 27  OM 28  OM 29  OM 30  \n",
       "200  OM 41  OM 42  OM 43  OM 44         \n",
       "300  OM 55  OM 56                       \n",
       "400  OM 66                              \n",
       "500  OM 77                              "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_operating_modes = get_operating_modes()\n",
    "df_operating_modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot table above depicts the names of the extracted operating modes in function of the two process parameters speed and torque. For example, an asset running in `OM 1` (top left) runs at a rotational speed of 100 rpm and a torque of 50 Nm, whereas an asset running in `OM 77` (bottom right) runs at 1200 rpm and 500 Nm. For this train-test split, there is a total of 77 operating modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='Link-operating-modes-to-performance-behaviour'></a>Linking operating modes to performance\n",
    "In this step, the performance behaviour is linked with the operating modes. This allows to derive **context-sensitive performance fingerprints**. As each vibration measurement is assigned to an operating mode, it is possible to derive fingerprints by aggregating all rows in $\\mathbf{W}$ annotated with the same operating mode.\n",
    "In the following, for each of the 77 operating modes, the reader can visualize the performance behaviour for each individual measurement based on the selected statistics (e.g. mean).\n",
    "Note that for the 77 operating modes, not all of them will have measurements in the training set. These operating modes will therefor not have a fingerprint, which explains why the number of operating modes mentioned at [the end of section 2 ](#number-of-oms) can be lower than 77.\n",
    "\n",
    "<mark> FFNG: Added that operating mode 28 has no meaurements, and therefore also no fingerprint. </mark>\n",
    "\n",
    "> RVEB: I reworded it to make it more generic. On my own laptop, the split was different, resulting in only 73 OMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b67595c89f44e0dbf594e652c5e29db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Select the operating mode', layout=Layout(width='60%'), options=((…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_fingerprints(model, df_V_train, meta_data_train, df_operating_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map reports the strength (color) of the weights of the vibration components (cf. NMF component illustration above, x-axis) for the three different directions (y-axis).\n",
    "The weights of the vibration components, which span from 0 to 0.343, are evalued in terms of power spectral density (PSD).\n",
    "\n",
    "    AMUR> Fabian, is correct the sentence above? I refactored your text. Can you also replace XXX? FFNG: replaced.\n",
    "    \n",
    "The PSD is a representation of how the power of the signal is distributed across different frequency components, providing insight into the dominant frequencies present in the vibration data over time.\n",
    "\n",
    "From the analysis of the operating modes, it is possible to observe that they present distinct performance fingerprints, as can be observed for instance when comparing operating mode (OM) 1 with OM 50. Whereas OM 1 predominantly expresses vibrations in the third component related to the noise floor for all vibration directions, OM 50 predominantly expresses vibrations in the first two components related to 40 and 80 orders. At the same time, operating modes with similar operating conditions show similar vibration fingerprints. For instance, OM 1 and OM 16 only differ slightly in the torque, which results (as expected) in similar vibration fingerprints.\n",
    "\n",
    "At the end of this offline phase, each operating mode has its own performance profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Online phase\n",
    "\n",
    "The general workflow of the online phase is reported in the figure below. Note that due to the nature of the data used in this use case, some steps are no longer necessary or become very simplified. These steps are still included for the sake of completeness.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_online-phase.png?raw=1\" alt=\"Overview online phase\" style=\"width:800px;\"/>\n",
    "\n",
    "This phase can be divided in five steps (See Figure above). The main goals of these phases can be summarized as follows:\n",
    "\n",
    "4. **Windowing incoming streaming**. Streaming data is usually divided into batches. This activity is generally performed to avoid processing each new received data point as it arrives. However, in our use case there no streaming data, for this reason windowing is not needed and this step is skipped.\n",
    "\n",
    "5. **Detecting of the operating context**. Each timestamp is associated with an operating mode which was identified during the offline phase. In this use case, operating modes are linked to vibration measurements without timestamps.\n",
    "\n",
    "    AMUR > Fabian, do we need or not timestamps? If not, make it clear in the offline phase too. For instance write: In this use case, operating modes are not associated to timestamps but only to ... FFNG: We don't need timestamps in the starterkit. I changed the text in the offline phase accordingly. \n",
    "\n",
    "6. **Deriving the performance profiles**. Each vibration measurement is characterized in terms of the vibration components that were extracted in the first step of the offline phase.\n",
    "\n",
    "7. **Estimating the fingerprint offset**. For each vibration measurement, the _offset_ is calculated between the online profiles and the offline fingerprints. The offset quantifies to what extent the observed vibration behaviour differs from the expected vibration behaviour expressed through the fingerprints.\n",
    "\n",
    "8. **Deriving alarms**. In the original paper [[2](#fingerhut2024)], based on the offset, an anomaly score is computed for each timestamp. This score is monitored over time to trigger alarms. In this use case, the monitoring of the anomaly score is not possible since the timestamps in the test set are not ordered. For this reason, this step is skipped.\n",
    "\n",
    "\n",
    "#### Detecting of the operating context and deriving the performance profiles.\n",
    "\n",
    "As in the offline phase, the operating modes are extracted from each timestamp present in the testing dataset. Then, the performance profiles are derived for each vibration measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of extracting the performance profiles is to capture and represent the key characteristics of the vibration signal in a way that reflects the asset's vibration behavior. Therefore, this step essentially mirrors the step 1 of the offline phase, with the exception that the vibration components are not derived again.\n",
    "\n",
    "The performance profiles approximate the vibration signal in terms of the representative vibration components that were extracted during step 1 of the offline phase. Similarly to this step, the approximation contains the essential information about the asset's performance.\n",
    "These profiles are extracted by applying NMF to the processed vibration measurements from the online phase, denoted as $\\mathbf{v}'$.\n",
    "The processed measurements $\\mathbf{v}'$ are a vector with the same number of elements as the columns in the performance matrix $\\mathbf{V}$. Since the step is applied to a data stream, multiple samples are no longer stacked together as in $\\mathbf{V}$ during the offline phase.\n",
    "\n",
    "In the online phase, NMF now uses the $h$ components from matrix $\\mathbf{H}$, which were already identified during the offline phase. Specifically, NMF decomposes $\\mathbf{v}'$ into a weight vector $\\mathbf{w}'$ using the fixed component matrix $\\mathbf{H}$, such that $\\mathbf{v}' \\approx \\mathbf{w}' \\times \\mathbf{H}$.\n",
    "\n",
    "Since the same components are used from the offline phase, the profiles match the format of the vibration fingerprints, making them suitable for subsequent anomaly detection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "0      71\n",
      "1      71\n",
      "2      71\n",
      "3      18\n",
      "4      18\n",
      "       ..\n",
      "640     7\n",
      "641     7\n",
      "642    52\n",
      "643    52\n",
      "644    52\n",
      "Name: cluster_label_unique, Length: 645, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'om' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_W_offline, df_W_online, fingerprints, test_vibration_measurement_periods_meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_df_W_offline_and_online\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_V_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_data_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_data_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_orders_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/compact/nmf_profiling.py:279\u001b[0m, in \u001b[0;36mget_df_W_offline_and_online\u001b[0;34m(_df_V_train, meta_data_train, meta_data_test, model, df_orders_test)\u001b[0m\n\u001b[1;32m    262\u001b[0m torque \u001b[38;5;241m=\u001b[39m meta_data_test[meta_data_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_sample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m unique_sample_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorque [Nm]\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mtry:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    om = cluster_label_unique_name_mapping[\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    om = -1\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m measurement_period \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    274\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    275\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m: group,\n\u001b[1;32m    276\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_sample_id\u001b[39m\u001b[38;5;124m'\u001b[39m: unique_sample_id,\n\u001b[1;32m    277\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrpm\u001b[39m\u001b[38;5;124m'\u001b[39m: rpm,\n\u001b[1;32m    278\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorque\u001b[39m\u001b[38;5;124m'\u001b[39m: torque,\n\u001b[0;32m--> 279\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_cluster_label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mom\u001b[49m}\n\u001b[1;32m    280\u001b[0m test_vibration_measurement_periods\u001b[38;5;241m.\u001b[39mappend(group)\n\u001b[1;32m    281\u001b[0m test_vibration_measurement_periods_meta_data\u001b[38;5;241m.\u001b[39mappend(measurement_period)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'om' is not defined"
     ]
    }
   ],
   "source": [
    "df_W_offline, df_W_online, fingerprints, test_vibration_measurement_periods_meta_data = get_df_W_offline_and_online(df_V_train, meta_data_train, meta_data_test, model, df_orders_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the derived weights $\\mathbf{w}'$, below the derived weights for a single measurement are illustrated.\n",
    "A vibration measurement consists of the three measurements directions (namely the x-, y- and z-axis, corresponding to the rows in the colormap below).\n",
    "The weights that are illustrated below consist of $h$ columns each, where $h$ represents the number of components determined in the offline phase.\n",
    "The format of the weights is the same as that of the fingerprint shown in the [previous section](#Link-operating-modes-to-performance-behaviour).\n",
    "\n",
    "    AMUR> Fabian, before you reported psd_x, etc. Now, the psd is gone from the matric below. Is this correct? FFNG: I changed the code in the viz.viz.show_fingerprints such that it should give as y-axis ticks \"x\" instead of \"psd_x\". Note that, because the compact module is downloaded and installed from github, the changes in compact modules will only come into effect, once we push the code to github. We will need to check, whether this will be actually the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(df_W_online, meta_data_test, df_operating_modes, N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we select measurement 10 from the test set, which was taken at 1000 rpm and 50 Nm and therefore corresponds to operating mode 10. It can be observed that components 1 and 4 have high values in all 3 measurement directions.\n",
    "\n",
    "> AMUR> Fabian, the dropdown section asks: period. What is a period? This was never mentioned before. Find a place to mention it (before the table) FFNG: Added 'As vibrations measurements are taken over time, we refer to them as \"period\".' Alternatively, we could rename the dropdown menu to \"which measurement\" @Robbert, what do you think?\n",
    ">\n",
    "> RVEB: Changed to \"select measurement\" and removed the explanation for period in the text, since it's no longer needed.\n",
    "\n",
    "When comparing this to the vibration fingerprint at OM 10 (see the [interactive widget for showing the fingerprints](#Link-operating-modes-to-performance-behaviour) and choose operating mode 10), we observe a similar behaviour. Hence, for this example, the observed beahviour is similar to the expected vibration behaviour, which is indicative that this vibration measurement can be considered as non-anomalous. This comparison is a representative example on how to check whether the vibration measurement is indicative of an anomaly.\n",
    "\n",
    "At the end of this step, for each online vibration measurement, a performance profile is extracted, which is of the same format as the vibration fingerprints.\n",
    "The performance profile contains the essential information of the vibration signal, while being of the same format as the vibration fingerprints.\n",
    "\n",
    "In the next section, the performance profiles are compared to the expected vibrations from the fingerprints in an automated fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the fingerprint offset\n",
    "\n",
    "In order to assess\n",
    "whether the performance of asset $i$ ($i = 1, . . . , n$) is normal or anomalous, it is necessary to quantify the distance - offset - between the observed and expected fingerprints. For this reason, for each performance measurement, the cosine distance between the derived weights $\\mathbf{w}'$ and the fingerprint $f_{ij} \\in F_i$, corresponding to the detected operating mode $O_{ij}$, ($j = 1, . . . , k_i$), is used to estimate the offset:\n",
    "\n",
    "$$ d_{cos}(\\mathbf{w}_i', \\mathbf{f}_{ij}) = 1 - \\dfrac{\\mathbf{w}_i' \\cdot \\mathbf{f}_{ij}}{\\|\\mathbf{w}_i'\\| \\ \\|\\mathbf{f}_{ij}\\|} $$\n",
    "\n",
    "where || · || is the magnitude of the corresponding vector. In this use case, being present only one asset, $n$=1.\n",
    "\n",
    "The derived weight vectors represent individual direction measurements as a result of decomposing per individual direction. The weight vectors are appended into a single ($3\\times h$)-dimensional vector $\\mathbf{w}_i'$  of rank 1 that is compared to the fingerprint $f_{1j}$ of the corresponding operating mode $O_{1j}$.\n",
    "\n",
    "Once the offsets are extracted, it is possible to evaluate to which extent they can be exploited for anomaly detection. For this reason, in the next code section, for each operating mode the offset of the derived weights to the corresponding fingerprint is computed. More specifically, a pivot table is created and shown below with the cosine distance in order to compare distances between measurements and all 76 . <mark> FFNG: This paragraph's reading flow was a bit off, so I changed it. BTW, why do we only show parts of the table? TODO: IN THIS TABLE, THE NAMES OF THE OPERATING MODES (0 - 75) DO NOT CORRESPOND TO THE NAMES PRESENTED IN PREVIOUS SECTIONS (1 - 77, where 0 is skipped and OM 28 has no fingerprints). We should stick to a single naming. </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_operating_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine = get_pivot_table(df_W_online, fingerprints, test_vibration_measurement_periods_meta_data)\n",
    "\n",
    "print(f'Pivot table with distances to all fingerprints (0 - {len(df_cosine.columns) - 7}), corresponding rpm and torque values, and additional information on the anomaly condition:')\n",
    "display(df_cosine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "In this section is evaluated to what extent the performance offset, computed at the last step of the methodology, can be exploited for detecting anomalies. In the plot below is analzed the relation between offset distance and pitting level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_cosine, y='distance_to_own_cluster_center', x='pitting_level')\n",
    "ax.set_ylabel('Distance to context sensitive fingerprint')\n",
    "ax.set_title(f'Distance to own cluster center per pitting level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplots above, a clear difference in terms of offset between healthy and faulty gearboxes can be observed. Indeed, healthy gearboxes, the one with pitting level = 0, have offset close to 0 wherease faulty gearbox, the one with pitting level above 0, have higher offset. This is expected, as the context-sensitive fingerprint is derived from healthy data without pitting and data with pitting is likely to have vibration patterns not present in the healthy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot provides only a partial view of the ability of the methodology to identify anomalies. To fully appreciate the performance of the methodology, it is necessary to adequately detect faulty gearboxes as anomalies.\n",
    "Whether a datapoint is labelled as anomalous depends on a predefined distance threshold.\n",
    "If the predefined distance threshold is exceeded, an anomaly is raised.\n",
    "To make our validation independent of the choice of this hyperparameter, we construct a ROC-curve by varying this distance threshold.\n",
    "\n",
    "A ROC curve is a visualisation for evaluating the performance of the anomaly detection. It plots the *true positive rate* (TPR) against the *false positive rate (FPR)* at different distance thresholds. In this use case, the TPR represents the rate of measurements exposed to pitting that were actually detected as anomaly whereas the FPR is the rate of measurements that were not exposed to pitting and still detected as anomaly.\n",
    "The higher is the area under the ROC-curve, the better the model is at detecting anomalies.\n",
    "\n",
    "Below the ROC-curve is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(df_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above it can be observed that the anomaly detection generally performs well with an area under the curve (AUC) of 0.979.\n",
    "Operators aim for a high TPR while minimizing false alarms (keeping the FPR low). Therefore, we additionally tracked the TPR at a stable FPR of 0.1 (TPR@FPR=0.1, red dashed line), which represents the TPR when there are 10% false positives. In this scenario, the TPR would be 96%. An alternatve objective criterium is to keep the FPR as low as possible at a high TPR.\n",
    "The plot above therefore also illustrates the FPR@TPR=0.9 (green dashed line).In that scenario, we check how many false alarms would be triggered if we want to guarantee that 90% of the gear pitting is detected. In this use case, FPR@TPR=0.9 is 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "In this SK is illustrated a data-driven methodology for contextual performance profiling and anomaly detection. The SK focuses on how to set up methodology parameters and correctly interpret its results. The methodology, validated on a gearbox that is subject to pitting, explains how to extract the contexts from an asset and how to use them to profile its performance. Furthermore, it proves that the performance profiles can be used to identify anomalies. For the latter, it has been shown that our method is able to detect most anomalies, while throwing few false alarms, as demonstrated through the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional information\n",
    "The methodology presented in this notebook is based on the papers [[1]](#fingerhut2023) and [[2]](#fingerhut2024).\n",
    "\n",
    "This Starter Kit was developed in the context of the Compact project [ToDo: Find an online reference]. For more information, please contact [info@elucidata.be](mailto:info@elucidata.be).\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Notebook\"), to deal in the Notebook without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Notebook, and to permit persons to whom the Notebook is provided to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies of the Notebook and/or copies of substantial portions of the Notebook.\n",
    "\n",
    "THE NOTEBOOK IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL SIRRIS, THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, DIRECT OR INDIRECT, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE NOTEBOOK OR THE USE OR OTHER DEALINGS IN THE NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id='fingerhut2023'>[1]</a> F. Fingerhut, S. Klein, M. Verbeke, S. Rajendran and E. Tsiporkova, \"Multi-view contextual performance profiling in rotating machinery,\" 2023 IEEE International Conference on Prognostics and Health Management (ICPHM), Montreal, QC, Canada, 2023, pp. 343-350, [doi: 10.1109/ICPHM57936.2023.10194172](https://ieeexplore.ieee.org/document/10194172).\n",
    "\n",
    "<a id='fingerhut2024'>[2]</a> F. Fingerhut, M. Verbeke and E. Tsiporkova, \"Unsupervised context-sensitive anomaly detection on streaming data relying on multi-view profiling,\" 2024 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS), Madrid, Spain, 2024, pp. 1-10, [doi: 10.1109/EAIS58494.2024.10569106](https://ieeexplore.ieee.org/document/10569106).\n",
    "\n",
    "<a id='nmf'>[3]</a> D. Lee, S. Seung, [“Learning the parts of objects by non-negative matrix factorization”](https://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf), 1999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ©, 2024, Sirris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

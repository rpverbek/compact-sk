{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914cfde-af38-4a83-8cb7-093565c3d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans,DBSCAN,AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "import sys\n",
    "import cluster_validation_metrics as cvm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import seaborn as sns\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "import os\n",
    "import kahypar as kahypar\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e6eea29-6c80-4204-8229-cc6cac7f97c9",
   "metadata": {},
   "source": [
    "## Data\n",
    "- On shore windfarm in Scotland\r",
    "- 14 wind turbines with 2MW rated powe\n",
    "- 5 years of SCADA data with 10 minutes sampling rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118077d5-6171-4ee9-8706-437636b9f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('../app/data/Penmanshiel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cee1d-bc61-416a-9574-19b5c76d3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_penmanshiel(turbine_number):\n",
    "    \n",
    "    local_file_scada = os.path.join(data_path, f'scada_T{turbine_number:02d}.csv')\n",
    "    df_scada = pd.read_csv(local_file_scada)\n",
    "    df_scada = df_scada.set_index('Datetime',drop=True)\n",
    "    \n",
    "    local_file_logs = os.path.join(data_path, f'logs_T{turbine_number:02d}.csv')\n",
    "    df_logs = pd.read_csv(local_file_logs)\n",
    "\n",
    "    return df_scada, df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfc979-ac35-49ff-a7dc-ab3f17c3d4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_penmanshiel=pd.DataFrame()\n",
    "for t_id in [1,2,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "    print(t_id)\n",
    "    df_scada, _ = read_data_penmanshiel(t_id)\n",
    "    df_scada = df_scada[(df_scada.index>=\"2018-01-01 00:00:00\") & (df_scada.index<=\"2022-12-31 23:50:00\")].copy()\n",
    "    df_scada.reset_index(inplace=True)\n",
    "    df_scada[\"Turbine\"] = f'T{t_id:02d}'\n",
    "    df_penmanshiel=pd.concat([df_penmanshiel,df_scada])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ce7b6-b0c0-45fe-a52b-ec30daf5f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel['Datetime']=pd.to_datetime(df_penmanshiel['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c5f68-1a4d-4340-9f07-b81692a6e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_penmanshiel.to_csv(\"penmanshiel_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36d3e4-e892-41fa-9aac-180c60505037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_penmanshiel=pd.read_csv(\"penmanshiel_preprocessed.csv\")\n",
    "# df_penmanshiel['Datetime']=pd.to_datetime(df_penmanshiel['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd01428-1ac1-41dd-916a-a45d91406f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_filtered = df_penmanshiel.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621bb870-8073-4d46-bcd7-9eae343f5eb7",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Motor current = Mean (Motor current axis 1,axis 2,axis 3)\n",
    "- Motor temperature = Mean (Motot temperature axis 1,axis 2,axis 3)\n",
    "- Blade angle = Mean (Blade angle A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b5d83-00e6-4000-a603-dd8e0b4dbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_filtered[\"Motor temperature (°C)\"] = df_penmanshiel_filtered[['Temperature motor axis 1 (°C)',\n",
    "                                          'Temperature motor axis 2 (°C)',\n",
    "                                          'Temperature motor axis 3 (°C)']].mean(axis=1)\n",
    "\n",
    "df_penmanshiel_filtered[\"Motor current (A)\"] = df_penmanshiel_filtered[['Motor current axis 1 (A)',\n",
    "                                          'Motor current axis 2 (A)',\n",
    "                                          'Motor current axis 3 (A)',]].mean(axis=1)\n",
    "\n",
    "temp = df_penmanshiel_filtered[['Blade angle (pitch position) A (°)','Blade angle (pitch position) B (°)','Blade angle (pitch position) C (°)']].apply(np.radians)\n",
    "# Convert angular values to Cartesian coordinates\n",
    "x = temp.apply(lambda row: np.cos(row), axis=1)\n",
    "y = temp.apply(lambda row: np.sin(row), axis=1)\n",
    "# Average the Cartesian coordinates\n",
    "mean_x = x.mean(axis=1)\n",
    "mean_y = y.mean(axis=1)\n",
    "# Convert the average Cartesian coordinates back to an angle in radians\n",
    "mean_angle_radians = np.arctan2(mean_y, mean_x)\n",
    "df_penmanshiel_filtered['Blade Angle (pitch position) (°)'] = np.degrees(mean_angle_radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9a3a8-cd0f-4985-8927-664c70d6996b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_penmanshiel_filtered[['Temperature motor axis 1 (°C)','Temperature motor axis 2 (°C)','Temperature motor axis 3 (°C)',\"Motor temperature (°C)\",\n",
    "                        'Motor current axis 1 (A)','Motor current axis 2 (A)','Motor current axis 3 (A)',\"Motor current (A)\",\n",
    "                        'Blade angle (pitch position) A (°)','Blade angle (pitch position) B (°)','Blade angle (pitch position) C (°)','Blade Angle (pitch position) (°)']].hist(figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debc6f2-3f90-4b45-b4eb-c5684fb47536",
   "metadata": {},
   "source": [
    "##### Convert Angular features to its sine and cosine components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490ec5a-2e59-4013-b741-624529b649b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Nacelle position (°)','Vane position 1+2 (°)','Blade Angle (pitch position) (°)']:\n",
    "    df_penmanshiel_filtered[col.split(\"(°)\")[0]+\"cos\"] = np.cos(np.radians(df_penmanshiel_filtered[col]))\n",
    "    df_penmanshiel_filtered[col.split(\"(°)\")[0]+\"sin\"] = np.sin(np.radians(df_penmanshiel_filtered[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbddaa8-f13b-4e15-9385-251b33402a93",
   "metadata": {},
   "source": [
    "##### Convert Wind direction and wind speed to u and v components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f001d79-46fa-4e76-8f90-bd446668a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_angles_lengths_to_u_v(angles, lengths, conversion='trigonometric', kind='deg'):\n",
    "    u = -np.sin(angles * np.pi / 180) * lengths\n",
    "    v = -np.cos(angles * np.pi / 180) * lengths\n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38135f5a-5e00-4a68-a379-739af32fedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_filtered[\"Wind direction u\"],df_penmanshiel_filtered[\"Wind direction v\"] = convert_angles_lengths_to_u_v(df_penmanshiel_filtered['Wind direction (°)'],df_penmanshiel_filtered['Wind speed (m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62550996-cbf6-4d84-9407-fab7817c098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_final=df_penmanshiel_filtered[['Datetime','Turbine',\n",
    "'Long Term Wind (m/s)','Wind direction u','Wind direction v',\n",
    " 'Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin',\n",
    " 'Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)',\n",
    " 'Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)',\n",
    " 'Motor temperature (°C)','Motor current (A)',\n",
    " 'Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)','Power (kW)']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f173f6-ab8a-4ca0-9294-163fa5acc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_final.drop([\"Datetime\",\"Turbine\",'Power (kW)'],axis=1).corr().stack().value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b222a0f-ede7-441e-9ea6-c084d050e90b",
   "metadata": {},
   "source": [
    "Correlation between features is between -0.88 to 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5d6dd-c4fe-4d24-933c-b87ff0778315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_penmanshiel_final.drop(['Turbine','Datetime'],axis=1).hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903febb-8d9c-42d2-81dd-7a13d70daec8",
   "metadata": {},
   "source": [
    "## Atomic asset behaviour extraction\n",
    "- Divide data into weeks\n",
    "- Calculate median per week per turbine for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6de56-d4ef-4927-965f-45e3187ea477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_per_week_median = df_penmanshiel_final.groupby([pd.Grouper(key='Datetime', freq='7D'),'Turbine']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb393235-5a3e-45e6-98bd-8ae2c8a57399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_per_week_median[\"Datetime\"]=df_penmanshiel_per_week_median.index.get_level_values(0)\n",
    "df_penmanshiel_per_week_median[\"week\"]=df_penmanshiel_per_week_median.index.get_level_values(0)\n",
    "df_penmanshiel_per_week_median[\"week\"]=pd.factorize(df_penmanshiel_per_week_median['week'])[0]\n",
    "df_penmanshiel_per_week_median[\"week\"]=df_penmanshiel_per_week_median[\"week\"]+1\n",
    "\n",
    "df_penmanshiel_per_week_median[\"Turbine\"]=df_penmanshiel_per_week_median.index.get_level_values(1)\n",
    "\n",
    "df_penmanshiel_per_week_median[\"week-turbine\"]=\"W\"+df_penmanshiel_per_week_median[\"week\"].astype(str)+\":\"+df_penmanshiel_per_week_median[\"Turbine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c004-4fdc-4e08-a2a5-39fa51897081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_per_week_median.set_index([\"week-turbine\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095735d9-a8bd-40a8-81eb-0067b9623fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_penmanshiel_per_week_median.drop(['week', 'Turbine','Datetime'],axis=1).hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686c13b-dc89-4e63-a208-0b91585eb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.line(df_penmanshiel_per_week_median.drop(['Datetime', 'week', 'Turbine','Power (kW)'],axis=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0095a-0e48-4ecd-840f-1a4deec7c758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436c0423-c576-4a6f-be4d-d5b6bef60d3f",
   "metadata": {},
   "source": [
    "## Real time elementary mode detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eff806-00b1-4f9f-9a79-ee8e8569b3db",
   "metadata": {},
   "source": [
    "Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c68de4-7c23-4b1f-9fb8-af7c86166248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame()\n",
    "df_train[\"Elementary mode\"]=None\n",
    "df_train[\"Composite mode\"]=None\n",
    "for i in range(1,54,1):\n",
    "    for turb in df_week_turbine_plot_id.columns:\n",
    "        ind = \"W\"+str(i)+\":\"+turb\n",
    "        df_train.loc[ind,\"Elementary mode\"]=str(df_week_turbine_plot.loc[i][turb])\n",
    "        df_train.loc[ind,\"Composite mode\"]=df_week_turbine_plot_id.loc[str(i)][turb]\n",
    "\n",
    "\n",
    "df_test=pd.DataFrame()\n",
    "df_test[\"Elementary mode\"]=None\n",
    "df_test[\"Composite mode\"]=None\n",
    "for i in range(54,len(df_week_turbine_plot_id)+1,1):\n",
    "    for turb in df_week_turbine_plot_id.columns:\n",
    "        ind = \"W\"+str(i)+\":\"+turb\n",
    "        df_test.loc[ind,\"Elementary mode\"]=str(df_week_turbine_plot.loc[i][turb])\n",
    "        df_test.loc[ind,\"Composite mode\"]=df_week_turbine_plot_id.loc[str(i)][turb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a4f98-5089-4479-aa62-3052e23f62b0",
   "metadata": {},
   "source": [
    "Each elementary mode can be defined with features that it is composed of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b060a-bc08-4946-a536-13e926144fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_feature_dict={}\n",
    "layer_feature_dict[\"Layer 1\"] = ['Wind direction u','Wind direction v']\n",
    "layer_feature_dict[\"Layer 2\"] = ['Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin']\n",
    "layer_feature_dict[\"Layer 3\"] = ['Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)']\n",
    "layer_feature_dict[\"Layer 4\"] = ['Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)']\n",
    "layer_feature_dict[\"Layer 5\"] = ['Motor temperature (°C)','Motor current (A)']\n",
    "layer_feature_dict[\"Layer 6\"] = ['Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecdf47-6c9c-42bf-a0b1-f0a37722a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_cluster_list=[]\n",
    "for ind,key in enumerate([Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]):\n",
    "    for _ in range(key[\"cluster\"].nunique()):\n",
    "        layer_cluster_list.append(\"Layer \"+str(ind+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a1a1f-d2b7-4000-ab26-1d4b039cf79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clusters_layers={}\n",
    "print(dict_clusters)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "feat_set=[]\n",
    "for key in dict_clusters.keys():\n",
    "    print(\"E\"+str(key))\n",
    "    print([layer_cluster_list[i] for i in dict_clusters[key]])\n",
    "    dict_clusters_layers[key]=[layer_cluster_list[i] for i in dict_clusters[key]]\n",
    "\n",
    "    feat=[]\n",
    "    for lyr in list(set(dict_clusters_layers[key])):\n",
    "        feat.append(layer_feature_dict[lyr])\n",
    "    feat=[item for sublist in feat for item in sublist]\n",
    "    feat=list(set(feat))\n",
    "    feat_set.append(feat)\n",
    "    print(feat)\n",
    "    print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079599c-cf4d-43c2-b16d-262f58c55154",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Centroid based\n",
    "Centroid of each elementary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6293f2-a262-4127-8d79-e12cdf7e7115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "fig=go.Figure()\n",
    "elementary_mode_centroid={}\n",
    "elementary_mode_centroid[\"centroid\"]={}\n",
    "elementary_mode_centroid[\"dist\"]={}\n",
    "for elem in [0,1,2,3,4]:\n",
    "    ind=list(df_train[df_train[\"Elementary mode\"].str.contains(str(elem))].index)\n",
    "\n",
    "    scaler=MinMaxScaler()\n",
    "    all_data=scaler.fit_transform(df_penmanshiel_per_week_median.loc[ind][feat_set[elem]].values)\n",
    "    elementary_mode_centroid[\"centroid\"][elem]=np.mean(all_data,axis=0)\n",
    "    print(\"Number of samples in training set: \",len(all_data))\n",
    "\n",
    "    if (len(ind)>1):\n",
    "        fig1=go.Figure()\n",
    "        pca = PCA(n_components=2).fit_transform(all_data)\n",
    "        fig1=px.scatter(x=pca[:, 0],y=pca[:, 1],title=\"E\"+str(elem)+\"-PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(all_data).explained_variance_ratio_),2)) +\")\")\n",
    "        fig1.show()\n",
    "    \n",
    "    dist = cdist(all_data,elementary_mode_centroid[\"centroid\"][elem].reshape(1,-1),metric=\"euclidean\")\n",
    "    elementary_mode_centroid[\"dist\"][elem]=np.max(dist)\n",
    "\n",
    "    fig.add_traces(go.Histogram(x=dist.flatten(),name=\"E\"+str(elem)))\n",
    "    fig.update_xaxes(title=\"Distance\")\n",
    "    fig.update_yaxes(title=\"Number of instances\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c4c9c-5f43-4026-8aca-2f3c39f72c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_mode_centroid[\"dist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd242d-1e18-4190-8775-07cecd73885a",
   "metadata": {},
   "source": [
    "New datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058c26d-9ed9-4046-9a7a-44c34f56e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"pred_elem\"]=None\n",
    "df_test[\"dist\"]=None\n",
    "df_test[\"thres\"]=None\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "    pred_elem=[]\n",
    "    dist_lst=[]\n",
    "    thresh_dist=[]\n",
    "    for elem in [0,1,2,3,4]:\n",
    "        test_vector=df_penmanshiel_per_week_median.loc[i][feat_set[elem]].values\n",
    "        \n",
    "        scaler=MinMaxScaler()\n",
    "        ind=list(df_train[df_train[\"Elementary mode\"].str.contains(str(elem))].index)\n",
    "        scaler.fit(df_penmanshiel_per_week_median.loc[ind][feat_set[elem]].values)\n",
    "        X_test=scaler.transform(test_vector.reshape(1,-1))\n",
    "\n",
    "        dist=cdist(elementary_mode_centroid[\"centroid\"][elem].reshape(1,-1),X_test)\n",
    "        if (dist<=elementary_mode_centroid[\"dist\"][elem]):\n",
    "            pred_elem.append(elem)\n",
    "            dist_lst.append(dist[0][0])\n",
    "            thresh_dist.append(elementary_mode_centroid[\"dist\"][elem])\n",
    "    df_test.loc[i,\"pred_elem\"]=str(pred_elem)\n",
    "    df_test.loc[i,\"dist\"]=str(dist_lst)\n",
    "    df_test.loc[i,\"thres\"]=str(thresh_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81581275-db9a-40fd-9311-31a8cf901ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a4db0-bdca-469b-951e-a7c49ef2d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "confusion_matrix(df_test[\"Elementary mode\"], df_test[\"pred_elem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf318b70-4f1f-440b-9c69-13fb45e6766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(df_test[\"Elementary mode\"], df_test[\"pred_elem\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827d0d9-57cf-4e0d-98f1-f0c70574ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Elementary mode\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97689f0d-247b-4a5e-8590-cae79c2dccbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458795e-55ee-45f9-b8bb-66898af8a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_mode_bb={}\n",
    "elementary_mode_bb[\"feats\"]={}\n",
    "elementary_mode_bb[\"min\"]={}\n",
    "elementary_mode_bb[\"max\"]={}\n",
    "\n",
    "fig=go.Figure()\n",
    "for elem in [0,1,2,3,4]:\n",
    "    ind=list(df_train[df_train[\"Elementary mode\"].str.contains(str(elem))].index)\n",
    "    \n",
    "    all_data=df_penmanshiel_per_week_median.loc[ind][feat_set[elem]]\n",
    "    elementary_mode_bb[\"feats\"][elem]=feat_set[elem]\n",
    "    elementary_mode_bb[\"min\"][elem]=all_data.min().values\n",
    "    elementary_mode_bb[\"max\"][elem]=all_data.max().values\n",
    "    fig.add_traces(go.Scatter(x=feat_set[elem],y=all_data.min().values,name=\"E\"+str(elem)+\"-min\"))\n",
    "    fig.add_traces(go.Scatter(x=feat_set[elem],y=all_data.max().values,name=\"E\"+str(elem)+\"-max\"))\n",
    "\n",
    "    test_vector=df_penmanshiel_per_week_median.loc[\"W55:T10\"][feat_set[4]].values\n",
    "    fig.add_traces(go.Scatter(x=feat_set[4],y=test_vector,name=\"Test vector\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1f8b1-4abf-4910-83b9-b45a205fd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a1a94-5f1c-4cb4-981a-47c18e46af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penmanshiel_per_week_median.loc[i][feat_set[1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fdd1b-177a-4418-8da7-600f8056e449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elementary_mode_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c8094-ea46-4ccd-adf6-b6fd09c69a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"pred_bb\"]=None\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "    pred_elem=[]\n",
    "    dist_lst=[]\n",
    "    thresh_dist=[]\n",
    "    for elem in [0,1,2,3,4]:\n",
    "        test_vector=df_penmanshiel_per_week_median.loc[i][feat_set[elem]].values\n",
    "        min_ = elementary_mode_bb[\"min\"][elem]\n",
    "        max_ = elementary_mode_bb[\"max\"][elem]\n",
    "        if (np.all((test_vector >= min_) & (test_vector <= max_))):\n",
    "            pred_elem.append(elem)\n",
    "    df_test.loc[i,\"pred_bb\"]=str(pred_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55030e-4817-4363-be04-5da257da7cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test[[\"Elementary mode\",\"pred_bb\"]].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7124f3a-c622-4c29-9ab2-57a0a01810a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "confusion_matrix(df_test[\"Elementary mode\"], df_test[\"pred_bb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e2e52-450f-461f-a301-2098e51ce7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(df_test[\"Elementary mode\"], df_test[\"pred_bb\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226e854-ca2f-4349-bce7-ad824916ee02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Layer clusters - hyperclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cc8e3-e469-41f3-918d-6920f8d9905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layercls_hypcls=pd.DataFrame(columns=layer_list_temp,index=['E' + str(i) for i in dict_clusters.keys()])\n",
    "\n",
    "for key in dict_clusters.keys():\n",
    "    print(\"E\"+str(key))\n",
    "    print([layer_list_temp[i] for i in dict_clusters[key]])\n",
    "    df_layercls_hypcls.loc[\"E\"+str(key),[layer_list_temp[i] for i in dict_clusters[key]]]=1\n",
    "df_layercls_hypcls=df_layercls_hypcls.notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c850e8-7022-46b3-bfe9-3942961b6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.imshow(df_layercls_hypcls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7adb5-044b-4d30-b1bd-10debb6ed2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [1]:\n",
    "    print(\"Cluster: \",key)\n",
    "    print(str(len(list(clus_sol[key]))) +\" turbine-week pairs\")\n",
    "    temp=df_penmanshiel_per_week_median.loc[list(clus_sol[key])]\n",
    "    print(\"Number of turbines: \",temp[\"Turbine\"].nunique())\n",
    "    print(\"Number of unique weeks: \",temp[\"week\"].nunique())\n",
    "          \n",
    "    fig=px.bar(temp[\"Turbine\"].values,color=temp[\"week\"].astype(str).values)\n",
    "    fig.update_xaxes(title=\"Turbine\")\n",
    "    fig.update_layout(legend_title_text=\"Week\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed497d82-1a42-4ec1-86dd-384f45710d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=df_test[[\"Elementary mode\",\"Composite mode\"]].copy()\n",
    "df_layer_hyper=pd.concat([df_train,temp])\n",
    "df_layer_hyper.loc[Layer1[\"week-turbine\"],\"Layer1\"]=Layer1[\"cluster\"].values\n",
    "df_layer_hyper.loc[Layer2[\"week-turbine\"],\"Layer2\"]=Layer2[\"cluster\"].values\n",
    "df_layer_hyper.loc[Layer3[\"week-turbine\"],\"Layer3\"]=Layer3[\"cluster\"].values\n",
    "df_layer_hyper.loc[Layer4[\"week-turbine\"],\"Layer4\"]=Layer4[\"cluster\"].values\n",
    "df_layer_hyper.loc[Layer5[\"week-turbine\"],\"Layer5\"]=Layer5[\"cluster\"].values\n",
    "df_layer_hyper.loc[Layer6[\"week-turbine\"],\"Layer6\"]=Layer6[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd927770-828e-4292-8096-2d4341c0fba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_layer_hyper.loc[\"W1:T10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe5a58-fe59-4b18-9434-c8a89d845a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7324a05-6474-4d81-8820-01036e8deb3d",
   "metadata": {},
   "source": [
    "## Hyperclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed77b7-35e6-4e9f-b2b5-07470426bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_clusters(final_clusters1, hypergraph1, method):\n",
    "    # mapping hyperedges to data objects to obtain the clustering solution of data objects\n",
    "    temp_del = 0\n",
    "    clustering_nodes = {}\n",
    "    for key, val in final_clusters1.items():\n",
    "        if method == 'donot_inc_key_in_cluster':\n",
    "            pins_center = []\n",
    "        elif method == \"inc_key_in_cluster\":\n",
    "            pins_center = list(hypergraph1.pins(key))\n",
    "        \n",
    "        for _ in val:\n",
    "            pins_center.extend(list(hypergraph1.pins(_)))\n",
    "        clustering_nodes[key] = set(pins_center)\n",
    "        temp_del = temp_del + len(set(pins_center))\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering of data objects\", clustering_nodes) # dict, key = center(hyperedge), values = data objects\n",
    "    \n",
    "    # replacing the index of the data object with its short id\n",
    "    clus_nodes_short_id = {}\n",
    "    for key, val in clustering_nodes.items():\n",
    "        # print(val)\n",
    "        clus_nodes_short_id[key] = {nodes_hyper[x] for x in val} # note that sets are not ordered\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering solution, key = center (hyperedge), val = set of short_ids\")\n",
    "        print(clus_nodes_short_id)\n",
    "    return clus_nodes_short_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbeed6-8b71-4dac-83f2-1bb7d8cb1473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year =\"2019\"\n",
    "df_oneyear=df_penmanshiel_per_week_median[df_penmanshiel_per_week_median[\"Datetime\"].astype(str).str.contains(year)]\n",
    "\n",
    "# creating layers\n",
    "Layer1 = df_oneyear.reset_index()[['week-turbine','Wind direction u','Wind direction v']].dropna().copy()\n",
    "Layer2 = df_oneyear.reset_index()[['week-turbine','Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin']].dropna().copy()\n",
    "Layer3 = df_oneyear.reset_index()[['week-turbine','Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)']].dropna().copy()\n",
    "Layer4 = df_oneyear.reset_index()[['week-turbine','Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)']].dropna().copy()\n",
    "Layer5 = df_oneyear.reset_index()[['week-turbine','Motor temperature (°C)','Motor current (A)']].dropna().copy()\n",
    "Layer6 = df_oneyear.reset_index()[['week-turbine','Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)']].dropna().copy()\n",
    "\n",
    "layer_data=[Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]\n",
    "layer_dict={}\n",
    "\n",
    "for l_id in range(len(layer_data)): # Specify number of layers\n",
    "    layer_dict['Layer '+str(l_id+1)] = {}\n",
    "    layer_dict['Layer '+str(l_id+1)][\"Layer_data\"] = layer_data[l_id]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    layer_data_transformed = scaler.fit_transform(layer_data[l_id].drop([\"week-turbine\"],axis=1).to_numpy())\n",
    "    layer_dict['Layer '+str(l_id+1)][\"Data_transformed\"] = layer_data_transformed\n",
    "\n",
    "db = DBSCAN(eps=0.1).fit(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "pca_layer1 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 1\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "fig.show()\n",
    "Layer1[\"cluster\"]=layer_dict[\"Layer 1\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer1=Layer1.copy()\n",
    "\n",
    "db = DBSCAN(eps=0.15).fit(layer_dict[\"Layer 2\"][\"Data_transformed\"])\n",
    "pca_layer2= PCA(n_components=6).fit_transform(layer_dict[\"Layer 2\"][\"Data_transformed\"])\n",
    "fig=px.scatter(x=pca_layer2[:, 0],y=pca_layer2[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 2\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "fig.show()\n",
    "Layer2[\"cluster\"]=layer_dict[\"Layer 2\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer2=Layer2.copy()\n",
    "\n",
    "db = DBSCAN(eps=0.12).fit(layer_dict[\"Layer 3\"][\"Data_transformed\"])\n",
    "pca_layer3= PCA(n_components=2).fit_transform(layer_dict[\"Layer 3\"][\"Data_transformed\"])\n",
    "fig=px.scatter(x=pca_layer3[:, 0],y=pca_layer3[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 3\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "fig.show()\n",
    "Layer3[\"cluster\"]=layer_dict[\"Layer 3\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer3=Layer3.copy()\n",
    "\n",
    "db = DBSCAN(eps=0.1).fit(layer_dict[\"Layer 4\"][\"Data_transformed\"])\n",
    "pca_layer4= PCA(n_components=2).fit_transform(layer_dict[\"Layer 4\"][\"Data_transformed\"])\n",
    "fig=px.scatter(x=pca_layer4[:, 0],y=pca_layer4[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 4\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "fig.show()\n",
    "Layer4[\"cluster\"]=layer_dict[\"Layer 4\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer4=Layer4.copy()\n",
    "\n",
    "db = DBSCAN(eps=0.04).fit(layer_dict[\"Layer 5\"][\"Data_transformed\"])\n",
    "pca_layer5 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 5\"][\"Data_transformed\"])\n",
    "fig=px.scatter(x=pca_layer5[:, 0],y=pca_layer5[:, 1],color=db.labels_.astype(str),title=\"PCA(1.0)\")\n",
    "fig.show()\n",
    "Layer5[\"cluster\"]=layer_dict[\"Layer 5\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer5=Layer5.copy()\n",
    "\n",
    "db = DBSCAN(eps=0.045).fit(layer_dict[\"Layer 6\"][\"Data_transformed\"])\n",
    "pca_layer6 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 6\"][\"Data_transformed\"])\n",
    "# fig=px.scatter(x=pca_layer6[:, 0],y=pca_layer6[:, 1],color=db.labels_.astype(str),title=\"PCA (1.0)\")\n",
    "# fig.show()\n",
    "Layer6[\"cluster\"]=layer_dict[\"Layer 6\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "dfincdb_layer6=Layer6.copy()\n",
    "\n",
    "# Drop outliers\n",
    "layer_data=[Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]\n",
    "for l_id, key in enumerate(layer_dict.keys()):\n",
    "    temp=layer_dict[key][\"Layer_data\"]\n",
    "    layer_dict[key][\"Layer_data\"]=temp[temp[\"cluster\"]!=-1]\n",
    "    layer_data[l_id]=temp[temp[\"cluster\"]!=-1]\n",
    "\n",
    "Layer1=Layer1[Layer1[\"cluster\"]!=-1]\n",
    "Layer2=Layer2[Layer2[\"cluster\"]!=-1]\n",
    "Layer3=Layer3[Layer3[\"cluster\"]!=-1]\n",
    "Layer4=Layer4[Layer4[\"cluster\"]!=-1]\n",
    "Layer5=Layer5[Layer5[\"cluster\"]!=-1]\n",
    "Layer6=Layer6[Layer6[\"cluster\"]!=-1]\n",
    "for key in layer_dict.keys():\n",
    "    print(key+\"--->\"+str(len(np.unique(layer_dict[key][\"Layer_data\"][\"cluster\"])))+\" clusters\")\n",
    "\n",
    "# Hypergraph\n",
    "# creating a nested list, where each inner list lists the ids in that cluster. \n",
    "lst = [v for v in Layer1.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "lst = lst + [v for v in Layer2.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "lst = lst + [v for v in Layer3.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "lst = lst + [v for v in Layer4.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "lst = lst + [v for v in Layer5.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "lst = lst + [v for v in Layer6.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "hyperedge_indices = []\n",
    "cnt = 0\n",
    "hyperedge_indices.append(cnt)\n",
    "for each in lst:\n",
    "    cnt = cnt+len(each)\n",
    "    hyperedge_indices.append(cnt)\n",
    "nodes_hyper = df_oneyear.index.tolist()\n",
    "hyperedges_1 = [item for sublist in lst for item in sublist]\n",
    "hyperedges = [nodes_hyper.index(i) for i in hyperedges_1]\n",
    "num_nodes = len(nodes_hyper)\n",
    "num_nets = len(hyperedge_indices)-1\n",
    "\n",
    "k = 2\n",
    "hypergraph = kahypar.Hypergraph(num_nodes, num_nets, hyperedge_indices, hyperedges, k)\n",
    "context = kahypar.Context()\n",
    "context.loadINIconfiguration(\"cut_kKaHyPar_sea20.ini\")\n",
    "\n",
    "node_incident_edges = [] # list containg the incident edges of each node in sublist.\n",
    "for each_node in hypergraph.nodes():\n",
    "    ie = []\n",
    "    for incident_edge in hypergraph.incidentEdges(each_node):\n",
    "        ie.append(incident_edge)\n",
    "    node_incident_edges.append(ie)\n",
    "cluster_list = [sublist for sublist in lst if len(sublist) > 1]\n",
    "\n",
    "#Obtaining the neighbourhood of each edge.\n",
    "total_edges = hypergraph.numEdges()\n",
    "\n",
    "neighbourhood = [] # neighbourhood of each edge is presented in order.\n",
    "number_neighbours = []\n",
    "for i in range(total_edges):\n",
    "    temp = []\n",
    "    for node, edges in enumerate(node_incident_edges):\n",
    "        if i in edges:\n",
    "            for e in edges:\n",
    "                temp.append(e)\n",
    "    neighbourhood.append(set(temp))\n",
    "    number_neighbours.append(len(set(temp)))\n",
    "layer_list_temp=[]\n",
    "for ind,key in enumerate([Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]):\n",
    "    for cls in range(key[\"cluster\"].nunique()):\n",
    "        layer_list_temp.append(\"Layer\"+str(ind+1)+\"- cluster\"+str(cls+1))\n",
    "\n",
    "# calculating Nearest Neighbourhood Similarity\n",
    "NNS = {}\n",
    "for i in range(total_edges):\n",
    "    for j in range(i, total_edges):\n",
    "        if i != j:\n",
    "            # print(\"checking intersection of \", i, \"and\", j, \"i.e.,\", neighbourhood[i], \"and\", neighbourhood[j])\n",
    "            intersection = neighbourhood[i].intersection(neighbourhood[j])\n",
    "            if (i not in intersection) or (j not in intersection): # or condition is not required. if i is in intersection then automatically j will be in the intersection. \n",
    "                # print(i, j, intersection, neighbourhood[i], neighbourhood[j])\n",
    "                NNS[(i,j)] = 0\n",
    "            else:\n",
    "                union = neighbourhood[i].union(neighbourhood[j])\n",
    "                NNS[(i,j)] = len(intersection)/len(union)\n",
    "cnt=0\n",
    "df_snns=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "for i in np.arange(0,total_edges,1):\n",
    "    for j in np.arange(0,total_edges,1):\n",
    "        if (i,j) in NNS:\n",
    "            cnt=cnt+1\n",
    "            df_snns.loc[i,j]=NNS[i,j]\n",
    "# converting the similarity matrix into distance matrix.\n",
    "NNS_dist = {k: 1-v for k, v in NNS.items()}\n",
    "\n",
    "# obtaining the distance matrix\n",
    "array_dist = []\n",
    "for i in range(total_edges):\n",
    "    temp = []\n",
    "    for j in range(total_edges):\n",
    "        if i != j:\n",
    "            temp.append(NNS_dist[(min(i, j), max(i,j))])\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    array_dist.append(temp)\n",
    "\n",
    "df_dist=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "for i in np.arange(0,total_edges,1):\n",
    "    for j in np.arange(0,total_edges,1):\n",
    "        # if (i<=j):\n",
    "        df_dist.loc[i,j]=np.round(array_dist[i][j],2)\n",
    "df_sim=1-df_dist\n",
    "\n",
    "# kmeans=KMeans(n_clusters=5).fit_predict(np.array(array_dist))\n",
    "kmeans = KMedoids(n_clusters=5, metric='precomputed', method='pam', init='k-medoids++', random_state=0).fit_predict(np.array(array_dist))\n",
    "dict_clusters = {0: [], 1:[], 2:[],3:[],4:[]}\n",
    "for index, each in enumerate(kmeans):\n",
    "    dict_clusters[each].append(index)\n",
    "\n",
    "Debug = False\n",
    "clus_sol = generate_final_clusters(dict_clusters, hypergraph, 'donot_inc_key_in_cluster')\n",
    "# print(clus_sol) # clus_sol is the final clustering solution based on k-medoids based method. key: cluster number, value: data objects in cluster\n",
    "cluster_objects_list=[list(value) for value in clus_sol.values()]\n",
    "for key in clus_sol.keys():\n",
    "    print(str(key) +\" : \"+str(len(list(clus_sol[key]))))\n",
    "\n",
    "fig=go.Figure()\n",
    "for key in clus_sol.keys():\n",
    "    temp=df_oneyear.reset_index().copy()\n",
    "    temp=temp[temp[\"week-turbine\"].isin(list(clus_sol[key]))].copy()\n",
    "\n",
    "    fig.add_trace(go.Box(y=temp['Power (kW)'], name='E'+str(key)))\n",
    "\n",
    "fig.update_layout(title=\"Active power\")\n",
    "# fig.update_xaxes(title=\"Cluster\")\n",
    "fig.update_yaxes(title=\"Power (kW)\")\n",
    "fig.show()\n",
    "\n",
    "df_fca=pd.DataFrame(index=df_oneyear.index.astype(str),columns=clus_sol.keys())\n",
    "for ind,row in df_fca.iterrows():\n",
    "    for clus in row.index:\n",
    "        if (ind in list(clus_sol[clus])):\n",
    "            df_fca.loc[ind,clus]=True\n",
    "        else:\n",
    "            df_fca.loc[ind,clus]=False\n",
    "df_fca.columns=df_fca.columns.astype(str)\n",
    "df_fca.columns=\"E\"+df_fca.columns\n",
    "\n",
    "from fcapy.context import FormalContext\n",
    "K = FormalContext.from_pandas(pd.DataFrame(df_fca))\n",
    "\n",
    "from fcapy.lattice import ConceptLattice\n",
    "L = ConceptLattice.from_context(K)\n",
    "\n",
    "from fcapy.visualizer import LineVizNx\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "vsl = LineVizNx()\n",
    "vsl.draw_concept_lattice(L, ax=ax, flg_node_indices=True,flg_new_intent_count_prefix=False)\n",
    "ax.set_title('week-turbine concept lattice')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "week_cluster={}\n",
    "for w_id in np.arange(1,df_oneyear[\"week\"].nunique()+1,1):\n",
    "    clst_lst=[]\n",
    "    for key in clus_sol.keys():\n",
    "        lst=[element for element in list(clus_sol[key]) if \"W\"+str(w_id)+\":\" in element]\n",
    "        if (len(lst)!=0):\n",
    "            clst_lst.append(key)\n",
    "    week_cluster[w_id]=clst_lst\n",
    "\n",
    "\n",
    "turbine_cluster={}\n",
    "for turb in df_oneyear[\"Turbine\"].unique():\n",
    "    clst_lst=[]\n",
    "    for key in clus_sol.keys():\n",
    "        lst=[element for element in list(clus_sol[key]) if turb in element]\n",
    "        if (len(lst)!=0):\n",
    "            clst_lst.append(key)\n",
    "    turbine_cluster[turb]=clst_lst\n",
    "\n",
    "df_week_cluster=pd.DataFrame(str(0),index=list(week_cluster.keys()),columns=np.arange(0,len(clus_sol.keys()),1))\n",
    "for ind,row in df_week_cluster.iterrows():\n",
    "    df_week_cluster.loc[ind,week_cluster[ind]]=str(1)\n",
    "df_week_cluster.index=df_week_cluster.index.astype(str)\n",
    "df_week_cluster.columns=df_week_cluster.columns.astype(str)\n",
    "\n",
    "\n",
    "df_turbine_cluster=pd.DataFrame(str(0),index=list(turbine_cluster.keys()),columns=np.arange(0,len(clus_sol.keys()),1))\n",
    "for ind,row in df_turbine_cluster.iterrows():\n",
    "    df_turbine_cluster.loc[ind,turbine_cluster[ind]]=str(1)\n",
    "df_turbine_cluster.index=df_turbine_cluster.index.astype(str)\n",
    "df_turbine_cluster.columns=df_turbine_cluster.columns.astype(str)\n",
    "\n",
    "df_week_turbine_plot=pd.DataFrame(index=df_week_cluster.index.astype(int),columns=df_turbine_cluster.index)\n",
    "unique_combination=[]\n",
    "for w_id in np.arange(1,df_oneyear[\"week\"].nunique()+1,1):\n",
    "    for t_id in df_turbine_cluster.index:\n",
    "        week_turb= \"W\"+str(w_id)+\":\"+t_id \n",
    "        comb = [key for key, values in clus_sol.items() if week_turb in values]\n",
    "        df_week_turbine_plot.loc[w_id,t_id]=comb\n",
    "        if comb not in unique_combination:\n",
    "            unique_combination.append(comb)\n",
    "unique_dict=dict(zip(np.arange(100,100+len(unique_combination)+1,1), unique_combination))\n",
    "unique_dict={tuple(value): key for key, value in unique_dict.items()}\n",
    "unique_dict.keys()\n",
    "\n",
    "df_week_turbine_plot_id=df_week_turbine_plot.copy()\n",
    "for ind,row in df_week_turbine_plot_id.iterrows():\n",
    "    for col in df_week_turbine_plot_id.columns:\n",
    "        df_week_turbine_plot_id.loc[ind,col]=unique_dict[tuple(df_week_turbine_plot_id.loc[ind,col])]\n",
    "        \n",
    "df_week_turbine_plot_id.index=df_week_turbine_plot_id.index.astype(str)\n",
    "df_week_turbine_plot_id=df_week_turbine_plot_id.astype(int).astype(str)\n",
    "fig = px.imshow(df_week_turbine_plot_id.T,labels=dict(x=\"Week\", y=\"Turbine\"))\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "print(unique_dict)\n",
    "\n",
    "if (year==\"2019\"):\n",
    "    df_train=pd.DataFrame()\n",
    "    df_train[\"Elementary mode\"]=None\n",
    "    for i in range(1,54,1):\n",
    "        for turb in df_week_turbine_plot.columns:\n",
    "            ind = \"W\"+str(i)+\":\"+turb\n",
    "            df_train.loc[ind,\"Elementary mode\"]=str(df_week_turbine_plot.loc[i][turb])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d16e3-9b70-40e8-9e1b-1549b797dacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_feature_dict={}\n",
    "layer_feature_dict[\"Layer 1\"] = ['Wind direction u','Wind direction v']\n",
    "layer_feature_dict[\"Layer 2\"] = ['Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin']\n",
    "layer_feature_dict[\"Layer 3\"] = ['Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)']\n",
    "layer_feature_dict[\"Layer 4\"] = ['Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)']\n",
    "layer_feature_dict[\"Layer 5\"] = ['Motor temperature (°C)','Motor current (A)']\n",
    "layer_feature_dict[\"Layer 6\"] = ['Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)']\n",
    "\n",
    "layer_cluster_list=[]\n",
    "for ind,key in enumerate([Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]):\n",
    "    for _ in range(key[\"cluster\"].nunique()):\n",
    "        layer_cluster_list.append(\"Layer \"+str(ind+1))\n",
    "\n",
    "dict_clusters_layers={}\n",
    "print(dict_clusters)\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "feat_set=[]\n",
    "for key in dict_clusters.keys():\n",
    "    print(\"E\"+str(key))\n",
    "    print([layer_cluster_list[i] for i in dict_clusters[key]])\n",
    "    dict_clusters_layers[key]=[layer_cluster_list[i] for i in dict_clusters[key]]\n",
    "\n",
    "    feat=[]\n",
    "    for lyr in list(set(dict_clusters_layers[key])):\n",
    "        feat.append(layer_feature_dict[lyr])\n",
    "    feat=[item for sublist in feat for item in sublist]\n",
    "    feat=list(set(feat))\n",
    "    feat_set.append(feat)\n",
    "    print(feat)\n",
    "    print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b56e1-0af7-4530-acdd-ae34ee2f402a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Boundingbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6950e02-bdf1-4324-9fb3-6c991f24fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame()\n",
    "df_test[\"Elementary mode\"]=None\n",
    "for i in range(54,(54*2)-2,1):\n",
    "    for turb in df_week_turbine_plot_id.columns:\n",
    "        ind = \"W\"+str(i)+\":\"+turb\n",
    "        df_test.loc[ind,\"Elementary mode\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2bdda-7605-4e90-926d-677e7e228b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_mode_bb={}\n",
    "elementary_mode_bb[\"feats\"]={}\n",
    "elementary_mode_bb[\"min\"]={}\n",
    "elementary_mode_bb[\"max\"]={}\n",
    "\n",
    "fig=go.Figure()\n",
    "for elem in [0,1,2,3,4]:\n",
    "    ind=list(df_train[df_train[\"Elementary mode\"].str.contains(str(elem))].index)\n",
    "    \n",
    "    all_data=df_penmanshiel_per_week_median.loc[ind][feat_set[elem]]\n",
    "    elementary_mode_bb[\"feats\"][elem]=feat_set[elem]\n",
    "    elementary_mode_bb[\"min\"][elem]=all_data.min().values\n",
    "    elementary_mode_bb[\"max\"][elem]=all_data.max().values\n",
    "    fig.add_traces(go.Scatter(x=feat_set[elem],y=all_data.min().values,name=\"E\"+str(elem)+\"-min\"))\n",
    "    fig.add_traces(go.Scatter(x=feat_set[elem],y=all_data.max().values,name=\"E\"+str(elem)+\"-max\"))\n",
    "\n",
    "    test_vector=df_penmanshiel_per_week_median.loc[\"W55:T10\"][feat_set[4]].values\n",
    "    fig.add_traces(go.Scatter(x=feat_set[4],y=test_vector,name=\"Test vector\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d029e-ed01-4da6-b86f-9740cfa1b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"pred_bb\"]=None\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "    pred_elem=[]\n",
    "    dist_lst=[]\n",
    "    thresh_dist=[]\n",
    "    for elem in [0,1,2,3,4]:\n",
    "        test_vector=df_penmanshiel_per_week_median.loc[i][feat_set[elem]].values\n",
    "        min_ = elementary_mode_bb[\"min\"][elem]\n",
    "        max_ = elementary_mode_bb[\"max\"][elem]\n",
    "        if (np.all((test_vector >= min_) & (test_vector <= max_))):\n",
    "            pred_elem.append(elem)\n",
    "    df_test.loc[i,\"pred_bb\"]=str(pred_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17a731-18e2-465d-a5b4-64836cfb2035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "df_week_turbine_plot_new=df_week_turbine_plot.copy()\n",
    "for week in np.arange(54,(54*2)-2,1):\n",
    "    df_week_turbine_plot_new.loc[week]=None\n",
    "    for turb in df_week_turbine_plot_new.columns:\n",
    "        df_week_turbine_plot_new.loc[week,turb]= json.loads(df_test.loc[\"W\"+str(week)+\":\"+turb][\"pred_bb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39ed89-f550-4a25-be07-3f323c843983",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combination=[]\n",
    "for week in np.arange(1,(54*2)-2,1):\n",
    "    for turb in df_week_turbine_plot_new.columns:\n",
    "        comb=df_week_turbine_plot_new.loc[week,turb]\n",
    "        if comb not in unique_combination:\n",
    "            unique_combination.append(comb)\n",
    "            \n",
    "unique_dict_=dict(zip(np.arange(100,100+len(unique_combination)+1,1), unique_combination))\n",
    "unique_dict_={tuple(value): key for key, value in unique_dict_.items()}\n",
    "unique_dict_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd59089-062c-44b0-84dd-6d7ac424280b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_week_turbine_plot_id=df_week_turbine_plot_new.copy()\n",
    "for ind,row in df_week_turbine_plot_id.iterrows():\n",
    "    for col in df_week_turbine_plot_id.columns:\n",
    "        df_week_turbine_plot_id.loc[ind,col]=unique_dict_[tuple(df_week_turbine_plot_id.loc[ind,col])]\n",
    "        \n",
    "df_week_turbine_plot_id.index=df_week_turbine_plot_id.index.astype(str)\n",
    "df_week_turbine_plot_id=df_week_turbine_plot_id.astype(int).astype(str)\n",
    "\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"2019-Train\",\"2020-Test\"),\n",
    "    shared_yaxes=True,  # Share y-axis if needed\n",
    "    horizontal_spacing=0.1  # Adjust spacing between plots\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=df_week_turbine_plot_id.iloc[0:53].T.replace({'A': 0}), x=df_week_turbine_plot_id.iloc[0:53].index,\n",
    "    y=df_week_turbine_plot_id.columns,\n",
    "    coloraxis=\"coloraxis\",text=df_week_turbine_plot_id.iloc[0:53].T,texttemplate=\"%{text}\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "print(unique_dict)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=df_week_turbine_plot_id.iloc[53:53*2].T.replace({'A': 0}), x=df_week_turbine_plot_id.iloc[53:53*2].index,\n",
    "    y=df_week_turbine_plot_id.columns,\n",
    "    coloraxis=\"coloraxis\",text=df_week_turbine_plot_id.iloc[53:53*2].T,texttemplate=\"%{text}\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "print(unique_dict_)\n",
    "\n",
    "# Update layout to add shared color axis\n",
    "fig.update_layout(\n",
    "    # coloraxis=dict(colorscale='Viridis'),\n",
    "    coloraxis_colorbar=dict(title=\"Shared Color Bar\"),\n",
    "    height=800,width=1100,\n",
    "    xaxis_title='Week',\n",
    "    yaxis_title='Turbine',\n",
    "    coloraxis_showscale=False\n",
    "\n",
    "    ,xaxis=dict(\n",
    "        titlefont=dict(size=18),\n",
    "        tickfont=dict(size=12)\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        titlefont=dict(size=18),\n",
    "        tickfont=dict(size=12)\n",
    "    )\n",
    "    \n",
    ")\n",
    "# fig.update_traces(\n",
    "#     textfont=dict(color=[[\"black\" if val == \"A\" else \"white\" for val in row] for row in z])\n",
    "# )\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3806b92-c578-4383-8951-12b8f6e20a2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Layer clusters - hyperclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28b907-451f-4e6e-9d6f-e70b83318294",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list_temp=[]\n",
    "for ind,key in enumerate([Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]):\n",
    "    for cls in range(key[\"cluster\"].nunique()):\n",
    "        layer_list_temp.append(\"Layer\"+str(ind+1)+\"- cluster\"+str(cls+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93d647-4406-44d3-9fdd-ede2b457bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layercls_hypcls=pd.DataFrame(columns=layer_list_temp,index=['E' + str(i) for i in dict_clusters.keys()])\n",
    "\n",
    "for key in dict_clusters.keys():\n",
    "    print(\"E\"+str(key))\n",
    "    print([layer_list_temp[i] for i in dict_clusters[key]])\n",
    "    df_layercls_hypcls.loc[\"E\"+str(key),[layer_list_temp[i] for i in dict_clusters[key]]]=1\n",
    "df_layercls_hypcls=df_layercls_hypcls.notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888dfc0-c03d-441e-9e6a-70fce6cbb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.imshow(df_layercls_hypcls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248e27e-c038-40fd-a74c-1b0f5109b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layer_hyper=df_train.copy()\n",
    "df_layer_hyper.loc[Layer1[\"week-turbine\"],\"Layer1\"]=Layer1[\"cluster\"].values+1\n",
    "df_layer_hyper.loc[Layer2[\"week-turbine\"],\"Layer2\"]=Layer2[\"cluster\"].values+1\n",
    "df_layer_hyper.loc[Layer3[\"week-turbine\"],\"Layer3\"]=Layer3[\"cluster\"].values+1\n",
    "df_layer_hyper.loc[Layer4[\"week-turbine\"],\"Layer4\"]=Layer4[\"cluster\"].values+1\n",
    "df_layer_hyper.loc[Layer5[\"week-turbine\"],\"Layer5\"]=Layer5[\"cluster\"].values+1\n",
    "df_layer_hyper.loc[Layer6[\"week-turbine\"],\"Layer6\"]=Layer6[\"cluster\"].values+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4052f-45cf-4179-bec5-4cfa7cef5f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_layer_hyper[df_layer_hyper.isna().any(axis=1)].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fea1a-aee0-4069-9fa0-e47b55313bfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Incremental DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b08422-01f6-40fb-837d-1a1877c6bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install incdbscan\n",
    "from incdbscan import IncrementalDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8caa8-c9af-4160-933c-3366b1c62bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DBSCAN = INCDBSCAN\n",
    "incdb = IncrementalDBSCAN(eps=0.1, min_pts=5)\n",
    "incdb.insert(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "dfincdb_layer1_temp=dfincdb_layer1.copy()\n",
    "dfincdb_layer1_temp[\"incdb_cluster\"]=incdb.get_cluster_labels(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"cluster\"].values.astype(str))\n",
    "fig.show()\n",
    "\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"incdb_cluster\"].values.astype(str))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacae9d-0538-41e7-9e68-39e1aee53f20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from incdbscan import IncrementalDBSCAN\n",
    "incdb = IncrementalDBSCAN(eps=0.1, min_pts=5)\n",
    "dfincdb_layer1_temp=dfincdb_layer1.copy()\n",
    "temp=layer_dict[\"Layer 1\"][\"Data_transformed\"].copy()\n",
    "\n",
    "incdb.insert(temp[0:400]) # 1st batch\n",
    "dfincdb_layer1_temp.loc[0:400-1,\"incdb_cluster\"]=incdb.get_cluster_labels(temp[0:400])\n",
    "\n",
    "incdb.insert(temp[400::]) # 2nd batch\n",
    "dfincdb_layer1_temp.loc[400::,\"incdb_cluster\"]=incdb.get_cluster_labels(temp[400::]) # get labels for 2nd batch\n",
    "# dfincdb_layer1_temp[\"incdb_cluster\"]=incdb.get_cluster_labels(temp) # get labels for all\n",
    "\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"cluster\"].values.astype(str),title=\"DBScan\")\n",
    "fig.show()\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"incdb_cluster\"].values.astype(str),title=\"IncDBScan\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bf6e2-139b-4946-89db-24b452e651e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60012bbd-8c0c-48f0-aef5-01454c346ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incdb = IncrementalDBSCAN(eps=0.1, min_pts=5)\n",
    "dfincdb_layer1_temp=dfincdb_layer1.copy()\n",
    "dfincdb_layer1_temp['incdb_cluster']=None\n",
    "temp=layer_dict[\"Layer 1\"][\"Data_transformed\"].copy()\n",
    "\n",
    "n=100\n",
    "incdb.insert(temp[0:n]) # 1st batch\n",
    "dfincdb_layer1_temp.loc[0:n-1,\"incdb_cluster\"]=list(incdb.get_cluster_labels(temp[0:n]))\n",
    "fig=px.scatter(x=pca_layer1[0:n, 0],y=pca_layer1[0:n, 1],color=dfincdb_layer1_temp.iloc[0:n][\"cluster\"].values.astype(str),title=\"DBScan\")\n",
    "fig.show()\n",
    "fig=px.scatter(x=pca_layer1[0:n, 0],y=pca_layer1[0:n, 1],color=dfincdb_layer1_temp.iloc[0:n][\"incdb_cluster\"].values.astype(str),title=\"IncDBScan\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843744f-6f58-485b-8a7f-70af129b7009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incdb.insert(temp[n::]) # 2nd batch\n",
    "# dfincdb_layer1_temp.loc[n::,\"incdb_cluster\"]=incdb.get_cluster_labels(temp[n::]) # get labels for 2nd batch\n",
    "dfincdb_layer1_temp[\"incdb_cluster\"]=incdb.get_cluster_labels(temp) # get labels for all\n",
    "\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"cluster\"].values.astype(str),title=\"DBScan\")\n",
    "fig.show()\n",
    "fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=dfincdb_layer1_temp[\"incdb_cluster\"].values.astype(str),title=\"IncDBScan\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372080ed-0d50-4164-abc4-fb7115c87d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incdb = IncrementalDBSCAN(eps=0.1, min_pts=5)\n",
    "dfincdb_layer1_temp=dfincdb_layer1.copy()\n",
    "dfincdb_layer1_temp['incdb_cluster']=None\n",
    "temp=layer_dict[\"Layer 1\"][\"Data_transformed\"].copy()\n",
    "\n",
    "j=0\n",
    "month=1\n",
    "for i in np.arange(56,len(temp),56):\n",
    "    incdb.insert(temp[j:i]) # 1st batch\n",
    "    dfincdb_layer1_temp.loc[0:i-1,\"incdb_cluster\"]=list(incdb.get_cluster_labels(temp[0:i]))\n",
    "    fig=px.scatter(x=pca_layer1[0:i, 0],y=pca_layer1[0:i, 1],color=dfincdb_layer1_temp.iloc[0:i][\"incdb_cluster\"].values.astype(str),title=\"Month-\"+str(month))\n",
    "    fig.show()\n",
    "    j=i\n",
    "    month=month+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c77f96-0472-42c9-a02b-630c3ed18306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d844878-37d6-49da-b284-e34d2d7b9f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year =\"2019\"\n",
    "df_oneyear=df_penmanshiel_per_week_median[df_penmanshiel_per_week_median[\"Datetime\"].astype(str).str.contains(year)]\n",
    "\n",
    "# creating layers\n",
    "Layer1 = df_oneyear.reset_index()[['week-turbine','Wind direction u','Wind direction v']].dropna().copy()\n",
    "Layer2 = df_oneyear.reset_index()[['week-turbine','Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin']].dropna().copy()\n",
    "Layer3 = df_oneyear.reset_index()[['week-turbine','Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)']].dropna().copy()\n",
    "Layer4 = df_oneyear.reset_index()[['week-turbine','Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)']].dropna().copy()\n",
    "Layer5 = df_oneyear.reset_index()[['week-turbine','Motor temperature (°C)','Motor current (A)']].dropna().copy()\n",
    "Layer6 = df_oneyear.reset_index()[['week-turbine','Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)']].dropna().copy()\n",
    "\n",
    "layer_data=[Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]\n",
    "layer_dict={}\n",
    "\n",
    "for l_id in range(len(layer_data)): # Specify number of layers\n",
    "    layer_dict['Layer '+str(l_id+1)] = {}\n",
    "    layer_dict['Layer '+str(l_id+1)][\"Layer_data\"] = layer_data[l_id]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    layer_data_transformed = scaler.fit_transform(layer_data[l_id].drop([\"week-turbine\"],axis=1).to_numpy())\n",
    "    layer_dict['Layer '+str(l_id+1)][\"Data_transformed\"] = layer_data_transformed\n",
    "\n",
    "incdb_1 = IncrementalDBSCAN(eps=0.1, min_pts=5) \n",
    "incdb_2 = IncrementalDBSCAN(eps=0.15, min_pts=5) \n",
    "incdb_3 = IncrementalDBSCAN(eps=0.12, min_pts=5) \n",
    "incdb_4 = IncrementalDBSCAN(eps=0.1, min_pts=5) \n",
    "incdb_5 = IncrementalDBSCAN(eps=0.04, min_pts=5) \n",
    "incdb_6 = IncrementalDBSCAN(eps=0.045, min_pts=5) \n",
    "\n",
    "temp=layer_dict[\"Layer 1\"][\"Data_transformed\"].copy()\n",
    "j=0\n",
    "month=1\n",
    "for i in np.arange(56,len(temp),56):\n",
    "\n",
    "    incdb_1.insert(layer_dict[\"Layer 1\"][\"Data_transformed\"][j:i]) \n",
    "    fig=px.scatter(x=pca_layer1[0:i, 0],y=pca_layer1[0:i, 1],\n",
    "                   color=list(incdb_1.get_cluster_labels(layer_dict[\"Layer 1\"][\"Data_transformed\"][0:i])).astype(str))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b065e-0e8a-41cf-be04-d054b0f3dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce58b8-9c64-41f1-8bfe-5cfdcded4c4d",
   "metadata": {},
   "source": [
    "## Incremental hypergraph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b2ace-b224-42a5-bca8-ed2b1eff5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_penmanshiel_per_week_median[df_penmanshiel_per_week_median[\"Datetime\"].astype(str).str.contains('2018|2019|2020')])\n",
    "len(df_penmanshiel_per_week_median[df_penmanshiel_per_week_median[\"Datetime\"].astype(str).str.contains('2018')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23c561b-7621-4670-8298-55b50a679242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960da3d-3b8d-4e65-bfe7-69fcb430f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_clusters(final_clusters1, hypergraph1, method):\n",
    "    # mapping hyperedges to data objects to obtain the clustering solution of data objects\n",
    "    temp_del = 0\n",
    "    clustering_nodes = {}\n",
    "    for key, val in final_clusters1.items():\n",
    "        if method == 'donot_inc_key_in_cluster':\n",
    "            pins_center = []\n",
    "        elif method == \"inc_key_in_cluster\":\n",
    "            pins_center = list(hypergraph1.pins(key))\n",
    "        \n",
    "        for _ in val:\n",
    "            pins_center.extend(list(hypergraph1.pins(_)))\n",
    "        clustering_nodes[key] = set(pins_center)\n",
    "        temp_del = temp_del + len(set(pins_center))\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering of data objects\", clustering_nodes) # dict, key = center(hyperedge), values = data objects\n",
    "    \n",
    "    # replacing the index of the data object with its short id\n",
    "    clus_nodes_short_id = {}\n",
    "    for key, val in clustering_nodes.items():\n",
    "        # print(val)\n",
    "        clus_nodes_short_id[key] = {nodes_hyper[x] for x in val} # note that sets are not ordered\n",
    "\n",
    "    if Debug == True:\n",
    "        print(\"clustering solution, key = center (hyperedge), val = set of short_ids\")\n",
    "        print(clus_nodes_short_id)\n",
    "    return clus_nodes_short_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ede072-a632-4ba0-ba8c-1eafc9a6a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(base_df, new_df, new_suffix):\n",
    "    merged_df = pd.merge(base_df, new_df, on='week-turbine', how='right', suffixes=('', new_suffix))\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06441e88-9a55-47c4-bd29-db9bddb226ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a06556-aa28-461a-952b-1db22db28cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year =\"2019\"\n",
    "# df_oneyear=df_penmanshiel_per_week_median[df_penmanshiel_per_week_median[\"Datetime\"].astype(str).str.contains(year)]\n",
    "\n",
    "show=True\n",
    "# for b in range(13):\n",
    "for b in [10]:\n",
    "    batch=b\n",
    "    print(batch)\n",
    "    df_oneyear=df_penmanshiel_per_week_median.iloc[0:(742+(56*batch))].copy()\n",
    "    # creating layers\n",
    "    Layer1 = df_oneyear.reset_index()[['week-turbine','Wind direction u','Wind direction v']].dropna().copy()\n",
    "    Layer2 = df_oneyear.reset_index()[['week-turbine','Nacelle position cos', 'Nacelle position sin','Vane position 1+2 cos', 'Vane position 1+2 sin','Blade Angle (pitch position) cos','Blade Angle (pitch position) sin']].dropna().copy()\n",
    "    Layer3 = df_oneyear.reset_index()[['week-turbine','Generator bearing rear temperature (°C)','Generator bearing front temperature (°C)','Generator RPM (RPM)','Rotor bearing temp (°C)','Drive train acceleration (mm/ss)']].dropna().copy()\n",
    "    Layer4 = df_oneyear.reset_index()[['week-turbine','Gear oil temperature (°C)','Gear oil inlet temperature (°C)','Gear oil pump pressure (bar)']].dropna().copy()\n",
    "    Layer5 = df_oneyear.reset_index()[['week-turbine','Motor temperature (°C)','Motor current (A)']].dropna().copy()\n",
    "    Layer6 = df_oneyear.reset_index()[['week-turbine','Tower Acceleration X (mm/ss)','Tower Acceleration y (mm/ss)']].dropna().copy()\n",
    "    \n",
    "    layer_data=[Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]\n",
    "    layer_dict={}\n",
    "    \n",
    "    for l_id in range(len(layer_data)): # Specify number of layers\n",
    "        layer_dict['Layer '+str(l_id+1)] = {}\n",
    "        layer_dict['Layer '+str(l_id+1)][\"Layer_data\"] = layer_data[l_id]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        layer_data_transformed = scaler.fit_transform(layer_data[l_id].drop([\"week-turbine\"],axis=1).to_numpy())\n",
    "        layer_dict['Layer '+str(l_id+1)][\"Data_transformed\"] = layer_data_transformed\n",
    "    \n",
    "    db = DBSCAN(eps=0.1).fit(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "    pca_layer1 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 1\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer1[:, 0],y=pca_layer1[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 1\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "        fig.show()\n",
    "    Layer1[\"cluster\"]=layer_dict[\"Layer 1\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer1=Layer1.copy()\n",
    "    \n",
    "    db = DBSCAN(eps=0.12).fit(layer_dict[\"Layer 2\"][\"Data_transformed\"])\n",
    "    pca_layer2= PCA(n_components=6).fit_transform(layer_dict[\"Layer 2\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer2[:, 0],y=pca_layer2[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 2\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "        fig.show()\n",
    "    Layer2[\"cluster\"]=layer_dict[\"Layer 2\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer2=Layer2.copy()\n",
    "    \n",
    "    db = DBSCAN(eps=0.1).fit(layer_dict[\"Layer 3\"][\"Data_transformed\"])\n",
    "    pca_layer3= PCA(n_components=2).fit_transform(layer_dict[\"Layer 3\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer3[:, 0],y=pca_layer3[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 3\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "        fig.show()\n",
    "    Layer3[\"cluster\"]=layer_dict[\"Layer 3\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer3=Layer3.copy()\n",
    "    \n",
    "    db = DBSCAN(eps=0.1).fit(layer_dict[\"Layer 4\"][\"Data_transformed\"])\n",
    "    pca_layer4= PCA(n_components=2).fit_transform(layer_dict[\"Layer 4\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer4[:, 0],y=pca_layer4[:, 1],color=db.labels_.astype(str),title=\"PCA (\" +str(np.round(np.sum(PCA(n_components=2).fit(layer_dict[\"Layer 4\"][\"Data_transformed\"]).explained_variance_ratio_),2)) +\")\")\n",
    "        fig.show()\n",
    "    Layer4[\"cluster\"]=layer_dict[\"Layer 4\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer4=Layer4.copy()\n",
    "    \n",
    "    db = DBSCAN(eps=0.05).fit(layer_dict[\"Layer 5\"][\"Data_transformed\"])\n",
    "    pca_layer5 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 5\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer5[:, 0],y=pca_layer5[:, 1],color=db.labels_.astype(str),title=\"PCA(1.0)\")\n",
    "        fig.show()\n",
    "    Layer5[\"cluster\"]=layer_dict[\"Layer 5\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer5=Layer5.copy()\n",
    "    \n",
    "    db = DBSCAN(eps=0.05).fit(layer_dict[\"Layer 6\"][\"Data_transformed\"])\n",
    "    pca_layer6 = PCA(n_components=2).fit_transform(layer_dict[\"Layer 6\"][\"Data_transformed\"])\n",
    "    if (show):\n",
    "        fig=px.scatter(x=pca_layer6[:, 0],y=pca_layer6[:, 1],color=db.labels_.astype(str),title=\"PCA (1.0)\")\n",
    "        fig.show()\n",
    "    Layer6[\"cluster\"]=layer_dict[\"Layer 6\"][\"Layer_data\"][\"cluster\"]=db.labels_\n",
    "    dfincdb_layer6=Layer6.copy()\n",
    "    \n",
    "    # Drop outliers\n",
    "    layer_data=[Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]\n",
    "    for l_id, key in enumerate(layer_dict.keys()):\n",
    "        temp=layer_dict[key][\"Layer_data\"]\n",
    "        layer_dict[key][\"Layer_data\"]=temp[temp[\"cluster\"]!=-1]\n",
    "        layer_data[l_id]=temp[temp[\"cluster\"]!=-1]\n",
    "    \n",
    "    Layer1=Layer1[Layer1[\"cluster\"]!=-1]\n",
    "    Layer2=Layer2[Layer2[\"cluster\"]!=-1]\n",
    "    Layer3=Layer3[Layer3[\"cluster\"]!=-1]\n",
    "    Layer4=Layer4[Layer4[\"cluster\"]!=-1]\n",
    "    Layer5=Layer5[Layer5[\"cluster\"]!=-1]\n",
    "    Layer6=Layer6[Layer6[\"cluster\"]!=-1]\n",
    "    if (show):\n",
    "        for key in layer_dict.keys():\n",
    "            print(key+\"--->\"+str(len(np.unique(layer_dict[key][\"Layer_data\"][\"cluster\"])))+\" clusters\")\n",
    "    \n",
    "    # Hypergraph\n",
    "    # creating a nested list, where each inner list lists the ids in that cluster. \n",
    "    lst = [v for v in Layer1.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    lst = lst + [v for v in Layer2.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    lst = lst + [v for v in Layer3.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    lst = lst + [v for v in Layer4.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    lst = lst + [v for v in Layer5.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    lst = lst + [v for v in Layer6.groupby('cluster')['week-turbine'].apply(list).values]\n",
    "    hyperedge_indices = []\n",
    "    cnt = 0\n",
    "    hyperedge_indices.append(cnt)\n",
    "    for each in lst:\n",
    "        cnt = cnt+len(each)\n",
    "        hyperedge_indices.append(cnt)\n",
    "    nodes_hyper = df_oneyear.index.tolist()\n",
    "    hyperedges_1 = [item for sublist in lst for item in sublist]\n",
    "    hyperedges = [nodes_hyper.index(i) for i in hyperedges_1]\n",
    "    num_nodes = len(nodes_hyper)\n",
    "    num_nets = len(hyperedge_indices)-1\n",
    "    \n",
    "    k = 2\n",
    "    hypergraph = kahypar.Hypergraph(num_nodes, num_nets, hyperedge_indices, hyperedges, k)\n",
    "    context = kahypar.Context()\n",
    "    context.loadINIconfiguration(\"cut_kKaHyPar_sea20.ini\")\n",
    "    \n",
    "    node_incident_edges = [] # list containg the incident edges of each node in sublist.\n",
    "    for each_node in hypergraph.nodes():\n",
    "        ie = []\n",
    "        for incident_edge in hypergraph.incidentEdges(each_node):\n",
    "            ie.append(incident_edge)\n",
    "        node_incident_edges.append(ie)\n",
    "    cluster_list = [sublist for sublist in lst if len(sublist) > 1]\n",
    "    \n",
    "    #Obtaining the neighbourhood of each edge.\n",
    "    total_edges = hypergraph.numEdges()\n",
    "    \n",
    "    neighbourhood = [] # neighbourhood of each edge is presented in order.\n",
    "    number_neighbours = []\n",
    "    for i in range(total_edges):\n",
    "        temp = []\n",
    "        for node, edges in enumerate(node_incident_edges):\n",
    "            if i in edges:\n",
    "                for e in edges:\n",
    "                    temp.append(e)\n",
    "        neighbourhood.append(set(temp))\n",
    "        number_neighbours.append(len(set(temp)))\n",
    "    layer_list_temp=[]\n",
    "    for ind,key in enumerate([Layer1,Layer2,Layer3,Layer4,Layer5,Layer6]):\n",
    "        for cls in range(key[\"cluster\"].nunique()):\n",
    "            layer_list_temp.append(\"Layer\"+str(ind+1)+\"- cluster\"+str(cls+1))\n",
    "    \n",
    "    # calculating Nearest Neighbourhood Similarity\n",
    "    NNS = {}\n",
    "    for i in range(total_edges):\n",
    "        for j in range(i, total_edges):\n",
    "            if i != j:\n",
    "                # print(\"checking intersection of \", i, \"and\", j, \"i.e.,\", neighbourhood[i], \"and\", neighbourhood[j])\n",
    "                intersection = neighbourhood[i].intersection(neighbourhood[j])\n",
    "                if (i not in intersection) or (j not in intersection): # or condition is not required. if i is in intersection then automatically j will be in the intersection. \n",
    "                    # print(i, j, intersection, neighbourhood[i], neighbourhood[j])\n",
    "                    NNS[(i,j)] = 0\n",
    "                else:\n",
    "                    union = neighbourhood[i].union(neighbourhood[j])\n",
    "                    NNS[(i,j)] = len(intersection)/len(union)\n",
    "    cnt=0\n",
    "    df_snns=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "    for i in np.arange(0,total_edges,1):\n",
    "        for j in np.arange(0,total_edges,1):\n",
    "            if (i,j) in NNS:\n",
    "                cnt=cnt+1\n",
    "                df_snns.loc[i,j]=NNS[i,j]\n",
    "    # converting the similarity matrix into distance matrix.\n",
    "    NNS_dist = {k: 1-v for k, v in NNS.items()}\n",
    "    \n",
    "    # obtaining the distance matrix\n",
    "    array_dist = []\n",
    "    for i in range(total_edges):\n",
    "        temp = []\n",
    "        for j in range(total_edges):\n",
    "            if i != j:\n",
    "                temp.append(NNS_dist[(min(i, j), max(i,j))])\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        array_dist.append(temp)\n",
    "    \n",
    "    df_dist=pd.DataFrame(index=np.arange(0,total_edges,1),columns=np.arange(0,total_edges,1))\n",
    "    for i in np.arange(0,total_edges,1):\n",
    "        for j in np.arange(0,total_edges,1):\n",
    "            # if (i<=j):\n",
    "            df_dist.loc[i,j]=np.round(array_dist[i][j],2)\n",
    "    df_sim=1-df_dist\n",
    "    \n",
    "    #KMedoids\n",
    "    # Clustering using KMedoids, and calculated distance matrix based on NNS\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "    \n",
    "    silhouette_score = []\n",
    "    labels_kmedoids = []\n",
    "    for num_clusters in range(2, total_edges):\n",
    "        kmedoids_ = KMedoids(n_clusters=num_clusters, metric='precomputed', method='pam', init='k-medoids++', random_state=0).fit_predict(np.array(array_dist))\n",
    "        # print(kmedoids_)\n",
    "        hy_sorted_cluster = []\n",
    "        for _ in range(num_clusters):\n",
    "            hy_sorted_cluster.extend(np.where(kmedoids_ == _)[0].tolist())\n",
    "    \n",
    "        heat_map = []\n",
    "        for i in hy_sorted_cluster:\n",
    "            temp = []\n",
    "            for j in hy_sorted_cluster:\n",
    "                if i != j:\n",
    "                    temp.append(NNS_dist[(min(i, j), max(i,j))])\n",
    "                else:\n",
    "                    temp.append(0)\n",
    "            heat_map.append(temp)\n",
    "            \n",
    "    \n",
    "        silhouette_score.append(metrics.silhouette_score(array_dist, kmedoids_, metric=\"precomputed\"))\n",
    "        labels_kmedoids.append(kmedoids_)\n",
    "    \n",
    "        index_heatmap = kmedoids_.copy()\n",
    "        index_heatmap.sort()\n",
    "        dataframe_heat_map = pd.DataFrame(heat_map, index=index_heatmap, columns=index_heatmap)\n",
    "    if (show):\n",
    "        i_ = 2\n",
    "        for each in silhouette_score:\n",
    "            print(i_, each)\n",
    "            i_ = i_+1\n",
    "\n",
    "        plt.plot(range(2,total_edges), silhouette_score)\n",
    "        plt.ylabel(\"silhouette score\")\n",
    "        plt.xlabel(\"number of clusters\")\n",
    "        \n",
    "    if (show):\n",
    "        # KMeans\n",
    "        cvm.find_optimal_number_of_clusters(array_dist, algorithm=KMeans, display= True, seed= 0, col_wrap=4 ,score_metrics = ['silhouette_score',\n",
    "                                     'calinski_harabasz_score',\n",
    "                                     'davies_bouldin_score',\n",
    "                                     'connectivity_score'], max_number_clusters=total_edges-1)\n",
    "        \n",
    "        # Agglomerative\n",
    "        cvm.find_optimal_number_of_clusters(array_dist, algorithm=AgglomerativeClustering, display= True, seed= 0, col_wrap=4 ,score_metrics = ['silhouette_score',\n",
    "                                     'calinski_harabasz_score',\n",
    "                                     'davies_bouldin_score',\n",
    "                                     'connectivity_score'], max_number_clusters=total_edges-1)\n",
    "    \n",
    "    # kmeans=KMeans(n_clusters=5).fit_predict(np.array(array_dist))\n",
    "    # kmeans = KMedoids(n_clusters=7, metric='precomputed', method='pam', init='k-medoids++', random_state=0).fit_predict(np.array(array_dist))\n",
    "    kmeans=AgglomerativeClustering(n_clusters=6).fit_predict(np.array(array_dist))\n",
    "    dict_clusters = {0: [], 1:[], 2:[],3:[],4:[],5:[]}\n",
    "    for index, each in enumerate(kmeans):\n",
    "        dict_clusters[each].append(index)\n",
    "    \n",
    "    Debug = False\n",
    "    clus_sol = generate_final_clusters(dict_clusters, hypergraph, 'donot_inc_key_in_cluster')\n",
    "    # print(clus_sol) # clus_sol is the final clustering solution based on k-medoids based method. key: cluster number, value: data objects in cluster\n",
    "    cluster_objects_list=[list(value) for value in clus_sol.values()]\n",
    "    if (show):\n",
    "        for key in clus_sol.keys():\n",
    "            print(str(key) +\" : \"+str(len(list(clus_sol[key]))))\n",
    "\n",
    "        fig=go.Figure()\n",
    "        for key in clus_sol.keys():\n",
    "            temp=df_oneyear.reset_index().copy()\n",
    "            temp=temp[temp[\"week-turbine\"].isin(list(clus_sol[key]))].copy()\n",
    "        \n",
    "            fig.add_trace(go.Box(y=temp['Power (kW)'], name='E'+str(key)))\n",
    "        \n",
    "        fig.update_layout(title=\"Active power\")\n",
    "        # fig.update_xaxes(title=\"Cluster\")\n",
    "        fig.update_yaxes(title=\"Power (kW)\")\n",
    "        fig.show()\n",
    "    \n",
    "    df_fca=pd.DataFrame(index=df_oneyear.index.astype(str),columns=clus_sol.keys())\n",
    "    for ind,row in df_fca.iterrows():\n",
    "        for clus in row.index:\n",
    "            if (ind in list(clus_sol[clus])):\n",
    "                df_fca.loc[ind,clus]=True\n",
    "            else:\n",
    "                df_fca.loc[ind,clus]=False\n",
    "    df_fca.columns=df_fca.columns.astype(str)\n",
    "    df_fca.columns=\"E\"+df_fca.columns\n",
    "    \n",
    "    from fcapy.context import FormalContext\n",
    "    K = FormalContext.from_pandas(pd.DataFrame(df_fca))\n",
    "    \n",
    "    from fcapy.lattice import ConceptLattice\n",
    "    L = ConceptLattice.from_context(K)\n",
    "    if (show):\n",
    "        from fcapy.visualizer import LineVizNx\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        vsl = LineVizNx()\n",
    "        vsl.draw_concept_lattice(L, ax=ax, flg_node_indices=True,flg_new_intent_count_prefix=False)\n",
    "        ax.set_title('week-turbine concept lattice')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    week_cluster={}\n",
    "    for w_id in np.arange(1,df_oneyear[\"week\"].nunique()+1,1):\n",
    "        clst_lst=[]\n",
    "        for key in clus_sol.keys():\n",
    "            lst=[element for element in list(clus_sol[key]) if \"W\"+str(w_id)+\":\" in element]\n",
    "            if (len(lst)!=0):\n",
    "                clst_lst.append(key)\n",
    "        week_cluster[w_id]=clst_lst\n",
    "    \n",
    "    \n",
    "    turbine_cluster={}\n",
    "    for turb in df_oneyear[\"Turbine\"].unique():\n",
    "        clst_lst=[]\n",
    "        for key in clus_sol.keys():\n",
    "            lst=[element for element in list(clus_sol[key]) if turb in element]\n",
    "            if (len(lst)!=0):\n",
    "                clst_lst.append(key)\n",
    "        turbine_cluster[turb]=clst_lst\n",
    "    \n",
    "    df_week_cluster=pd.DataFrame(str(0),index=list(week_cluster.keys()),columns=np.arange(0,len(clus_sol.keys()),1))\n",
    "    for ind,row in df_week_cluster.iterrows():\n",
    "        df_week_cluster.loc[ind,week_cluster[ind]]=str(1)\n",
    "    df_week_cluster.index=df_week_cluster.index.astype(str)\n",
    "    df_week_cluster.columns=df_week_cluster.columns.astype(str)\n",
    "    \n",
    "    \n",
    "    df_turbine_cluster=pd.DataFrame(str(0),index=list(turbine_cluster.keys()),columns=np.arange(0,len(clus_sol.keys()),1))\n",
    "    for ind,row in df_turbine_cluster.iterrows():\n",
    "        df_turbine_cluster.loc[ind,turbine_cluster[ind]]=str(1)\n",
    "    df_turbine_cluster.index=df_turbine_cluster.index.astype(str)\n",
    "    df_turbine_cluster.columns=df_turbine_cluster.columns.astype(str)\n",
    "    \n",
    "    df_week_turbine_plot=pd.DataFrame(index=df_week_cluster.index.astype(int),columns=df_turbine_cluster.index)\n",
    "    unique_combination=[]\n",
    "    for w_id in np.arange(1,df_oneyear[\"week\"].nunique()+1,1):\n",
    "        for t_id in df_turbine_cluster.index:\n",
    "            week_turb= \"W\"+str(w_id)+\":\"+t_id \n",
    "            comb = [key for key, values in clus_sol.items() if week_turb in values]\n",
    "            df_week_turbine_plot.loc[w_id,t_id]=comb\n",
    "            if comb not in unique_combination:\n",
    "                unique_combination.append(comb)\n",
    "    unique_dict=dict(zip(np.arange(100,100+len(unique_combination)+1,1), unique_combination))\n",
    "    unique_dict={tuple(value): key for key, value in unique_dict.items()}\n",
    "    unique_dict.keys()\n",
    "    \n",
    "    df_week_turbine_plot_id=df_week_turbine_plot.copy()\n",
    "    for ind,row in df_week_turbine_plot_id.iterrows():\n",
    "        for col in df_week_turbine_plot_id.columns:\n",
    "            df_week_turbine_plot_id.loc[ind,col]=unique_dict[tuple(df_week_turbine_plot_id.loc[ind,col])]\n",
    "            \n",
    "    df_week_turbine_plot_id.index=df_week_turbine_plot_id.index.astype(str)\n",
    "    df_week_turbine_plot_id=df_week_turbine_plot_id.astype(int).astype(str)\n",
    "    if (show):\n",
    "        fig = px.imshow(df_week_turbine_plot_id.T,labels=dict(x=\"Week\", y=\"Turbine\"))\n",
    "        fig.update_layout(height=700)\n",
    "        fig.show()\n",
    "        print(unique_dict)\n",
    "    \n",
    "    if (batch==0): # Initial batch\n",
    "        Layer1_merging=Layer1[[\"week-turbine\",\"cluster\"]].copy()\n",
    "        Layer2_merging=Layer2[[\"week-turbine\",\"cluster\"]].copy()\n",
    "        Layer3_merging=Layer3[[\"week-turbine\",\"cluster\"]].copy()\n",
    "        Layer4_merging=Layer4[[\"week-turbine\",\"cluster\"]].copy()\n",
    "        Layer5_merging=Layer5[[\"week-turbine\",\"cluster\"]].copy()\n",
    "        Layer6_merging=Layer6[[\"week-turbine\",\"cluster\"]].copy()\n",
    "    \n",
    "    else:\n",
    "        Layer1_merging=merge_dataframes(Layer1_merging,Layer1[[\"week-turbine\",\"cluster\"]], '_'+str(batch))\n",
    "        Layer2_merging=merge_dataframes(Layer2_merging,Layer2[[\"week-turbine\",\"cluster\"]], '_'+str(batch))\n",
    "        Layer3_merging=merge_dataframes(Layer3_merging,Layer3[[\"week-turbine\",\"cluster\"]], '_'+str(batch))\n",
    "        Layer4_merging=merge_dataframes(Layer4_merging,Layer4[[\"week-turbine\",\"cluster\"]], '_'+str(batch))\n",
    "        Layer5_merging=merge_dataframes(Layer5_merging,Layer5[[\"week-turbine\",\"cluster\"]], '_'+str(batch))\n",
    "        Layer6_merging=merge_dataframes(Layer6_merging,Layer6[[\"week-turbine\",\"cluster\"]], '_'+str(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04063b-2523-4421-a90e-6645d2b51df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "for l_id,val in enumerate([Layer1_merging,Layer2_merging,Layer3_merging,Layer4_merging,Layer5_merging,Layer6_merging]):\n",
    "    temp=val.nunique().drop('week-turbine').dropna()\n",
    "    # temp=pd.concat([temp.iloc[1:], temp.iloc[:1]]).iloc[::-1]\n",
    "    fig.add_traces(go.Scatter(x=temp.index,y=temp.values,name=\"Layer \"+str(l_id+1)))\n",
    "\n",
    "# fig.write_html(\"Layer_clusters_initial_period_1year.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e1d09-6b0c-43b3-b4d9-03a243b91973",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=Layer2_merging[[\"cluster\",\"cluster_1\"]].dropna()\n",
    "adjusted_mutual_info_score(temp[\"cluster\"],temp[\"cluster_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f11f3-322a-4711-aa93-df8e0f302d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.line(Layer6_merging.drop(\"week-turbine\",axis=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac098b-0610-4317-bc75-cc09695d5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer6_merging[\"cluster_4\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38c065-7c56-4f38-b2cd-907c800bbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1_merging.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab51cc8-c69e-424e-b63c-bd53f1d8d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking Layer clusters \n",
    "df_layer_clusters_ami=pd.DataFrame(index=[\"Layer1\",\"Layer2\",\"Layer3\",\"Layer4\",\"Layer5\",\"Layer6\"],columns=Layer1_merging.columns[2::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59455d5-5157-423a-aedc-6918cae1cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layer_clusters_ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4741a-d5df-4b2a-a68f-739755c219b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "for l_id,val in enumerate([Layer1_merging,Layer2_merging,Layer3_merging,Layer4_merging,Layer5_merging,Layer6_merging]):\n",
    "    print(l_id)\n",
    "    temp=val\n",
    "    init='cluster'\n",
    "    for col in df_layer_clusters_ami.columns:\n",
    "        temp_=temp[[init,col]].dropna()\n",
    "        df_layer_clusters_ami.loc[\"Layer\"+str(l_id+1),col]=adjusted_mutual_info_score(temp_[init],temp_[col])\n",
    "        init=col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a0969-6e4e-4f2b-92cb-89321586874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layer_clusters_ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8c9fd-a95d-49d2-a83d-ffc0c9fce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.line(df_layer_clusters_ami.T)\n",
    "# fig.write_html(\"Layer_clusters_initial_period_1year_ami.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edfb28-ae69-4e2b-9bd1-40b726d1d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.line(Layer6_merging.drop(\"week-turbine\",axis=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a42ca1-9060-4b13-b716-dfd1ebe529b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.line(Layer2_merging.drop(\"week-turbine\",axis=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c20d31-58ce-489d-91d6-8932e53259f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=Layer6_merging[[\"cluster_3\",\"cluster_4\"]].dropna()\n",
    "adjusted_mutual_info_score(temp[\"cluster_3\"],temp[\"cluster_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc30d3-5a15-47ea-9e73-b7cce256f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=go.Figure()\n",
    "for l_id,val in enumerate([Layer1_merging,Layer2_merging,Layer3_merging,Layer4_merging,Layer5_merging,Layer6_merging]):\n",
    "    temp=val.nunique().drop('week-turbine').dropna()\n",
    "    # temp=pd.concat([temp.iloc[1:], temp.iloc[:1]]).iloc[::-1]\n",
    "    fig.add_traces(go.Scatter(x=temp.index,y=temp.values,name=\"Layer \"+str(l_id+1)))\n",
    "\n",
    "# fig.write_html(\"Layer_clusters_initial_period_1year.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35098fb-024d-4ea8-ae59-0437e9b73399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1828e4-a387-4b22-b824-4c4f497d464d",
   "metadata": {},
   "source": [
    "#### Assign clusters using historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2758488-f2d8-444b-ae3f-9671c0ecc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layercls_hypcls=pd.DataFrame(columns=layer_list_temp,index=['E' + str(i) for i in dict_clusters.keys()])\n",
    "elem_modes_historical={}\n",
    "for key in dict_clusters.keys():\n",
    "    print(\"E\"+str(key))\n",
    "    print([layer_list_temp[i] for i in dict_clusters[key]])\n",
    "\n",
    "    elem_modes_historical[\"E\"+str(key)]=[layer_list_temp[i] for i in dict_clusters[key]]\n",
    "    \n",
    "    df_layercls_hypcls.loc[\"E\"+str(key),[layer_list_temp[i] for i in dict_clusters[key]]]=1\n",
    "df_layercls_hypcls=df_layercls_hypcls.notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910b7b2-9a1d-49be-9851-2ab1a90b0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.imshow(df_layercls_hypcls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6759b7-3329-4dc1-87a1-19210cf28ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[Layer1.rename(columns={'cluster': 'Layer1'})[['week-turbine','Layer1']],\n",
    "     Layer2.rename(columns={'cluster': 'Layer2'})[['week-turbine','Layer2']],\n",
    "     Layer3.rename(columns={'cluster': 'Layer3'})[['week-turbine','Layer3']],\n",
    "     Layer4.rename(columns={'cluster': 'Layer4'})[['week-turbine','Layer4']],\n",
    "     Layer5.rename(columns={'cluster': 'Layer5'})[['week-turbine','Layer5']],\n",
    "     Layer6.rename(columns={'cluster': 'Layer6'})[['week-turbine','Layer6']]]\n",
    "from functools import reduce\n",
    "df_layercluster=reduce(lambda left, right: pd.merge(left, right, on='week-turbine', how='outer'), dfs)\n",
    "\n",
    "# Reorder dataframe\n",
    "df_layercluster[\"week-turbine\"]=pd.Categorical(df_layercluster['week-turbine'], categories=df_oneyear.index, ordered=True)\n",
    "df_layercluster=df_layercluster.sort_values(\"week-turbine\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d59bcb-3ca5-4825-b375-84a2d6b11e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_batch=9\n",
    "\n",
    "df_layercluster_newbatch=df_layercluster[(742+(56*testing_batch))::].copy()\n",
    "for ind,row in df_layercluster_newbatch.iterrows():\n",
    "    for col in df_layercluster_newbatch.columns[1::]:\n",
    "        if pd.notna(row[col]):\n",
    "            df_layercluster_newbatch.loc[ind,col]=col + \"- \"+\"cluster\"+str(int(row[col])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ca571-a9ab-4be6-8674-3f1ef93af8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_layercluster_newbatch[\"Elementary_modes_prediction_historical\"]=None\n",
    "df_layercluster_newbatch[\"Elementary_modes_prediction_hypergraph\"]=None\n",
    "for ind,row in df_layercluster_newbatch.iterrows():\n",
    "    elemmode=[]\n",
    "    for elem in elem_modes_historical.keys():\n",
    "        if (len(list(set(row[1::].values) & set(elem_modes_historical[elem])))!=0):\n",
    "            elemmode.append(elem) \n",
    "    df_layercluster_newbatch.at[ind,\"Elementary_modes_prediction_historical\"]=elemmode\n",
    "    temp=df_fca.loc[row[\"week-turbine\"]]\n",
    "    df_layercluster_newbatch.at[ind,\"Elementary_modes_prediction_hypergraph\"]=temp[temp].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecb5df-7505-41b5-9a4e-5eb41abb21e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_layercluster_newbatch[\"week\"]=df_oneyear[(742+(56*testing_batch))::][\"week\"].values\n",
    "df_layercluster_newbatch[\"Turbine\"]=df_oneyear[(742+(56*testing_batch))::][\"Turbine\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4a3b8-90f2-4a47-880c-0448f2d1b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_layercluster_newbatch[\"Elementary_modes_prediction_hypergraph\"].astype(str),\n",
    "                            df_layercluster_newbatch[\"Elementary_modes_prediction_historical\"].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047e32b-2bfa-4ad5-a6ae-f7ccf316d0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
